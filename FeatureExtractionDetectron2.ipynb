{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeatureExtractionDetectron2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agam-kashyap/HatefulMemes/blob/master/FeatureExtractionDetectron2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEUoaDW9NLIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b671c80c-c23b-4183-ad34-1ca23ac3e6fc"
      },
      "source": [
        "!wget -O Lnmwdnq3YcF7F3YsJncp.zip --no-check-certificate --no-proxy \"https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=yFNnmbUHzXx840VEt6ttk0Cj3F0%3D&Expires=1599867150\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-08 10:24:59--  https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ&Signature=yFNnmbUHzXx840VEt6ttk0Cj3F0%3D&Expires=1599867150\n",
            "Resolving drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)... 52.218.229.3\n",
            "Connecting to drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)|52.218.229.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3599495834 (3.4G) [application/zip]\n",
            "Saving to: ‘Lnmwdnq3YcF7F3YsJncp.zip’\n",
            "\n",
            "Lnmwdnq3YcF7F3YsJnc 100%[===================>]   3.35G  22.8MB/s    in 2m 34s  \n",
            "\n",
            "2020-09-08 10:27:33 (22.3 MB/s) - ‘Lnmwdnq3YcF7F3YsJncp.zip’ saved [3599495834/3599495834]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msq1OnhrQcZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6658e2a0-930f-4186-f748-55e38d197587"
      },
      "source": [
        "!unzip -P KexZs4tn8hujn1nK Lnmwdnq3YcF7F3YsJncp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: data/img/97160.png      \n",
            "  inflating: data/img/17904.png      \n",
            "  inflating: data/img/64285.png      \n",
            "  inflating: data/img/06349.png      \n",
            "  inflating: data/img/80576.png      \n",
            "  inflating: data/img/74513.png      \n",
            "  inflating: data/img/08934.png      \n",
            "  inflating: data/img/18764.png      \n",
            "  inflating: data/img/48635.png      \n",
            "  inflating: data/img/45269.png      \n",
            "  inflating: data/img/08957.png      \n",
            "  inflating: data/img/74965.png      \n",
            "  inflating: data/img/14278.png      \n",
            "  inflating: data/img/81064.png      \n",
            "  inflating: data/img/54263.png      \n",
            "  inflating: data/img/65298.png      \n",
            "  inflating: data/img/79512.png      \n",
            "  inflating: data/img/58367.png      \n",
            "  inflating: data/img/18693.png      \n",
            "  inflating: data/img/75210.png      \n",
            "  inflating: data/img/40837.png      \n",
            "  inflating: data/img/23045.png      \n",
            "  inflating: data/img/16048.png      \n",
            "  inflating: data/img/10453.png      \n",
            "  inflating: data/img/89762.png      \n",
            "  inflating: data/img/21869.png      \n",
            "  inflating: data/img/98016.png      \n",
            "  inflating: data/img/20751.png      \n",
            "  inflating: data/img/67109.png      \n",
            "  inflating: data/img/97520.png      \n",
            "  inflating: data/img/05283.png      \n",
            "  inflating: data/img/59321.png      \n",
            "  inflating: data/img/61497.png      \n",
            "  inflating: data/img/12039.png      \n",
            "  inflating: data/img/08761.png      \n",
            "  inflating: data/img/28461.png      \n",
            "  inflating: data/img/75462.png      \n",
            "  inflating: data/img/18350.png      \n",
            "  inflating: data/img/65213.png      \n",
            "  inflating: data/img/05762.png      \n",
            "  inflating: data/img/91034.png      \n",
            "  inflating: data/img/47680.png      \n",
            "  inflating: data/img/60314.png      \n",
            "  inflating: data/img/01953.png      \n",
            "  inflating: data/img/16849.png      \n",
            "  inflating: data/img/87165.png      \n",
            "  inflating: data/img/36521.png      \n",
            "  inflating: data/img/87054.png      \n",
            "  inflating: data/img/98425.png      \n",
            "  inflating: data/img/36840.png      \n",
            "  inflating: data/img/28516.png      \n",
            "  inflating: data/img/64027.png      \n",
            "  inflating: data/img/79081.png      \n",
            "  inflating: data/img/72490.png      \n",
            "  inflating: data/img/26371.png      \n",
            "  inflating: data/img/69425.png      \n",
            "  inflating: data/img/03795.png      \n",
            "  inflating: data/img/61537.png      \n",
            "  inflating: data/img/53048.png      \n",
            "  inflating: data/img/87034.png      \n",
            "  inflating: data/img/65904.png      \n",
            "  inflating: data/img/84106.png      \n",
            "  inflating: data/img/45891.png      \n",
            "  inflating: data/img/84179.png      \n",
            "  inflating: data/img/47809.png      \n",
            "  inflating: data/img/34021.png      \n",
            "  inflating: data/img/04873.png      \n",
            "  inflating: data/img/07248.png      \n",
            "  inflating: data/img/95274.png      \n",
            "  inflating: data/img/72345.png      \n",
            "  inflating: data/img/79603.png      \n",
            "  inflating: data/img/97502.png      \n",
            "  inflating: data/img/67420.png      \n",
            "  inflating: data/img/52091.png      \n",
            "  inflating: data/img/40578.png      \n",
            "  inflating: data/img/76894.png      \n",
            "  inflating: data/img/25149.png      \n",
            "  inflating: data/img/57209.png      \n",
            "  inflating: data/img/43651.png      \n",
            "  inflating: data/img/37092.png      \n",
            "  inflating: data/img/62904.png      \n",
            "  inflating: data/img/73690.png      \n",
            "  inflating: data/img/17532.png      \n",
            "  inflating: data/img/54601.png      \n",
            "  inflating: data/img/91260.png      \n",
            "  inflating: data/img/18052.png      \n",
            "  inflating: data/img/47905.png      \n",
            "  inflating: data/img/78314.png      \n",
            "  inflating: data/img/48539.png      \n",
            "  inflating: data/img/89523.png      \n",
            "  inflating: data/img/85710.png      \n",
            "  inflating: data/img/81603.png      \n",
            "  inflating: data/img/78321.png      \n",
            "  inflating: data/img/25497.png      \n",
            "  inflating: data/img/47368.png      \n",
            "  inflating: data/img/12547.png      \n",
            "  inflating: data/img/68431.png      \n",
            "  inflating: data/img/39287.png      \n",
            "  inflating: data/img/63290.png      \n",
            "  inflating: data/img/98526.png      \n",
            "  inflating: data/img/71320.png      \n",
            "  inflating: data/img/04732.png      \n",
            "  inflating: data/img/76421.png      \n",
            "  inflating: data/img/10357.png      \n",
            "  inflating: data/img/85930.png      \n",
            "  inflating: data/img/73596.png      \n",
            "  inflating: data/img/62981.png      \n",
            "  inflating: data/img/50624.png      \n",
            "  inflating: data/img/07356.png      \n",
            "  inflating: data/img/37621.png      \n",
            "  inflating: data/img/75934.png      \n",
            "  inflating: data/img/89320.png      \n",
            "  inflating: data/img/19406.png      \n",
            "  inflating: data/img/17493.png      \n",
            "  inflating: data/img/41206.png      \n",
            "  inflating: data/img/42379.png      \n",
            "  inflating: data/img/68423.png      \n",
            "  inflating: data/img/89362.png      \n",
            "  inflating: data/img/60289.png      \n",
            "  inflating: data/img/26571.png      \n",
            "  inflating: data/img/70842.png      \n",
            "  inflating: data/img/92417.png      \n",
            "  inflating: data/img/10234.png      \n",
            "  inflating: data/img/86509.png      \n",
            "  inflating: data/img/23018.png      \n",
            "  inflating: data/img/48731.png      \n",
            "  inflating: data/img/50198.png      \n",
            "  inflating: data/img/50428.png      \n",
            "  inflating: data/img/68913.png      \n",
            "  inflating: data/img/84379.png      \n",
            "  inflating: data/img/59806.png      \n",
            "  inflating: data/img/86753.png      \n",
            "  inflating: data/img/04683.png      \n",
            "  inflating: data/img/15798.png      \n",
            "  inflating: data/img/16075.png      \n",
            "  inflating: data/img/26374.png      \n",
            "  inflating: data/img/17832.png      \n",
            "  inflating: data/img/46213.png      \n",
            "  inflating: data/img/78619.png      \n",
            "  inflating: data/img/16903.png      \n",
            "  inflating: data/img/06345.png      \n",
            "  inflating: data/img/02654.png      \n",
            "  inflating: data/img/30549.png      \n",
            "  inflating: data/img/02795.png      \n",
            "  inflating: data/img/70426.png      \n",
            "  inflating: data/img/73849.png      \n",
            "  inflating: data/img/48310.png      \n",
            "  inflating: data/img/80714.png      \n",
            "  inflating: data/img/05276.png      \n",
            "  inflating: data/img/69178.png      \n",
            "  inflating: data/img/46087.png      \n",
            "  inflating: data/img/45201.png      \n",
            "  inflating: data/img/68043.png      \n",
            "  inflating: data/img/86043.png      \n",
            "  inflating: data/img/81962.png      \n",
            "  inflating: data/img/49327.png      \n",
            "  inflating: data/img/62541.png      \n",
            "  inflating: data/img/58467.png      \n",
            "  inflating: data/img/98461.png      \n",
            "  inflating: data/img/07285.png      \n",
            "  inflating: data/img/92075.png      \n",
            "  inflating: data/img/30251.png      \n",
            "  inflating: data/img/81645.png      \n",
            "  inflating: data/img/06213.png      \n",
            "  inflating: data/img/85923.png      \n",
            "  inflating: data/img/61492.png      \n",
            "  inflating: data/img/08291.png      \n",
            "  inflating: data/img/14869.png      \n",
            "  inflating: data/img/14268.png      \n",
            "  inflating: data/img/98567.png      \n",
            "  inflating: data/img/54019.png      \n",
            "  inflating: data/img/94205.png      \n",
            "  inflating: data/img/49083.png      \n",
            "  inflating: data/img/87539.png      \n",
            "  inflating: data/img/45368.png      \n",
            "  inflating: data/img/04758.png      \n",
            "  inflating: data/img/32560.png      \n",
            "  inflating: data/img/31609.png      \n",
            "  inflating: data/img/82657.png      \n",
            "  inflating: data/img/96380.png      \n",
            "  inflating: data/img/38215.png      \n",
            "  inflating: data/img/15746.png      \n",
            "  inflating: data/img/73568.png      \n",
            "  inflating: data/img/36021.png      \n",
            "  inflating: data/img/06985.png      \n",
            "  inflating: data/img/86127.png      \n",
            "  inflating: data/img/24860.png      \n",
            "  inflating: data/img/94870.png      \n",
            "  inflating: data/img/53740.png      \n",
            "  inflating: data/img/52631.png      \n",
            "  inflating: data/img/23407.png      \n",
            "  inflating: data/img/10967.png      \n",
            "  inflating: data/img/08564.png      \n",
            "  inflating: data/img/73259.png      \n",
            "  inflating: data/img/35796.png      \n",
            "  inflating: data/img/70841.png      \n",
            "  inflating: data/img/87160.png      \n",
            "  inflating: data/img/80397.png      \n",
            "  inflating: data/img/72084.png      \n",
            "  inflating: data/img/58210.png      \n",
            "  inflating: data/img/79146.png      \n",
            "  inflating: data/img/87619.png      \n",
            "  inflating: data/img/07164.png      \n",
            "  inflating: data/img/72301.png      \n",
            "  inflating: data/img/58071.png      \n",
            "  inflating: data/img/37296.png      \n",
            "  inflating: data/img/02439.png      \n",
            "  inflating: data/img/65024.png      \n",
            "  inflating: data/img/46237.png      \n",
            "  inflating: data/img/73218.png      \n",
            "  inflating: data/img/07984.png      \n",
            "  inflating: data/img/87610.png      \n",
            "  inflating: data/img/26984.png      \n",
            "  inflating: data/img/56908.png      \n",
            "  inflating: data/img/63028.png      \n",
            "  inflating: data/img/42589.png      \n",
            "  inflating: data/img/05294.png      \n",
            "  inflating: data/img/41589.png      \n",
            "  inflating: data/img/92768.png      \n",
            "  inflating: data/img/54038.png      \n",
            "  inflating: data/img/85346.png      \n",
            "  inflating: data/img/04127.png      \n",
            "  inflating: data/img/82501.png      \n",
            "  inflating: data/img/42591.png      \n",
            "  inflating: data/img/93184.png      \n",
            "  inflating: data/img/92546.png      \n",
            "  inflating: data/img/38071.png      \n",
            "  inflating: data/img/86795.png      \n",
            "  inflating: data/img/89542.png      \n",
            "  inflating: data/img/06534.png      \n",
            "  inflating: data/img/06524.png      \n",
            "  inflating: data/img/96071.png      \n",
            "  inflating: data/img/19586.png      \n",
            "  inflating: data/img/47386.png      \n",
            "  inflating: data/img/34751.png      \n",
            "  inflating: data/img/94107.png      \n",
            "  inflating: data/img/25908.png      \n",
            "  inflating: data/img/56290.png      \n",
            "  inflating: data/img/13624.png      \n",
            "  inflating: data/img/32765.png      \n",
            "  inflating: data/img/28431.png      \n",
            "  inflating: data/img/61298.png      \n",
            "  inflating: data/img/08657.png      \n",
            "  inflating: data/img/65871.png      \n",
            "  inflating: data/img/27801.png      \n",
            "  inflating: data/img/80152.png      \n",
            "  inflating: data/img/65743.png      \n",
            "  inflating: data/img/69418.png      \n",
            "  inflating: data/img/04256.png      \n",
            "  inflating: data/img/28317.png      \n",
            "  inflating: data/img/56248.png      \n",
            "  inflating: data/img/90321.png      \n",
            "  inflating: data/img/13049.png      \n",
            "  inflating: data/img/58129.png      \n",
            "  inflating: data/img/73549.png      \n",
            "  inflating: data/img/75209.png      \n",
            "  inflating: data/img/63285.png      \n",
            "  inflating: data/img/10963.png      \n",
            "  inflating: data/img/16827.png      \n",
            "  inflating: data/img/53481.png      \n",
            "  inflating: data/img/35780.png      \n",
            "  inflating: data/img/52318.png      \n",
            "  inflating: data/img/15270.png      \n",
            "  inflating: data/img/23094.png      \n",
            "  inflating: data/img/10358.png      \n",
            "  inflating: data/img/53471.png      \n",
            "  inflating: data/img/93275.png      \n",
            "  inflating: data/img/27806.png      \n",
            "  inflating: data/img/43271.png      \n",
            "  inflating: data/img/14569.png      \n",
            "  inflating: data/img/93512.png      \n",
            "  inflating: data/img/35912.png      \n",
            "  inflating: data/img/67415.png      \n",
            "  inflating: data/img/20617.png      \n",
            "  inflating: data/img/72418.png      \n",
            "  inflating: data/img/26497.png      \n",
            "  inflating: data/img/98103.png      \n",
            "  inflating: data/img/34602.png      \n",
            "  inflating: data/img/50286.png      \n",
            "  inflating: data/img/48136.png      \n",
            "  inflating: data/img/43759.png      \n",
            "  inflating: data/img/50317.png      \n",
            "  inflating: data/img/21497.png      \n",
            "  inflating: data/img/98427.png      \n",
            "  inflating: data/img/68592.png      \n",
            "  inflating: data/img/13460.png      \n",
            "  inflating: data/img/38019.png      \n",
            "  inflating: data/img/89754.png      \n",
            "  inflating: data/img/43206.png      \n",
            "  inflating: data/img/58246.png      \n",
            "  inflating: data/img/03567.png      \n",
            "  inflating: data/img/18504.png      \n",
            "  inflating: data/img/58130.png      \n",
            "  inflating: data/img/86215.png      \n",
            "  inflating: data/img/01469.png      \n",
            "  inflating: data/img/87634.png      \n",
            "  inflating: data/img/13469.png      \n",
            "  inflating: data/img/56149.png      \n",
            "  inflating: data/img/50784.png      \n",
            "  inflating: data/img/96381.png      \n",
            "  inflating: data/img/21047.png      \n",
            "  inflating: data/img/81263.png      \n",
            "  inflating: data/img/13450.png      \n",
            "  inflating: data/img/49208.png      \n",
            "  inflating: data/img/49836.png      \n",
            "  inflating: data/img/91754.png      \n",
            "  inflating: data/img/92038.png      \n",
            "  inflating: data/img/48296.png      \n",
            "  inflating: data/img/96037.png      \n",
            "  inflating: data/img/96821.png      \n",
            "  inflating: data/img/47926.png      \n",
            "  inflating: data/img/49615.png      \n",
            "  inflating: data/img/85496.png      \n",
            "  inflating: data/img/65940.png      \n",
            "  inflating: data/img/69158.png      \n",
            "  inflating: data/img/57913.png      \n",
            "  inflating: data/img/10283.png      \n",
            "  inflating: data/img/69503.png      \n",
            "  inflating: data/img/65203.png      \n",
            "  inflating: data/img/09124.png      \n",
            "  inflating: data/img/19684.png      \n",
            "  inflating: data/img/41806.png      \n",
            "  inflating: data/img/56719.png      \n",
            "  inflating: data/img/79352.png      \n",
            "  inflating: data/img/92573.png      \n",
            "  inflating: data/img/81257.png      \n",
            "  inflating: data/img/16207.png      \n",
            "  inflating: data/img/38475.png      \n",
            "  inflating: data/img/91853.png      \n",
            "  inflating: data/img/14360.png      \n",
            "  inflating: data/img/72364.png      \n",
            "  inflating: data/img/04892.png      \n",
            "  inflating: data/img/04718.png      \n",
            "  inflating: data/img/48715.png      \n",
            "  inflating: data/img/68401.png      \n",
            "  inflating: data/img/26839.png      \n",
            "  inflating: data/img/85291.png      \n",
            "  inflating: data/img/08173.png      \n",
            "  inflating: data/img/28463.png      \n",
            "  inflating: data/img/95640.png      \n",
            "  inflating: data/img/19230.png      \n",
            "  inflating: data/img/60938.png      \n",
            "  inflating: data/img/09834.png      \n",
            "  inflating: data/img/82950.png      \n",
            "  inflating: data/img/09657.png      \n",
            "  inflating: data/img/70623.png      \n",
            "  inflating: data/img/85136.png      \n",
            "  inflating: data/img/67512.png      \n",
            "  inflating: data/img/56910.png      \n",
            "  inflating: data/img/08294.png      \n",
            "  inflating: data/img/17938.png      \n",
            "  inflating: data/img/90127.png      \n",
            "  inflating: data/img/14052.png      \n",
            "  inflating: data/img/02831.png      \n",
            "  inflating: data/img/29735.png      \n",
            "  inflating: data/img/89602.png      \n",
            "  inflating: data/img/25634.png      \n",
            "  inflating: data/img/01937.png      \n",
            "  inflating: data/img/35902.png      \n",
            "  inflating: data/img/21704.png      \n",
            "  inflating: data/img/65892.png      \n",
            "  inflating: data/img/92317.png      \n",
            "  inflating: data/img/72605.png      \n",
            "  inflating: data/img/02481.png      \n",
            "  inflating: data/img/30859.png      \n",
            "  inflating: data/img/74658.png      \n",
            "  inflating: data/img/93016.png      \n",
            "  inflating: data/img/73506.png      \n",
            "  inflating: data/img/08653.png      \n",
            "  inflating: data/img/61892.png      \n",
            "  inflating: data/img/37190.png      \n",
            "  inflating: data/img/26983.png      \n",
            "  inflating: data/img/93251.png      \n",
            "  inflating: data/img/82165.png      \n",
            "  inflating: data/img/96342.png      \n",
            "  inflating: data/img/54023.png      \n",
            "  inflating: data/img/36910.png      \n",
            "  inflating: data/img/24791.png      \n",
            "  inflating: data/img/05749.png      \n",
            "  inflating: data/img/60794.png      \n",
            "  inflating: data/img/53210.png      \n",
            "  inflating: data/img/79182.png      \n",
            "  inflating: data/img/51708.png      \n",
            "  inflating: data/img/73498.png      \n",
            "  inflating: data/img/10258.png      \n",
            "  inflating: data/img/56294.png      \n",
            "  inflating: data/img/54927.png      \n",
            "  inflating: data/img/06845.png      \n",
            "  inflating: data/img/59617.png      \n",
            "  inflating: data/img/08297.png      \n",
            "  inflating: data/img/29063.png      \n",
            "  inflating: data/img/56189.png      \n",
            "  inflating: data/img/17398.png      \n",
            "  inflating: data/img/84269.png      \n",
            "  inflating: data/img/65312.png      \n",
            "  inflating: data/img/75092.png      \n",
            "  inflating: data/img/65732.png      \n",
            "  inflating: data/img/01765.png      \n",
            "  inflating: data/img/67142.png      \n",
            "  inflating: data/img/78469.png      \n",
            "  inflating: data/img/97345.png      \n",
            "  inflating: data/img/02416.png      \n",
            "  inflating: data/img/49758.png      \n",
            "  inflating: data/img/76138.png      \n",
            "  inflating: data/img/46150.png      \n",
            "  inflating: data/img/03968.png      \n",
            "  inflating: data/img/52469.png      \n",
            "  inflating: data/img/24853.png      \n",
            "  inflating: data/img/94687.png      \n",
            "  inflating: data/img/67250.png      \n",
            "  inflating: data/img/84759.png      \n",
            "  inflating: data/img/57302.png      \n",
            "  inflating: data/img/91453.png      \n",
            "  inflating: data/img/60513.png      \n",
            "  inflating: data/img/06847.png      \n",
            "  inflating: data/img/95718.png      \n",
            "  inflating: data/img/47918.png      \n",
            "  inflating: data/img/57389.png      \n",
            "  inflating: data/img/90573.png      \n",
            "  inflating: data/img/72830.png      \n",
            "  inflating: data/img/53261.png      \n",
            "  inflating: data/img/42086.png      \n",
            "  inflating: data/img/69140.png      \n",
            "  inflating: data/img/41578.png      \n",
            "  inflating: data/img/83756.png      \n",
            "  inflating: data/img/19542.png      \n",
            "  inflating: data/img/35091.png      \n",
            "  inflating: data/img/75198.png      \n",
            "  inflating: data/img/92146.png      \n",
            "  inflating: data/img/82914.png      \n",
            "  inflating: data/img/97465.png      \n",
            "  inflating: data/img/04918.png      \n",
            "  inflating: data/img/04726.png      \n",
            "  inflating: data/img/96312.png      \n",
            "  inflating: data/img/34152.png      \n",
            "  inflating: data/img/94230.png      \n",
            "  inflating: data/img/13765.png      \n",
            "  inflating: data/img/91075.png      \n",
            "  inflating: data/img/76305.png      \n",
            "  inflating: data/img/46827.png      \n",
            "  inflating: data/img/91037.png      \n",
            "  inflating: data/img/42975.png      \n",
            "  inflating: data/img/83476.png      \n",
            "  inflating: data/img/27195.png      \n",
            "  inflating: data/img/87526.png      \n",
            "  inflating: data/img/14327.png      \n",
            "  inflating: data/img/75193.png      \n",
            "  inflating: data/img/10254.png      \n",
            "  inflating: data/img/58136.png      \n",
            "  inflating: data/img/72638.png      \n",
            "  inflating: data/img/86072.png      \n",
            "  inflating: data/img/74825.png      \n",
            "  inflating: data/img/07215.png      \n",
            "  inflating: data/img/91072.png      \n",
            "  inflating: data/img/34678.png      \n",
            "  inflating: data/img/03798.png      \n",
            "  inflating: data/img/59072.png      \n",
            "  inflating: data/img/87532.png      \n",
            "  inflating: data/img/95064.png      \n",
            "  inflating: data/img/43295.png      \n",
            "  inflating: data/img/72061.png      \n",
            "  inflating: data/img/90846.png      \n",
            "  inflating: data/img/58069.png      \n",
            "  inflating: data/img/17682.png      \n",
            "  inflating: data/img/37425.png      \n",
            "  inflating: data/img/63827.png      \n",
            "  inflating: data/img/07956.png      \n",
            "  inflating: data/img/82730.png      \n",
            "  inflating: data/img/92450.png      \n",
            "  inflating: data/img/78563.png      \n",
            "  inflating: data/img/46051.png      \n",
            "  inflating: data/img/73026.png      \n",
            "  inflating: data/img/80534.png      \n",
            "  inflating: data/img/45029.png      \n",
            "  inflating: data/img/85612.png      \n",
            "  inflating: data/img/32147.png      \n",
            "  inflating: data/img/24730.png      \n",
            "  inflating: data/img/56392.png      \n",
            "  inflating: data/img/76924.png      \n",
            "  inflating: data/img/18029.png      \n",
            "  inflating: data/img/10389.png      \n",
            "  inflating: data/img/12603.png      \n",
            "  inflating: data/img/54801.png      \n",
            "  inflating: data/img/21849.png      \n",
            "  inflating: data/img/97685.png      \n",
            "  inflating: data/img/68379.png      \n",
            "  inflating: data/img/14097.png      \n",
            "  inflating: data/img/76951.png      \n",
            "  inflating: data/img/31975.png      \n",
            "  inflating: data/img/76821.png      \n",
            "  inflating: data/img/65183.png      \n",
            "  inflating: data/img/57164.png      \n",
            "  inflating: data/img/49165.png      \n",
            "  inflating: data/img/51493.png      \n",
            "  inflating: data/img/13542.png      \n",
            "  inflating: data/img/98064.png      \n",
            "  inflating: data/img/74391.png      \n",
            "  inflating: data/img/67435.png      \n",
            "  inflating: data/img/30942.png      \n",
            "  inflating: data/img/39416.png      \n",
            "  inflating: data/img/49652.png      \n",
            "  inflating: data/img/86257.png      \n",
            "  inflating: data/img/59832.png      \n",
            "  inflating: data/img/75291.png      \n",
            "  inflating: data/img/64391.png      \n",
            "  inflating: data/img/05436.png      \n",
            "  inflating: data/img/01756.png      \n",
            "  inflating: data/img/05841.png      \n",
            "  inflating: data/img/12035.png      \n",
            "  inflating: data/img/04819.png      \n",
            "  inflating: data/img/37580.png      \n",
            "  inflating: data/img/28517.png      \n",
            "  inflating: data/img/75832.png      \n",
            "  inflating: data/img/40726.png      \n",
            "  inflating: data/img/82079.png      \n",
            "  inflating: data/img/10463.png      \n",
            "  inflating: data/img/94130.png      \n",
            "  inflating: data/img/54782.png      \n",
            "  inflating: data/img/52936.png      \n",
            "  inflating: data/img/08495.png      \n",
            "  inflating: data/img/69815.png      \n",
            "  inflating: data/img/68459.png      \n",
            "  inflating: data/img/51894.png      \n",
            "  inflating: data/img/08671.png      \n",
            "  inflating: data/img/72145.png      \n",
            "  inflating: data/img/93605.png      \n",
            "  inflating: data/img/92831.png      \n",
            "  inflating: data/img/30862.png      \n",
            "  inflating: data/img/63718.png      \n",
            "  inflating: data/img/49328.png      \n",
            "  inflating: data/img/62913.png      \n",
            "  inflating: data/img/52037.png      \n",
            "  inflating: data/img/38154.png      \n",
            "  inflating: data/img/76890.png      \n",
            "  inflating: data/img/17508.png      \n",
            "  inflating: data/img/50793.png      \n",
            "  inflating: data/img/38956.png      \n",
            "  inflating: data/img/79321.png      \n",
            "  inflating: data/img/32491.png      \n",
            "  inflating: data/img/39175.png      \n",
            "  inflating: data/img/18940.png      \n",
            "  inflating: data/img/79125.png      \n",
            "  inflating: data/img/18739.png      \n",
            "  inflating: data/img/40761.png      \n",
            "  inflating: data/img/75102.png      \n",
            "  inflating: data/img/74502.png      \n",
            "  inflating: data/img/91358.png      \n",
            "  inflating: data/img/12374.png      \n",
            "  inflating: data/img/95326.png      \n",
            "  inflating: data/img/78539.png      \n",
            "  inflating: data/img/07849.png      \n",
            "  inflating: data/img/76532.png      \n",
            "  inflating: data/img/38465.png      \n",
            "  inflating: data/img/28401.png      \n",
            "  inflating: data/img/95081.png      \n",
            "  inflating: data/img/52903.png      \n",
            "  inflating: data/img/90128.png      \n",
            "  inflating: data/img/30958.png      \n",
            "  inflating: data/img/09523.png      \n",
            "  inflating: data/img/35678.png      \n",
            "  inflating: data/img/32794.png      \n",
            "  inflating: data/img/37140.png      \n",
            "  inflating: data/img/72450.png      \n",
            "  inflating: data/img/60913.png      \n",
            "  inflating: data/img/93820.png      \n",
            "  inflating: data/img/40539.png      \n",
            "  inflating: data/img/61092.png      \n",
            "  inflating: data/img/51270.png      \n",
            "  inflating: data/img/08254.png      \n",
            "  inflating: data/img/84061.png      \n",
            "  inflating: data/img/98067.png      \n",
            "  inflating: data/img/85614.png      \n",
            "  inflating: data/img/97524.png      \n",
            "  inflating: data/img/86491.png      \n",
            "  inflating: data/img/83427.png      \n",
            "  inflating: data/img/96507.png      \n",
            "  inflating: data/img/84703.png      \n",
            "  inflating: data/img/23564.png      \n",
            "  inflating: data/img/07824.png      \n",
            "  inflating: data/img/71938.png      \n",
            "  inflating: data/img/64078.png      \n",
            "  inflating: data/img/42903.png      \n",
            "  inflating: data/img/43078.png      \n",
            "  inflating: data/img/38756.png      \n",
            "  inflating: data/img/67328.png      \n",
            "  inflating: data/img/50739.png      \n",
            "  inflating: data/img/51076.png      \n",
            "  inflating: data/img/80521.png      \n",
            "  inflating: data/img/71302.png      \n",
            "  inflating: data/img/06531.png      \n",
            "  inflating: data/img/56708.png      \n",
            "  inflating: data/img/06458.png      \n",
            "  inflating: data/img/51376.png      \n",
            "  inflating: data/img/86253.png      \n",
            "  inflating: data/img/03528.png      \n",
            "  inflating: data/img/20718.png      \n",
            "  inflating: data/img/89153.png      \n",
            "  inflating: data/img/12678.png      \n",
            "  inflating: data/img/70825.png      \n",
            "  inflating: data/img/76483.png      \n",
            "  inflating: data/img/38051.png      \n",
            "  inflating: data/img/21809.png      \n",
            "  inflating: data/img/30175.png      \n",
            "  inflating: data/img/97204.png      \n",
            "  inflating: data/img/47629.png      \n",
            "  inflating: data/img/78592.png      \n",
            "  inflating: data/img/53649.png      \n",
            "  inflating: data/img/20194.png      \n",
            "  inflating: data/img/14570.png      \n",
            "  inflating: data/img/40256.png      \n",
            "  inflating: data/img/75920.png      \n",
            "  inflating: data/img/60345.png      \n",
            "  inflating: data/img/81532.png      \n",
            "  inflating: data/img/59871.png      \n",
            "  inflating: data/img/05874.png      \n",
            "  inflating: data/img/04621.png      \n",
            "  inflating: data/img/27304.png      \n",
            "  inflating: data/img/53268.png      \n",
            "  inflating: data/img/95278.png      \n",
            "  inflating: data/img/26795.png      \n",
            "  inflating: data/img/27634.png      \n",
            "  inflating: data/img/24061.png      \n",
            "  inflating: data/img/20318.png      \n",
            "  inflating: data/img/54708.png      \n",
            "  inflating: data/img/30721.png      \n",
            "  inflating: data/img/14823.png      \n",
            "  inflating: data/img/26057.png      \n",
            "  inflating: data/img/41986.png      \n",
            "  inflating: data/img/56138.png      \n",
            "  inflating: data/img/42830.png      \n",
            "  inflating: data/img/74192.png      \n",
            "  inflating: data/img/03241.png      \n",
            "  inflating: data/img/89756.png      \n",
            "  inflating: data/img/53096.png      \n",
            "  inflating: data/img/58190.png      \n",
            "  inflating: data/img/25473.png      \n",
            "  inflating: data/img/07385.png      \n",
            "  inflating: data/img/61208.png      \n",
            "  inflating: data/img/92785.png      \n",
            "  inflating: data/img/09768.png      \n",
            "  inflating: data/img/01726.png      \n",
            "  inflating: data/img/61382.png      \n",
            "  inflating: data/img/71584.png      \n",
            "  inflating: data/img/70851.png      \n",
            "  inflating: data/img/01382.png      \n",
            "  inflating: data/img/47359.png      \n",
            "  inflating: data/img/12453.png      \n",
            "  inflating: data/img/87530.png      \n",
            "  inflating: data/img/42175.png      \n",
            "  inflating: data/img/62354.png      \n",
            "  inflating: data/img/41092.png      \n",
            "  inflating: data/img/21706.png      \n",
            "  inflating: data/img/07516.png      \n",
            "  inflating: data/img/32605.png      \n",
            "  inflating: data/img/59463.png      \n",
            "  inflating: data/img/17028.png      \n",
            "  inflating: data/img/38927.png      \n",
            "  inflating: data/img/64390.png      \n",
            "  inflating: data/img/86194.png      \n",
            "  inflating: data/img/35716.png      \n",
            "  inflating: data/img/23615.png      \n",
            "  inflating: data/img/93068.png      \n",
            "  inflating: data/img/62057.png      \n",
            "  inflating: data/img/18946.png      \n",
            "  inflating: data/img/83461.png      \n",
            "  inflating: data/img/95672.png      \n",
            "  inflating: data/img/98305.png      \n",
            "  inflating: data/img/94802.png      \n",
            "  inflating: data/img/79384.png      \n",
            "  inflating: data/img/29357.png      \n",
            "  inflating: data/img/89326.png      \n",
            "  inflating: data/img/05872.png      \n",
            "  inflating: data/img/03289.png      \n",
            "  inflating: data/img/82941.png      \n",
            "  inflating: data/img/82635.png      \n",
            "  inflating: data/img/21957.png      \n",
            "  inflating: data/img/43180.png      \n",
            "  inflating: data/img/25137.png      \n",
            "  inflating: data/img/51437.png      \n",
            "  inflating: data/img/47015.png      \n",
            "  inflating: data/img/63908.png      \n",
            "  inflating: data/img/45698.png      \n",
            "  inflating: data/img/38461.png      \n",
            "  inflating: data/img/37948.png      \n",
            "  inflating: data/img/84921.png      \n",
            "  inflating: data/img/56712.png      \n",
            "  inflating: data/img/67312.png      \n",
            "  inflating: data/img/71906.png      \n",
            "  inflating: data/img/47128.png      \n",
            "  inflating: data/img/85417.png      \n",
            "  inflating: data/img/57630.png      \n",
            "  inflating: data/img/96407.png      \n",
            "  inflating: data/img/20936.png      \n",
            "  inflating: data/img/26095.png      \n",
            "  inflating: data/img/28197.png      \n",
            "  inflating: data/img/80172.png      \n",
            "  inflating: data/img/58604.png      \n",
            "  inflating: data/img/12973.png      \n",
            "  inflating: data/img/93287.png      \n",
            "  inflating: data/img/98654.png      \n",
            "  inflating: data/img/62357.png      \n",
            "  inflating: data/img/17834.png      \n",
            "  inflating: data/img/57490.png      \n",
            "  inflating: data/img/21598.png      \n",
            "  inflating: data/img/30597.png      \n",
            "  inflating: data/img/20867.png      \n",
            "  inflating: data/img/12793.png      \n",
            "  inflating: data/img/60143.png      \n",
            "  inflating: data/img/45802.png      \n",
            "  inflating: data/img/52487.png      \n",
            "  inflating: data/img/89540.png      \n",
            "  inflating: data/img/16538.png      \n",
            "  inflating: data/img/58672.png      \n",
            "  inflating: data/img/28397.png      \n",
            "  inflating: data/img/75362.png      \n",
            "  inflating: data/img/72965.png      \n",
            "  inflating: data/img/09867.png      \n",
            "  inflating: data/img/41720.png      \n",
            "  inflating: data/img/34072.png      \n",
            "  inflating: data/img/19256.png      \n",
            "  inflating: data/img/91627.png      \n",
            "  inflating: data/img/90723.png      \n",
            "  inflating: data/img/60971.png      \n",
            "  inflating: data/img/94156.png      \n",
            "  inflating: data/img/43895.png      \n",
            "  inflating: data/img/03275.png      \n",
            "  inflating: data/img/80156.png      \n",
            "  inflating: data/img/26183.png      \n",
            "  inflating: data/img/43127.png      \n",
            "  inflating: data/img/13970.png      \n",
            "  inflating: data/img/20745.png      \n",
            "  inflating: data/img/43521.png      \n",
            "  inflating: data/img/21067.png      \n",
            "  inflating: data/img/68259.png      \n",
            "  inflating: data/img/59718.png      \n",
            "  inflating: data/img/68973.png      \n",
            "  inflating: data/img/35801.png      \n",
            "  inflating: data/img/03547.png      \n",
            "  inflating: data/img/15439.png      \n",
            "  inflating: data/img/50743.png      \n",
            "  inflating: data/img/17809.png      \n",
            "  inflating: data/img/09482.png      \n",
            "  inflating: data/img/67413.png      \n",
            "  inflating: data/img/53879.png      \n",
            "  inflating: data/img/27395.png      \n",
            "  inflating: data/img/46790.png      \n",
            "  inflating: data/img/96305.png      \n",
            "  inflating: data/img/97531.png      \n",
            "  inflating: data/img/20498.png      \n",
            "  inflating: data/img/15938.png      \n",
            "  inflating: data/img/54790.png      \n",
            "  inflating: data/img/50764.png      \n",
            "  inflating: data/img/18246.png      \n",
            "  inflating: data/img/26905.png      \n",
            "  inflating: data/img/79461.png      \n",
            "  inflating: data/img/46387.png      \n",
            "  inflating: data/img/26189.png      \n",
            "  inflating: data/img/86543.png      \n",
            "  inflating: data/img/52617.png      \n",
            "  inflating: data/img/01749.png      \n",
            "  inflating: data/img/41382.png      \n",
            "  inflating: data/img/10475.png      \n",
            "  inflating: data/img/01546.png      \n",
            "  inflating: data/img/82637.png      \n",
            "  inflating: data/img/07912.png      \n",
            "  inflating: data/img/23745.png      \n",
            "  inflating: data/img/62850.png      \n",
            "  inflating: data/img/04536.png      \n",
            "  inflating: data/img/39658.png      \n",
            "  inflating: data/img/92683.png      \n",
            "  inflating: data/img/34291.png      \n",
            "  inflating: data/img/05784.png      \n",
            "  inflating: data/img/81975.png      \n",
            "  inflating: data/img/59028.png      \n",
            "  inflating: data/img/72354.png      \n",
            "  inflating: data/img/47385.png      \n",
            "  inflating: data/img/43162.png      \n",
            "  inflating: data/img/91486.png      \n",
            "  inflating: data/img/52104.png      \n",
            "  inflating: data/img/19647.png      \n",
            "  inflating: data/img/38259.png      \n",
            "  inflating: data/img/84153.png      \n",
            "  inflating: data/img/59621.png      \n",
            "  inflating: data/img/19523.png      \n",
            "  inflating: data/img/17453.png      \n",
            "  inflating: data/img/02653.png      \n",
            "  inflating: data/img/58026.png      \n",
            "  inflating: data/img/74625.png      \n",
            "  inflating: data/img/56972.png      \n",
            "  inflating: data/img/25310.png      \n",
            "  inflating: data/img/58209.png      \n",
            "  inflating: data/img/63579.png      \n",
            "  inflating: data/img/72035.png      \n",
            "  inflating: data/img/90587.png      \n",
            "  inflating: data/img/01954.png      \n",
            "  inflating: data/img/05261.png      \n",
            "  inflating: data/img/58061.png      \n",
            "  inflating: data/img/23401.png      \n",
            "  inflating: data/img/28360.png      \n",
            "  inflating: data/img/17908.png      \n",
            "  inflating: data/img/46715.png      \n",
            "  inflating: data/img/36092.png      \n",
            "  inflating: data/img/86453.png      \n",
            "  inflating: data/img/58471.png      \n",
            "  inflating: data/img/46198.png      \n",
            "  inflating: data/img/59682.png      \n",
            "  inflating: data/img/32168.png      \n",
            "  inflating: data/img/37609.png      \n",
            "  inflating: data/img/10692.png      \n",
            "  inflating: data/img/02943.png      \n",
            "  inflating: data/img/46785.png      \n",
            "  inflating: data/img/89745.png      \n",
            "  inflating: data/img/14695.png      \n",
            "  inflating: data/img/63758.png      \n",
            "  inflating: data/img/62019.png      \n",
            "  inflating: data/img/71354.png      \n",
            "  inflating: data/img/09321.png      \n",
            "  inflating: data/img/32906.png      \n",
            "  inflating: data/img/90756.png      \n",
            "  inflating: data/img/61094.png      \n",
            "  inflating: data/img/34067.png      \n",
            "  inflating: data/img/97816.png      \n",
            "  inflating: data/img/57412.png      \n",
            "  inflating: data/img/81097.png      \n",
            "  inflating: data/img/70932.png      \n",
            "  inflating: data/img/02594.png      \n",
            "  inflating: data/img/59370.png      \n",
            "  inflating: data/img/72658.png      \n",
            "  inflating: data/img/45120.png      \n",
            "  inflating: data/img/64820.png      \n",
            "  inflating: data/img/10625.png      \n",
            "  inflating: data/img/03495.png      \n",
            "  inflating: data/img/92481.png      \n",
            "  inflating: data/img/84670.png      \n",
            "  inflating: data/img/65832.png      \n",
            "  inflating: data/img/71486.png      \n",
            "  inflating: data/img/61723.png      \n",
            "  inflating: data/img/57496.png      \n",
            "  inflating: data/img/12548.png      \n",
            "  inflating: data/img/91764.png      \n",
            "  inflating: data/img/52416.png      \n",
            "  inflating: data/img/39265.png      \n",
            "  inflating: data/img/68975.png      \n",
            "  inflating: data/img/41538.png      \n",
            "  inflating: data/img/52476.png      \n",
            "  inflating: data/img/10285.png      \n",
            "  inflating: data/img/60357.png      \n",
            "  inflating: data/img/98531.png      \n",
            "  inflating: data/img/87063.png      \n",
            "  inflating: data/img/51482.png      \n",
            "  inflating: data/img/15290.png      \n",
            "  inflating: data/img/47031.png      \n",
            "  inflating: data/img/49076.png      \n",
            "  inflating: data/img/71524.png      \n",
            "  inflating: data/img/63412.png      \n",
            "  inflating: data/img/74236.png      \n",
            "  inflating: data/img/69235.png      \n",
            "  inflating: data/img/05164.png      \n",
            "  inflating: data/img/16372.png      \n",
            "  inflating: data/img/15649.png      \n",
            "  inflating: data/img/91273.png      \n",
            "  inflating: data/img/19320.png      \n",
            "  inflating: data/img/47530.png      \n",
            "  inflating: data/img/46097.png      \n",
            "  inflating: data/img/90718.png      \n",
            "  inflating: data/img/90643.png      \n",
            "  inflating: data/img/93084.png      \n",
            "  inflating: data/img/47591.png      \n",
            "  inflating: data/img/91468.png      \n",
            "  inflating: data/img/12956.png      \n",
            "  inflating: data/img/54973.png      \n",
            "  inflating: data/img/73128.png      \n",
            "  inflating: data/img/35086.png      \n",
            "  inflating: data/img/09162.png      \n",
            "  inflating: data/img/02793.png      \n",
            "  inflating: data/img/30259.png      \n",
            "  inflating: data/img/71942.png      \n",
            "  inflating: data/img/34506.png      \n",
            "  inflating: data/img/08172.png      \n",
            "  inflating: data/img/62439.png      \n",
            "  inflating: data/img/85947.png      \n",
            "  inflating: data/img/26014.png      \n",
            "  inflating: data/img/64015.png      \n",
            "  inflating: data/img/21408.png      \n",
            "  inflating: data/img/50614.png      \n",
            "  inflating: data/img/59307.png      \n",
            "  inflating: data/img/35607.png      \n",
            "  inflating: data/img/63207.png      \n",
            "  inflating: data/img/05976.png      \n",
            "  inflating: data/img/93148.png      \n",
            "  inflating: data/img/13289.png      \n",
            "  inflating: data/img/19248.png      \n",
            "  inflating: data/img/48271.png      \n",
            "  inflating: data/img/06987.png      \n",
            "  inflating: data/img/78219.png      \n",
            "  inflating: data/img/73562.png      \n",
            "  inflating: data/img/28610.png      \n",
            "  inflating: data/img/36982.png      \n",
            "  inflating: data/img/93085.png      \n",
            "  inflating: data/img/08563.png      \n",
            "  inflating: data/img/51367.png      \n",
            "  inflating: data/img/48513.png      \n",
            "  inflating: data/img/32174.png      \n",
            "  inflating: data/img/15906.png      \n",
            "  inflating: data/img/27568.png      \n",
            "  inflating: data/img/08613.png      \n",
            "  inflating: data/img/28601.png      \n",
            "  inflating: data/img/06375.png      \n",
            "  inflating: data/img/30961.png      \n",
            "  inflating: data/img/58716.png      \n",
            "  inflating: data/img/27461.png      \n",
            "  inflating: data/img/64175.png      \n",
            "  inflating: data/img/61532.png      \n",
            "  inflating: data/img/71392.png      \n",
            "  inflating: data/img/83061.png      \n",
            "  inflating: data/img/71943.png      \n",
            "  inflating: data/img/56819.png      \n",
            "  inflating: data/img/29706.png      \n",
            "  inflating: data/img/35210.png      \n",
            "  inflating: data/img/07931.png      \n",
            "  inflating: data/img/29051.png      \n",
            "  inflating: data/img/73652.png      \n",
            "  inflating: data/img/53814.png      \n",
            "  inflating: data/img/67084.png      \n",
            "  inflating: data/img/27401.png      \n",
            "  inflating: data/img/97842.png      \n",
            "  inflating: data/img/68213.png      \n",
            "  inflating: data/img/79850.png      \n",
            "  inflating: data/img/08243.png      \n",
            "  inflating: data/img/64532.png      \n",
            "  inflating: data/img/48705.png      \n",
            "  inflating: data/img/34508.png      \n",
            "  inflating: data/img/02153.png      \n",
            "  inflating: data/img/01579.png      \n",
            "  inflating: data/img/46832.png      \n",
            "  inflating: data/img/97236.png      \n",
            "  inflating: data/img/49618.png      \n",
            "  inflating: data/img/12968.png      \n",
            "  inflating: data/img/52960.png      \n",
            "  inflating: data/img/25718.png      \n",
            "  inflating: data/img/09241.png      \n",
            "  inflating: data/img/05931.png      \n",
            "  inflating: data/img/05129.png      \n",
            "  inflating: data/img/59041.png      \n",
            "  inflating: data/img/35861.png      \n",
            "  inflating: data/img/97162.png      \n",
            "  inflating: data/img/07354.png      \n",
            "  inflating: data/img/02185.png      \n",
            "  inflating: data/img/93462.png      \n",
            "  inflating: data/img/48356.png      \n",
            "  inflating: data/img/46359.png      \n",
            "  inflating: data/img/13402.png      \n",
            "  inflating: data/img/31960.png      \n",
            "  inflating: data/img/27956.png      \n",
            "  inflating: data/img/28691.png      \n",
            "  inflating: data/img/76095.png      \n",
            "  inflating: data/img/32608.png      \n",
            "  inflating: data/img/76219.png      \n",
            "  inflating: data/img/71364.png      \n",
            "  inflating: data/img/45016.png      \n",
            "  inflating: data/img/53714.png      \n",
            "  inflating: data/img/09462.png      \n",
            "  inflating: data/img/70941.png      \n",
            "  inflating: data/img/82651.png      \n",
            "  inflating: data/img/51460.png      \n",
            "  inflating: data/img/04623.png      \n",
            "  inflating: data/img/36204.png      \n",
            "  inflating: data/img/70285.png      \n",
            "  inflating: data/img/13592.png      \n",
            "  inflating: data/img/03124.png      \n",
            "  inflating: data/img/56749.png      \n",
            "  inflating: data/img/97348.png      \n",
            "  inflating: data/img/93271.png      \n",
            "  inflating: data/img/96514.png      \n",
            "  inflating: data/img/91042.png      \n",
            "  inflating: data/img/68720.png      \n",
            "  inflating: data/img/43971.png      \n",
            "  inflating: data/img/98601.png      \n",
            "  inflating: data/img/21853.png      \n",
            "  inflating: data/img/15948.png      \n",
            "  inflating: data/img/20643.png      \n",
            "  inflating: data/img/95086.png      \n",
            "  inflating: data/img/57128.png      \n",
            "  inflating: data/img/85314.png      \n",
            "  inflating: data/img/36248.png      \n",
            "  inflating: data/img/64029.png      \n",
            "  inflating: data/img/37298.png      \n",
            "  inflating: data/img/97153.png      \n",
            "  inflating: data/img/46053.png      \n",
            "  inflating: data/img/19326.png      \n",
            "  inflating: data/img/69823.png      \n",
            "  inflating: data/img/43269.png      \n",
            "  inflating: data/img/27861.png      \n",
            "  inflating: data/img/60132.png      \n",
            "  inflating: data/img/49856.png      \n",
            "  inflating: data/img/58924.png      \n",
            "  inflating: data/img/01634.png      \n",
            "  inflating: data/img/01627.png      \n",
            "  inflating: data/img/62804.png      \n",
            "  inflating: data/img/40621.png      \n",
            "  inflating: data/img/75239.png      \n",
            "  inflating: data/img/14602.png      \n",
            "  inflating: data/img/53624.png      \n",
            "  inflating: data/img/91428.png      \n",
            "  inflating: data/img/43716.png      \n",
            "  inflating: data/img/23154.png      \n",
            "  inflating: data/img/35178.png      \n",
            "  inflating: data/img/79832.png      \n",
            "  inflating: data/img/73980.png      \n",
            "  inflating: data/img/40752.png      \n",
            "  inflating: data/img/68192.png      \n",
            "  inflating: data/img/42759.png      \n",
            "  inflating: data/img/70231.png      \n",
            "  inflating: data/img/61840.png      \n",
            "  inflating: data/img/76459.png      \n",
            "  inflating: data/img/70835.png      \n",
            "  inflating: data/img/93172.png      \n",
            "  inflating: data/img/46538.png      \n",
            "  inflating: data/img/74901.png      \n",
            "  inflating: data/img/32514.png      \n",
            "  inflating: data/img/36197.png      \n",
            "  inflating: data/img/80153.png      \n",
            "  inflating: data/img/19705.png      \n",
            "  inflating: data/img/08645.png      \n",
            "  inflating: data/img/93524.png      \n",
            "  inflating: data/img/02973.png      \n",
            "  inflating: data/img/56830.png      \n",
            "  inflating: data/img/91602.png      \n",
            "  inflating: data/img/12873.png      \n",
            "  inflating: data/img/81972.png      \n",
            "  inflating: data/img/68201.png      \n",
            "  inflating: data/img/43702.png      \n",
            "  inflating: data/img/29478.png      \n",
            "  inflating: data/img/28637.png      \n",
            "  inflating: data/img/95874.png      \n",
            "  inflating: data/img/45732.png      \n",
            "  inflating: data/img/58023.png      \n",
            "  inflating: data/img/85927.png      \n",
            "  inflating: data/img/05864.png      \n",
            "  inflating: data/img/25841.png      \n",
            "  inflating: data/img/78532.png      \n",
            "  inflating: data/img/06547.png      \n",
            "  inflating: data/img/94625.png      \n",
            "  inflating: data/img/23810.png      \n",
            "  inflating: data/img/48579.png      \n",
            "  inflating: data/img/17294.png      \n",
            "  inflating: data/img/85237.png      \n",
            "  inflating: data/img/01423.png      \n",
            "  inflating: data/img/73125.png      \n",
            "  inflating: data/img/08234.png      \n",
            "  inflating: data/img/04765.png      \n",
            "  inflating: data/img/38741.png      \n",
            "  inflating: data/img/97258.png      \n",
            "  inflating: data/img/34609.png      \n",
            "  inflating: data/img/56249.png      \n",
            "  inflating: data/img/30168.png      \n",
            "  inflating: data/img/48153.png      \n",
            "  inflating: data/img/89325.png      \n",
            "  inflating: data/img/48927.png      \n",
            "  inflating: data/img/42507.png      \n",
            "  inflating: data/img/34219.png      \n",
            "  inflating: data/img/82396.png      \n",
            "  inflating: data/img/18294.png      \n",
            "  inflating: data/img/72904.png      \n",
            "  inflating: data/img/59124.png      \n",
            "  inflating: data/img/81679.png      \n",
            "  inflating: data/img/75320.png      \n",
            "  inflating: data/img/84021.png      \n",
            "  inflating: data/img/70496.png      \n",
            "  inflating: data/img/16208.png      \n",
            "  inflating: data/img/21405.png      \n",
            "  inflating: data/img/48520.png      \n",
            "  inflating: data/img/69305.png      \n",
            "  inflating: data/img/30948.png      \n",
            "  inflating: data/img/10786.png      \n",
            "  inflating: data/img/07382.png      \n",
            "  inflating: data/img/89437.png      \n",
            "  inflating: data/img/35746.png      \n",
            "  inflating: data/img/42083.png      \n",
            "  inflating: data/img/65037.png      \n",
            "  inflating: data/img/51348.png      \n",
            "  inflating: data/img/37649.png      \n",
            "  inflating: data/img/47693.png      \n",
            "  inflating: data/img/81692.png      \n",
            "  inflating: data/img/73416.png      \n",
            "  inflating: data/img/90365.png      \n",
            "  inflating: data/img/69423.png      \n",
            "  inflating: data/img/83497.png      \n",
            "  inflating: data/img/57923.png      \n",
            "  inflating: data/img/96150.png      \n",
            "  inflating: data/img/14079.png      \n",
            "  inflating: data/img/23581.png      \n",
            "  inflating: data/img/71095.png      \n",
            "  inflating: data/img/72430.png      \n",
            "  inflating: data/img/16857.png      \n",
            "  inflating: data/img/04563.png      \n",
            "  inflating: data/img/53492.png      \n",
            "  inflating: data/img/91270.png      \n",
            "  inflating: data/img/41325.png      \n",
            "  inflating: data/img/69780.png      \n",
            "  inflating: data/img/49750.png      \n",
            "  inflating: data/img/92104.png      \n",
            "  inflating: data/img/40792.png      \n",
            "  inflating: data/img/46172.png      \n",
            "  inflating: data/img/18542.png      \n",
            "  inflating: data/img/51309.png      \n",
            "  inflating: data/img/71249.png      \n",
            "  inflating: data/img/10289.png      \n",
            "  inflating: data/img/43085.png      \n",
            "  inflating: data/img/04265.png      \n",
            "  inflating: data/img/40563.png      \n",
            "  inflating: data/img/04926.png      \n",
            "  inflating: data/img/84097.png      \n",
            "  inflating: data/img/17086.png      \n",
            "  inflating: data/img/67125.png      \n",
            "  inflating: data/img/63210.png      \n",
            "  inflating: data/img/94378.png      \n",
            "  inflating: data/img/41387.png      \n",
            "  inflating: data/img/06412.png      \n",
            "  inflating: data/img/10976.png      \n",
            "  inflating: data/img/92341.png      \n",
            "  inflating: data/img/83794.png      \n",
            "  inflating: data/img/34209.png      \n",
            "  inflating: data/img/81752.png      \n",
            "  inflating: data/img/40268.png      \n",
            "  inflating: data/img/25461.png      \n",
            "  inflating: data/img/97320.png      \n",
            "  inflating: data/img/62739.png      \n",
            "  inflating: data/img/38164.png      \n",
            "  inflating: data/img/42015.png      \n",
            "  inflating: data/img/75648.png      \n",
            "  inflating: data/img/34910.png      \n",
            "  inflating: data/img/24658.png      \n",
            "  inflating: data/img/96417.png      \n",
            "  inflating: data/img/04876.png      \n",
            "  inflating: data/img/82760.png      \n",
            "  inflating: data/img/94138.png      \n",
            "  inflating: data/img/02814.png      \n",
            "  inflating: data/img/51846.png      \n",
            "  inflating: data/img/21479.png      \n",
            "  inflating: data/img/13085.png      \n",
            "  inflating: data/img/64758.png      \n",
            "  inflating: data/img/32470.png      \n",
            "  inflating: data/img/52931.png      \n",
            "  inflating: data/img/02983.png      \n",
            "  inflating: data/img/78231.png      \n",
            "  inflating: data/img/02649.png      \n",
            "  inflating: data/img/91284.png      \n",
            "  inflating: data/img/59864.png      \n",
            "  inflating: data/img/67198.png      \n",
            "  inflating: data/img/61834.png      \n",
            "  inflating: data/img/43128.png      \n",
            "  inflating: data/img/54310.png      \n",
            "  inflating: data/img/61297.png      \n",
            "  inflating: data/img/43095.png      \n",
            "  inflating: data/img/92578.png      \n",
            "  inflating: data/img/95812.png      \n",
            "  inflating: data/img/63492.png      \n",
            "  inflating: data/img/85213.png      \n",
            "  inflating: data/img/37052.png      \n",
            "  inflating: data/img/70592.png      \n",
            "  inflating: data/img/10935.png      \n",
            "  inflating: data/img/96405.png      \n",
            "  inflating: data/img/92783.png      \n",
            "  inflating: data/img/62134.png      \n",
            "  inflating: data/img/84510.png      \n",
            "  inflating: data/img/74215.png      \n",
            "  inflating: data/img/71305.png      \n",
            "  inflating: data/img/14975.png      \n",
            "  inflating: data/img/27105.png      \n",
            "  inflating: data/img/56473.png      \n",
            "  inflating: data/img/18406.png      \n",
            "  inflating: data/img/86512.png      \n",
            "  inflating: data/img/01637.png      \n",
            "  inflating: data/img/65098.png      \n",
            "  inflating: data/img/04156.png      \n",
            "  inflating: data/img/19742.png      \n",
            "  inflating: data/img/76148.png      \n",
            "  inflating: data/img/17845.png      \n",
            "  inflating: data/img/75349.png      \n",
            "  inflating: data/img/02168.png      \n",
            "  inflating: data/img/47096.png      \n",
            "  inflating: data/img/95487.png      \n",
            "  inflating: data/img/39051.png      \n",
            "  inflating: data/img/86145.png      \n",
            "  inflating: data/img/05941.png      \n",
            "  inflating: data/img/94180.png      \n",
            "  inflating: data/img/06125.png      \n",
            "  inflating: data/img/10482.png      \n",
            "  inflating: data/img/65371.png      \n",
            "  inflating: data/img/89610.png      \n",
            "  inflating: data/img/97068.png      \n",
            "  inflating: data/img/60317.png      \n",
            "  inflating: data/img/89561.png      \n",
            "  inflating: data/img/80957.png      \n",
            "  inflating: data/img/26407.png      \n",
            "  inflating: data/img/46215.png      \n",
            "  inflating: data/img/16904.png      \n",
            "  inflating: data/img/37902.png      \n",
            "  inflating: data/img/50813.png      \n",
            "  inflating: data/img/13465.png      \n",
            "  inflating: data/img/26390.png      \n",
            "  inflating: data/img/81254.png      \n",
            "  inflating: data/img/47192.png      \n",
            "  inflating: data/img/17205.png      \n",
            "  inflating: data/img/34876.png      \n",
            "  inflating: data/img/43817.png      \n",
            "  inflating: data/img/08964.png      \n",
            "  inflating: data/img/95821.png      \n",
            "  inflating: data/img/16873.png      \n",
            "  inflating: data/img/59671.png      \n",
            "  inflating: data/img/59178.png      \n",
            "  inflating: data/img/71532.png      \n",
            "  inflating: data/img/70935.png      \n",
            "  inflating: data/img/83920.png      \n",
            "  inflating: data/img/01943.png      \n",
            "  inflating: data/img/41958.png      \n",
            "  inflating: data/img/94786.png      \n",
            "  inflating: data/img/41503.png      \n",
            "  inflating: data/img/83905.png      \n",
            "  inflating: data/img/91574.png      \n",
            "  inflating: data/img/60483.png      \n",
            "  inflating: data/img/13520.png      \n",
            "  inflating: data/img/78014.png      \n",
            "  inflating: data/img/81304.png      \n",
            "  inflating: data/img/25643.png      \n",
            "  inflating: data/img/47326.png      \n",
            "  inflating: data/img/68394.png      \n",
            "  inflating: data/img/47162.png      \n",
            "  inflating: data/img/51293.png      \n",
            "  inflating: data/img/03764.png      \n",
            "  inflating: data/img/92086.png      \n",
            "  inflating: data/img/80935.png      \n",
            "  inflating: data/img/45817.png      \n",
            "  inflating: data/img/45609.png      \n",
            "  inflating: data/img/67103.png      \n",
            "  inflating: data/img/50861.png      \n",
            "  inflating: data/img/36985.png      \n",
            "  inflating: data/img/96042.png      \n",
            "  inflating: data/img/01842.png      \n",
            "  inflating: data/img/26973.png      \n",
            "  inflating: data/img/72641.png      \n",
            "  inflating: data/img/26890.png      \n",
            "  inflating: data/img/32409.png      \n",
            "  inflating: data/img/49156.png      \n",
            "  inflating: data/img/53204.png      \n",
            "  inflating: data/img/35719.png      \n",
            "  inflating: data/img/60234.png      \n",
            "  inflating: data/img/84305.png      \n",
            "  inflating: data/img/35869.png      \n",
            "  inflating: data/img/43026.png      \n",
            "  inflating: data/img/59031.png      \n",
            "  inflating: data/img/32965.png      \n",
            "  inflating: data/img/95482.png      \n",
            "  inflating: data/img/13426.png      \n",
            "  inflating: data/img/28147.png      \n",
            "  inflating: data/img/62158.png      \n",
            "  inflating: data/img/82761.png      \n",
            "  inflating: data/img/94501.png      \n",
            "  inflating: data/img/50732.png      \n",
            "  inflating: data/img/25168.png      \n",
            "  inflating: data/img/27960.png      \n",
            "  inflating: data/img/93542.png      \n",
            "  inflating: data/img/04582.png      \n",
            "  inflating: data/img/72536.png      \n",
            "  inflating: data/img/78903.png      \n",
            "  inflating: data/img/87169.png      \n",
            "  inflating: data/img/67943.png      \n",
            "  inflating: data/img/45180.png      \n",
            "  inflating: data/img/13209.png      \n",
            "  inflating: data/img/87936.png      \n",
            "  inflating: data/img/21963.png      \n",
            "  inflating: data/img/20957.png      \n",
            "  inflating: data/img/75816.png      \n",
            "  inflating: data/img/43905.png      \n",
            "  inflating: data/img/24351.png      \n",
            "  inflating: data/img/98756.png      \n",
            "  inflating: data/img/05384.png      \n",
            "  inflating: data/img/78956.png      \n",
            "  inflating: data/img/15349.png      \n",
            "  inflating: data/img/45283.png      \n",
            "  inflating: data/img/02471.png      \n",
            "  inflating: data/img/87541.png      \n",
            "  inflating: data/img/79803.png      \n",
            "  inflating: data/img/29581.png      \n",
            "  inflating: data/img/13602.png      \n",
            "  inflating: data/img/50289.png      \n",
            "  inflating: data/img/37259.png      \n",
            "  inflating: data/img/89764.png      \n",
            "  inflating: data/img/37250.png      \n",
            "  inflating: data/img/36497.png      \n",
            "  inflating: data/img/83954.png      \n",
            "  inflating: data/img/37180.png      \n",
            "  inflating: data/img/56490.png      \n",
            "  inflating: data/img/46857.png      \n",
            "  inflating: data/img/12754.png      \n",
            "  inflating: data/img/42538.png      \n",
            "  inflating: data/img/07193.png      \n",
            "  inflating: data/img/95763.png      \n",
            "  inflating: data/img/05863.png      \n",
            "  inflating: data/img/38402.png      \n",
            "  inflating: data/img/06392.png      \n",
            "  inflating: data/img/72380.png      \n",
            "  inflating: data/img/40516.png      \n",
            "  inflating: data/img/28146.png      \n",
            "  inflating: data/img/51630.png      \n",
            "  inflating: data/img/76093.png      \n",
            "  inflating: data/img/37904.png      \n",
            "  inflating: data/img/87235.png      \n",
            "  inflating: data/img/48175.png      \n",
            "  inflating: data/img/68193.png      \n",
            "  inflating: data/img/59237.png      \n",
            "  inflating: data/img/24135.png      \n",
            "  inflating: data/img/93728.png      \n",
            "  inflating: data/img/57089.png      \n",
            "  inflating: data/img/37814.png      \n",
            "  inflating: data/img/59128.png      \n",
            "  inflating: data/img/95201.png      \n",
            "  inflating: data/img/62458.png      \n",
            "  inflating: data/img/89306.png      \n",
            "  inflating: data/img/46701.png      \n",
            "  inflating: data/img/29430.png      \n",
            "  inflating: data/img/90146.png      \n",
            "  inflating: data/img/86271.png      \n",
            "  inflating: data/img/59073.png      \n",
            "  inflating: data/img/73082.png      \n",
            "  inflating: data/img/65917.png      \n",
            "  inflating: data/img/03591.png      \n",
            "  inflating: data/img/84176.png      \n",
            "  inflating: data/img/32540.png      \n",
            "  inflating: data/img/49061.png      \n",
            "  inflating: data/img/06481.png      \n",
            "  inflating: data/img/15720.png      \n",
            "  inflating: data/img/29731.png      \n",
            "  inflating: data/img/34207.png      \n",
            "  inflating: data/img/79516.png      \n",
            "  inflating: data/img/17230.png      \n",
            "  inflating: data/img/03214.png      \n",
            "  inflating: data/img/29438.png      \n",
            "  inflating: data/img/04768.png      \n",
            "  inflating: data/img/03254.png      \n",
            "  inflating: data/img/52078.png      \n",
            "  inflating: data/img/26409.png      \n",
            "  inflating: data/img/83492.png      \n",
            "  inflating: data/img/65140.png      \n",
            "  inflating: data/img/69085.png      \n",
            "  inflating: data/img/26985.png      \n",
            "  inflating: data/img/97041.png      \n",
            "  inflating: data/img/57681.png      \n",
            "  inflating: data/img/97683.png      \n",
            "  inflating: data/img/81047.png      \n",
            "  inflating: data/img/61872.png      \n",
            "  inflating: data/img/96213.png      \n",
            "  inflating: data/img/62085.png      \n",
            "  inflating: data/img/89425.png      \n",
            "  inflating: data/img/62751.png      \n",
            "  inflating: data/img/20915.png      \n",
            "  inflating: data/img/38129.png      \n",
            "  inflating: data/img/98157.png      \n",
            "  inflating: data/img/97601.png      \n",
            "  inflating: data/img/83059.png      \n",
            "  inflating: data/img/83615.png      \n",
            "  inflating: data/img/57438.png      \n",
            "  inflating: data/img/98752.png      \n",
            "  inflating: data/img/19637.png      \n",
            "  inflating: data/img/08954.png      \n",
            "  inflating: data/img/98270.png      \n",
            "  inflating: data/img/12890.png      \n",
            "  inflating: data/img/67059.png      \n",
            "  inflating: data/img/19502.png      \n",
            "  inflating: data/img/97564.png      \n",
            "  inflating: data/img/47836.png      \n",
            "  inflating: data/img/87094.png      \n",
            "  inflating: data/img/35907.png      \n",
            "  inflating: data/img/37420.png      \n",
            "  inflating: data/img/85092.png      \n",
            "  inflating: data/img/34875.png      \n",
            "  inflating: data/img/87925.png      \n",
            "  inflating: data/img/50679.png      \n",
            "  inflating: data/img/79531.png      \n",
            "  inflating: data/img/26784.png      \n",
            "  inflating: data/img/80276.png      \n",
            "  inflating: data/img/31920.png      \n",
            "  inflating: data/img/76910.png      \n",
            "  inflating: data/img/46528.png      \n",
            "  inflating: data/img/67194.png      \n",
            "  inflating: data/img/86701.png      \n",
            "  inflating: data/img/21850.png      \n",
            "  inflating: data/img/83509.png      \n",
            "  inflating: data/img/35620.png      \n",
            "  inflating: data/img/69137.png      \n",
            "  inflating: data/img/37859.png      \n",
            "  inflating: data/img/56374.png      \n",
            "  inflating: data/img/41523.png      \n",
            "  inflating: data/img/36928.png      \n",
            "  inflating: data/img/03691.png      \n",
            "  inflating: data/img/39765.png      \n",
            "  inflating: data/img/14603.png      \n",
            "  inflating: data/img/05612.png      \n",
            "  inflating: data/img/75068.png      \n",
            "  inflating: data/img/17842.png      \n",
            "  inflating: data/img/07895.png      \n",
            "  inflating: data/img/19543.png      \n",
            "  inflating: data/img/68731.png      \n",
            "  inflating: data/img/53486.png      \n",
            "  inflating: data/img/34186.png      \n",
            "  inflating: data/img/07241.png      \n",
            "  inflating: data/img/13806.png      \n",
            "  inflating: data/img/67138.png      \n",
            "  inflating: data/img/49810.png      \n",
            "  inflating: data/img/29635.png      \n",
            "  inflating: data/img/26078.png      \n",
            "  inflating: data/img/30518.png      \n",
            "  inflating: data/img/04268.png      \n",
            "  inflating: data/img/80954.png      \n",
            "  inflating: data/img/42315.png      \n",
            "  inflating: data/img/95108.png      \n",
            "  inflating: data/img/06315.png      \n",
            "  inflating: data/img/20738.png      \n",
            "  inflating: data/img/51283.png      \n",
            "  inflating: data/img/70158.png      \n",
            "  inflating: data/img/56741.png      \n",
            "  inflating: data/img/46518.png      \n",
            "  inflating: data/img/78405.png      \n",
            "  inflating: data/img/43907.png      \n",
            "  inflating: data/img/21504.png      \n",
            "  inflating: data/img/30291.png      \n",
            "  inflating: data/img/62319.png      \n",
            "  inflating: data/img/78134.png      \n",
            "  inflating: data/img/40756.png      \n",
            "  inflating: data/img/47159.png      \n",
            "  inflating: data/img/84362.png      \n",
            "  inflating: data/img/12349.png      \n",
            "  inflating: data/img/93082.png      \n",
            "  inflating: data/img/70843.png      \n",
            "  inflating: data/img/50684.png      \n",
            "  inflating: data/img/54082.png      \n",
            "  inflating: data/img/82940.png      \n",
            "  inflating: data/img/09824.png      \n",
            "  inflating: data/img/94062.png      \n",
            "  inflating: data/img/12935.png      \n",
            "  inflating: data/img/83290.png      \n",
            "  inflating: data/img/04857.png      \n",
            "  inflating: data/img/92365.png      \n",
            "  inflating: data/img/43856.png      \n",
            "  inflating: data/img/18963.png      \n",
            "  inflating: data/img/32768.png      \n",
            "  inflating: data/img/51629.png      \n",
            "  inflating: data/img/37862.png      \n",
            "  inflating: data/img/93687.png      \n",
            "  inflating: data/img/95380.png      \n",
            "  inflating: data/img/38491.png      \n",
            "  inflating: data/img/76814.png      \n",
            "  inflating: data/img/41706.png      \n",
            "  inflating: data/img/13096.png      \n",
            "  inflating: data/img/15209.png      \n",
            "  inflating: data/img/14092.png      \n",
            "  inflating: data/img/28617.png      \n",
            "  inflating: data/img/42185.png      \n",
            "  inflating: data/img/95264.png      \n",
            "  inflating: data/img/51679.png      \n",
            "  inflating: data/img/26375.png      \n",
            "  inflating: data/img/15892.png      \n",
            "  inflating: data/img/19384.png      \n",
            "  inflating: data/img/23910.png      \n",
            "  inflating: data/img/86159.png      \n",
            "  inflating: data/img/84692.png      \n",
            "  inflating: data/img/93671.png      \n",
            "  inflating: data/img/41936.png      \n",
            "  inflating: data/img/51968.png      \n",
            "  inflating: data/img/04569.png      \n",
            "  inflating: data/img/75821.png      \n",
            "  inflating: data/img/05471.png      \n",
            "  inflating: data/img/69870.png      \n",
            "  inflating: data/img/89350.png      \n",
            "  inflating: data/img/32649.png      \n",
            "  inflating: data/img/47960.png      \n",
            "  inflating: data/img/61423.png      \n",
            "  inflating: data/img/90847.png      \n",
            "  inflating: data/img/10936.png      \n",
            "  inflating: data/img/18679.png      \n",
            "  inflating: data/img/05928.png      \n",
            "  inflating: data/img/29384.png      \n",
            "  inflating: data/img/01268.png      \n",
            "  inflating: data/img/03276.png      \n",
            "  inflating: data/img/68145.png      \n",
            "  inflating: data/img/61085.png      \n",
            "  inflating: data/img/52708.png      \n",
            "  inflating: data/img/30796.png      \n",
            "  inflating: data/img/74915.png      \n",
            "  inflating: data/img/87461.png      \n",
            "  inflating: data/img/31068.png      \n",
            "  inflating: data/img/36128.png      \n",
            "  inflating: data/img/38954.png      \n",
            "  inflating: data/img/31925.png      \n",
            "  inflating: data/img/73956.png      \n",
            "  inflating: data/img/76938.png      \n",
            "  inflating: data/img/06135.png      \n",
            "  inflating: data/img/45671.png      \n",
            "  inflating: data/img/83976.png      \n",
            "  inflating: data/img/57281.png      \n",
            "  inflating: data/img/57193.png      \n",
            "  inflating: data/img/71028.png      \n",
            "  inflating: data/img/36097.png      \n",
            "  inflating: data/img/84516.png      \n",
            "  inflating: data/img/14865.png      \n",
            "  inflating: data/img/52361.png      \n",
            "  inflating: data/img/35487.png      \n",
            "  inflating: data/img/94620.png      \n",
            "  inflating: data/img/12074.png      \n",
            "  inflating: data/img/47896.png      \n",
            "  inflating: data/img/62815.png      \n",
            "  inflating: data/img/28590.png      \n",
            "  inflating: data/img/70914.png      \n",
            "  inflating: data/img/17234.png      \n",
            "  inflating: data/img/86504.png      \n",
            "  inflating: data/img/01568.png      \n",
            "  inflating: data/img/86750.png      \n",
            "  inflating: data/img/69405.png      \n",
            "  inflating: data/img/84502.png      \n",
            "  inflating: data/img/46720.png      \n",
            "  inflating: data/img/29507.png      \n",
            "  inflating: data/img/91536.png      \n",
            "  inflating: data/img/39528.png      \n",
            "  inflating: data/img/70834.png      \n",
            "  inflating: data/img/85192.png      \n",
            "  inflating: data/img/53012.png      \n",
            "  inflating: data/img/35601.png      \n",
            "  inflating: data/img/43852.png      \n",
            "  inflating: data/img/75298.png      \n",
            "  inflating: data/img/13852.png      \n",
            "  inflating: data/img/50748.png      \n",
            "  inflating: data/img/05914.png      \n",
            "  inflating: data/img/47628.png      \n",
            "  inflating: data/img/10748.png      \n",
            "  inflating: data/img/28436.png      \n",
            "  inflating: data/img/90516.png      \n",
            "  inflating: data/img/57608.png      \n",
            "  inflating: data/img/81792.png      \n",
            "  inflating: data/img/35870.png      \n",
            "  inflating: data/img/14598.png      \n",
            "  inflating: data/img/12036.png      \n",
            "  inflating: data/img/05214.png      \n",
            "  inflating: data/img/84629.png      \n",
            "  inflating: data/img/43581.png      \n",
            "  inflating: data/img/23794.png      \n",
            "  inflating: data/img/45072.png      \n",
            "  inflating: data/img/32587.png      \n",
            "  inflating: data/img/81245.png      \n",
            "  inflating: data/img/52607.png      \n",
            "  inflating: data/img/20791.png      \n",
            "  inflating: data/img/37642.png      \n",
            "  inflating: data/img/29304.png      \n",
            "  inflating: data/img/94823.png      \n",
            "  inflating: data/img/98643.png      \n",
            "  inflating: data/img/50473.png      \n",
            "  inflating: data/img/24810.png      \n",
            "  inflating: data/img/53182.png      \n",
            "  inflating: data/img/05389.png      \n",
            "  inflating: data/img/54781.png      \n",
            "  inflating: data/img/50879.png      \n",
            "  inflating: data/img/37129.png      \n",
            "  inflating: data/img/74386.png      \n",
            "  inflating: data/img/82419.png      \n",
            "  inflating: data/img/14038.png      \n",
            "  inflating: data/img/50738.png      \n",
            "  inflating: data/img/43928.png      \n",
            "  inflating: data/img/56413.png      \n",
            "  inflating: data/img/46357.png      \n",
            "  inflating: data/img/39156.png      \n",
            "  inflating: data/img/42810.png      \n",
            "  inflating: data/img/53476.png      \n",
            "  inflating: data/img/28403.png      \n",
            "  inflating: data/img/10479.png      \n",
            "  inflating: data/img/98047.png      \n",
            "  inflating: data/img/80193.png      \n",
            "  inflating: data/img/72318.png      \n",
            "  inflating: data/img/30742.png      \n",
            "  inflating: data/img/82163.png      \n",
            "  inflating: data/img/41603.png      \n",
            "  inflating: data/img/53821.png      \n",
            "  inflating: data/img/10589.png      \n",
            "  inflating: data/img/18926.png      \n",
            "  inflating: data/img/46953.png      \n",
            "  inflating: data/img/35412.png      \n",
            "  inflating: data/img/39470.png      \n",
            "  inflating: data/img/51243.png      \n",
            "  inflating: data/img/24586.png      \n",
            "  inflating: data/img/41925.png      \n",
            "  inflating: data/img/25943.png      \n",
            "  inflating: data/img/78302.png      \n",
            "  inflating: data/img/34250.png      \n",
            "  inflating: data/img/68190.png      \n",
            "  inflating: data/img/36480.png      \n",
            "  inflating: data/img/70613.png      \n",
            "  inflating: data/img/32854.png      \n",
            "  inflating: data/img/70196.png      \n",
            "  inflating: data/img/01547.png      \n",
            "  inflating: data/img/82379.png      \n",
            "  inflating: data/img/15734.png      \n",
            "  inflating: data/img/20819.png      \n",
            "  inflating: data/img/39867.png      \n",
            "  inflating: data/img/52761.png      \n",
            "  inflating: data/img/62197.png      \n",
            "  inflating: data/img/71480.png      \n",
            "  inflating: data/img/45708.png      \n",
            "  inflating: data/img/08125.png      \n",
            "  inflating: data/img/59130.png      \n",
            "  inflating: data/img/49215.png      \n",
            "  inflating: data/img/83549.png      \n",
            "  inflating: data/img/12675.png      \n",
            "  inflating: data/img/34157.png      \n",
            "  inflating: data/img/12785.png      \n",
            "  inflating: data/img/19047.png      \n",
            "  inflating: data/img/46710.png      \n",
            "  inflating: data/img/24857.png      \n",
            "  inflating: data/img/14675.png      \n",
            "  inflating: data/img/54731.png      \n",
            "  inflating: data/img/35604.png      \n",
            "  inflating: data/img/21083.png      \n",
            "  inflating: data/img/62514.png      \n",
            "  inflating: data/img/23864.png      \n",
            "  inflating: data/img/92183.png      \n",
            "  inflating: data/img/46918.png      \n",
            "  inflating: data/img/69348.png      \n",
            "  inflating: data/img/34206.png      \n",
            "  inflating: data/img/01269.png      \n",
            "  inflating: data/img/17265.png      \n",
            "  inflating: data/img/98426.png      \n",
            "  inflating: data/img/86254.png      \n",
            "  inflating: data/img/95371.png      \n",
            "  inflating: data/img/63902.png      \n",
            "  inflating: data/img/05362.png      \n",
            "  inflating: data/img/83052.png      \n",
            "  inflating: data/img/31290.png      \n",
            "  inflating: data/img/07839.png      \n",
            "  inflating: data/img/57284.png      \n",
            "  inflating: data/img/01256.png      \n",
            "  inflating: data/img/82369.png      \n",
            "  inflating: data/img/46735.png      \n",
            "  inflating: data/img/93071.png      \n",
            "  inflating: data/img/34975.png      \n",
            "  inflating: data/img/85316.png      \n",
            "  inflating: data/img/97503.png      \n",
            "  inflating: data/img/74602.png      \n",
            "  inflating: data/img/73258.png      \n",
            "  inflating: data/img/39821.png      \n",
            "  inflating: data/img/34825.png      \n",
            "  inflating: data/img/25783.png      \n",
            "  inflating: data/img/37256.png      \n",
            "  inflating: data/img/17089.png      \n",
            "  inflating: data/img/46783.png      \n",
            "  inflating: data/img/46732.png      \n",
            "  inflating: data/img/45987.png      \n",
            "  inflating: data/img/28134.png      \n",
            "  inflating: data/img/95341.png      \n",
            "  inflating: data/img/95176.png      \n",
            "  inflating: data/img/57162.png      \n",
            "  inflating: data/img/50341.png      \n",
            "  inflating: data/img/16487.png      \n",
            "  inflating: data/img/56948.png      \n",
            "  inflating: data/img/03759.png      \n",
            "  inflating: data/img/67843.png      \n",
            "  inflating: data/img/46527.png      \n",
            "  inflating: data/img/54931.png      \n",
            "  inflating: data/img/60549.png      \n",
            "  inflating: data/img/02351.png      \n",
            "  inflating: data/img/40127.png      \n",
            "  inflating: data/img/62301.png      \n",
            "  inflating: data/img/05297.png      \n",
            "  inflating: data/img/98057.png      \n",
            "  inflating: data/img/63981.png      \n",
            "  inflating: data/img/63719.png      \n",
            "  inflating: data/img/97283.png      \n",
            "  inflating: data/img/32981.png      \n",
            "  inflating: data/img/28670.png      \n",
            "  inflating: data/img/85026.png      \n",
            "  inflating: data/img/94087.png      \n",
            "  inflating: data/img/14852.png      \n",
            "  inflating: data/img/25164.png      \n",
            "  inflating: data/img/63587.png      \n",
            "  inflating: data/img/81359.png      \n",
            "  inflating: data/img/35917.png      \n",
            "  inflating: data/img/27950.png      \n",
            "  inflating: data/img/23690.png      \n",
            "  inflating: data/img/39580.png      \n",
            "  inflating: data/img/98517.png      \n",
            "  inflating: data/img/81063.png      \n",
            "  inflating: data/img/17652.png      \n",
            "  inflating: data/img/26943.png      \n",
            "  inflating: data/img/17530.png      \n",
            "  inflating: data/img/76351.png      \n",
            "  inflating: data/img/14690.png      \n",
            "  inflating: data/img/13875.png      \n",
            "  inflating: data/img/93247.png      \n",
            "  inflating: data/img/93826.png      \n",
            "  inflating: data/img/83024.png      \n",
            "  inflating: data/img/48269.png      \n",
            "  inflating: data/img/39170.png      \n",
            "  inflating: data/img/12086.png      \n",
            "  inflating: data/img/10527.png      \n",
            "  inflating: data/img/80614.png      \n",
            "  inflating: data/img/87296.png      \n",
            "  inflating: data/img/36581.png      \n",
            "  inflating: data/img/36542.png      \n",
            "  inflating: data/img/85901.png      \n",
            "  inflating: data/img/59301.png      \n",
            "  inflating: data/img/72369.png      \n",
            "  inflating: data/img/71368.png      \n",
            "  inflating: data/img/05617.png      \n",
            "  inflating: data/img/83701.png      \n",
            "  inflating: data/img/28751.png      \n",
            "  inflating: data/img/19324.png      \n",
            "  inflating: data/img/06752.png      \n",
            "  inflating: data/img/48052.png      \n",
            "  inflating: data/img/39726.png      \n",
            "  inflating: data/img/06728.png      \n",
            "  inflating: data/img/02456.png      \n",
            "  inflating: data/img/01936.png      \n",
            "  inflating: data/img/56891.png      \n",
            "  inflating: data/img/06579.png      \n",
            "  inflating: data/img/68742.png      \n",
            "  inflating: data/img/79824.png      \n",
            "  inflating: data/img/23907.png      \n",
            "  inflating: data/img/35219.png      \n",
            "  inflating: data/img/26543.png      \n",
            "  inflating: data/img/92534.png      \n",
            "  inflating: data/img/97142.png      \n",
            "  inflating: data/img/35786.png      \n",
            "  inflating: data/img/89123.png      \n",
            "  inflating: data/img/34258.png      \n",
            "  inflating: data/img/38724.png      \n",
            "  inflating: data/img/43816.png      \n",
            "  inflating: data/img/69518.png      \n",
            "  inflating: data/img/09283.png      \n",
            "  inflating: data/img/12657.png      \n",
            "  inflating: data/img/90827.png      \n",
            "  inflating: data/img/40193.png      \n",
            "  inflating: data/img/18204.png      \n",
            "  inflating: data/img/47183.png      \n",
            "  inflating: data/img/41203.png      \n",
            "  inflating: data/img/61825.png      \n",
            "  inflating: data/img/50361.png      \n",
            "  inflating: data/img/47056.png      \n",
            "  inflating: data/img/83651.png      \n",
            "  inflating: data/img/68719.png      \n",
            "  inflating: data/img/20654.png      \n",
            "  inflating: data/img/94237.png      \n",
            "  inflating: data/img/07915.png      \n",
            "  inflating: data/img/97543.png      \n",
            "  inflating: data/img/31892.png      \n",
            "  inflating: data/img/45927.png      \n",
            "  inflating: data/img/25071.png      \n",
            "  inflating: data/img/02857.png      \n",
            "  inflating: data/img/81734.png      \n",
            "  inflating: data/img/63985.png      \n",
            "  inflating: data/img/49703.png      \n",
            "  inflating: data/img/29761.png      \n",
            "  inflating: data/img/19530.png      \n",
            "  inflating: data/img/14078.png      \n",
            "  inflating: data/img/60931.png      \n",
            "  inflating: data/img/27915.png      \n",
            "  inflating: data/img/51390.png      \n",
            "  inflating: data/img/34765.png      \n",
            "  inflating: data/img/57621.png      \n",
            "  inflating: data/img/94380.png      \n",
            "  inflating: data/img/52386.png      \n",
            "  inflating: data/img/06348.png      \n",
            "  inflating: data/img/26358.png      \n",
            "  inflating: data/img/69024.png      \n",
            "  inflating: data/img/59034.png      \n",
            "  inflating: data/img/07612.png      \n",
            "  inflating: data/img/20491.png      \n",
            "  inflating: data/img/30815.png      \n",
            "  inflating: data/img/58301.png      \n",
            "  inflating: data/img/26417.png      \n",
            "  inflating: data/img/87196.png      \n",
            "  inflating: data/img/71358.png      \n",
            "  inflating: data/img/45960.png      \n",
            "  inflating: data/img/54917.png      \n",
            "  inflating: data/img/05148.png      \n",
            "  inflating: data/img/14650.png      \n",
            "  inflating: data/img/96325.png      \n",
            "  inflating: data/img/01497.png      \n",
            "  inflating: data/img/70319.png      \n",
            "  inflating: data/img/91740.png      \n",
            "  inflating: data/img/79612.png      \n",
            "  inflating: data/img/56912.png      \n",
            "  inflating: data/img/21309.png      \n",
            "  inflating: data/img/38910.png      \n",
            "  inflating: data/img/71529.png      \n",
            "  inflating: data/img/16054.png      \n",
            "  inflating: data/img/98125.png      \n",
            "  inflating: data/img/12643.png      \n",
            "  inflating: data/img/29075.png      \n",
            "  inflating: data/img/89251.png      \n",
            "  inflating: data/img/71628.png      \n",
            "  inflating: data/img/17504.png      \n",
            "  inflating: data/img/26534.png      \n",
            "  inflating: data/img/95810.png      \n",
            "  inflating: data/img/10962.png      \n",
            "  inflating: data/img/63280.png      \n",
            "  inflating: data/img/61502.png      \n",
            "  inflating: data/img/92170.png      \n",
            "  inflating: data/img/69078.png      \n",
            "  inflating: data/img/67014.png      \n",
            "  inflating: data/img/86593.png      \n",
            "  inflating: data/img/29158.png      \n",
            "  inflating: data/img/40721.png      \n",
            "  inflating: data/img/69203.png      \n",
            "  inflating: data/img/98504.png      \n",
            "  inflating: data/img/85729.png      \n",
            "  inflating: data/img/91286.png      \n",
            "  inflating: data/img/07319.png      \n",
            "  inflating: data/img/85472.png      \n",
            "  inflating: data/img/89152.png      \n",
            "  inflating: data/img/13528.png      \n",
            "  inflating: data/img/67980.png      \n",
            "  inflating: data/img/70356.png      \n",
            "  inflating: data/img/42836.png      \n",
            "  inflating: data/img/19532.png      \n",
            "  inflating: data/img/09765.png      \n",
            "  inflating: data/img/47605.png      \n",
            "  inflating: data/img/30586.png      \n",
            "  inflating: data/img/12580.png      \n",
            "  inflating: data/img/89236.png      \n",
            "  inflating: data/img/81320.png      \n",
            "  inflating: data/img/05219.png      \n",
            "  inflating: data/img/40216.png      \n",
            "  inflating: data/img/82510.png      \n",
            "  inflating: data/img/69012.png      \n",
            "  inflating: data/img/36478.png      \n",
            "  inflating: data/img/89471.png      \n",
            "  inflating: data/img/43197.png      \n",
            "  inflating: data/img/78351.png      \n",
            "  inflating: data/img/01468.png      \n",
            "  inflating: data/img/46730.png      \n",
            "  inflating: data/img/23648.png      \n",
            "  inflating: data/img/06852.png      \n",
            "  inflating: data/img/89621.png      \n",
            "  inflating: data/img/10287.png      \n",
            "  inflating: data/img/20739.png      \n",
            "  inflating: data/img/01483.png      \n",
            "  inflating: data/img/32876.png      \n",
            "  inflating: data/img/94158.png      \n",
            "  inflating: data/img/64701.png      \n",
            "  inflating: data/img/28641.png      \n",
            "  inflating: data/img/80265.png      \n",
            "  inflating: data/img/71503.png      \n",
            "  inflating: data/img/40297.png      \n",
            "  inflating: data/img/80615.png      \n",
            "  inflating: data/img/83972.png      \n",
            "  inflating: data/img/63042.png      \n",
            "  inflating: data/img/48106.png      \n",
            "  inflating: data/img/87096.png      \n",
            "  inflating: data/img/05726.png      \n",
            "  inflating: data/img/63140.png      \n",
            "  inflating: data/img/45829.png      \n",
            "  inflating: data/img/20715.png      \n",
            "  inflating: data/img/69512.png      \n",
            "  inflating: data/img/05846.png      \n",
            "  inflating: data/img/72819.png      \n",
            "  inflating: data/img/16093.png      \n",
            "  inflating: data/img/21450.png      \n",
            "  inflating: data/img/10985.png      \n",
            "  inflating: data/img/61407.png      \n",
            "  inflating: data/img/59460.png      \n",
            "  inflating: data/img/51403.png      \n",
            "  inflating: data/img/09426.png      \n",
            "  inflating: data/img/93841.png      \n",
            "  inflating: data/img/14906.png      \n",
            "  inflating: data/img/65378.png      \n",
            "  inflating: data/img/31097.png      \n",
            "  inflating: data/img/38609.png      \n",
            "  inflating: data/img/30982.png      \n",
            "  inflating: data/img/16520.png      \n",
            "  inflating: data/img/73154.png      \n",
            "  inflating: data/img/96801.png      \n",
            "  inflating: data/img/49870.png      \n",
            "  inflating: data/img/67254.png      \n",
            "  inflating: data/img/41802.png      \n",
            "  inflating: data/img/35724.png      \n",
            "  inflating: data/img/41835.png      \n",
            "  inflating: data/img/26315.png      \n",
            "  inflating: data/img/90427.png      \n",
            "  inflating: data/img/20459.png      \n",
            "  inflating: data/img/01736.png      \n",
            "  inflating: data/img/85034.png      \n",
            "  inflating: data/img/56274.png      \n",
            "  inflating: data/img/65804.png      \n",
            "  inflating: data/img/02371.png      \n",
            "  inflating: data/img/30871.png      \n",
            "  inflating: data/img/29460.png      \n",
            "  inflating: data/img/34728.png      \n",
            "  inflating: data/img/08163.png      \n",
            "  inflating: data/img/71459.png      \n",
            "  inflating: data/img/14859.png      \n",
            "  inflating: data/img/81390.png      \n",
            "  inflating: data/img/42853.png      \n",
            "  inflating: data/img/81964.png      \n",
            "  inflating: data/img/21740.png      \n",
            "  inflating: data/img/20145.png      \n",
            "  inflating: data/img/87341.png      \n",
            "  inflating: data/img/28175.png      \n",
            "  inflating: data/img/83025.png      \n",
            "  inflating: data/img/06471.png      \n",
            "  inflating: data/img/17962.png      \n",
            "  inflating: data/img/69073.png      \n",
            "  inflating: data/img/08516.png      \n",
            "  inflating: data/img/19457.png      \n",
            "  inflating: data/img/61780.png      \n",
            "  inflating: data/img/82705.png      \n",
            "  inflating: data/img/36189.png      \n",
            "  inflating: data/img/20931.png      \n",
            "  inflating: data/img/58649.png      \n",
            "  inflating: data/img/21836.png      \n",
            "  inflating: data/img/78450.png      \n",
            "  inflating: data/img/48923.png      \n",
            "  inflating: data/img/60741.png      \n",
            "  inflating: data/img/79346.png      \n",
            "  inflating: data/img/85073.png      \n",
            "  inflating: data/img/09357.png      \n",
            "  inflating: data/img/20956.png      \n",
            "  inflating: data/img/84013.png      \n",
            "  inflating: data/img/51489.png      \n",
            "  inflating: data/img/57260.png      \n",
            "  inflating: data/img/58642.png      \n",
            "  inflating: data/img/53426.png      \n",
            "  inflating: data/img/23859.png      \n",
            "  inflating: data/img/39862.png      \n",
            "  inflating: data/img/02614.png      \n",
            "  inflating: data/img/42675.png      \n",
            "  inflating: data/img/62184.png      \n",
            "  inflating: data/img/09347.png      \n",
            "  inflating: data/img/34298.png      \n",
            "  inflating: data/img/64705.png      \n",
            "  inflating: data/img/05869.png      \n",
            "  inflating: data/img/91268.png      \n",
            "  inflating: data/img/64017.png      \n",
            "  inflating: data/img/91367.png      \n",
            "  inflating: data/img/29816.png      \n",
            "  inflating: data/img/14589.png      \n",
            "  inflating: data/img/54896.png      \n",
            "  inflating: data/img/49017.png      \n",
            "  inflating: data/img/53720.png      \n",
            "  inflating: data/img/93405.png      \n",
            "  inflating: data/img/19345.png      \n",
            "  inflating: data/img/73681.png      \n",
            "  inflating: data/img/40796.png      \n",
            "  inflating: data/img/78923.png      \n",
            "  inflating: data/img/92750.png      \n",
            "  inflating: data/img/56328.png      \n",
            "  inflating: data/img/78239.png      \n",
            "  inflating: data/img/09852.png      \n",
            "  inflating: data/img/64321.png      \n",
            "  inflating: data/img/28435.png      \n",
            "  inflating: data/img/84321.png      \n",
            "  inflating: data/img/34056.png      \n",
            "  inflating: data/img/75106.png      \n",
            "  inflating: data/img/23514.png      \n",
            "  inflating: data/img/12546.png      \n",
            "  inflating: data/img/58732.png      \n",
            "  inflating: data/img/75203.png      \n",
            "  inflating: data/img/80347.png      \n",
            "  inflating: data/img/83704.png      \n",
            "  inflating: data/img/86713.png      \n",
            "  inflating: data/img/49360.png      \n",
            "  inflating: data/img/37601.png      \n",
            "  inflating: data/img/12490.png      \n",
            "  inflating: data/img/92845.png      \n",
            "  inflating: data/img/01829.png      \n",
            "  inflating: data/img/40965.png      \n",
            "  inflating: data/img/53904.png      \n",
            "  inflating: data/img/86934.png      \n",
            "  inflating: data/img/84329.png      \n",
            "  inflating: data/img/30157.png      \n",
            "  inflating: data/img/43216.png      \n",
            "  inflating: data/img/40265.png      \n",
            "  inflating: data/img/47136.png      \n",
            "  inflating: data/img/26153.png      \n",
            "  inflating: data/img/29437.png      \n",
            "  inflating: data/img/31729.png      \n",
            "  inflating: data/img/82190.png      \n",
            "  inflating: data/img/03861.png      \n",
            "  inflating: data/img/97013.png      \n",
            "  inflating: data/img/95764.png      \n",
            "  inflating: data/img/34721.png      \n",
            "  inflating: data/img/70549.png      \n",
            "  inflating: data/img/71365.png      \n",
            "  inflating: data/img/09716.png      \n",
            "  inflating: data/img/98537.png      \n",
            "  inflating: data/img/42065.png      \n",
            "  inflating: data/img/28976.png      \n",
            "  inflating: data/img/84971.png      \n",
            "  inflating: data/img/08146.png      \n",
            "  inflating: data/img/48250.png      \n",
            "  inflating: data/img/39214.png      \n",
            "  inflating: data/img/64789.png      \n",
            "  inflating: data/img/81260.png      \n",
            "  inflating: data/img/35170.png      \n",
            "  inflating: data/img/09531.png      \n",
            "  inflating: data/img/76214.png      \n",
            "  inflating: data/img/30579.png      \n",
            "  inflating: data/img/12637.png      \n",
            "  inflating: data/img/06842.png      \n",
            "  inflating: data/img/42380.png      \n",
            "  inflating: data/img/13046.png      \n",
            "  inflating: data/img/47208.png      \n",
            "  inflating: data/img/35402.png      \n",
            "  inflating: data/img/01452.png      \n",
            "  inflating: data/img/41830.png      \n",
            "  inflating: data/img/58304.png      \n",
            "  inflating: data/img/25483.png      \n",
            "  inflating: data/img/74250.png      \n",
            "  inflating: data/img/92854.png      \n",
            "  inflating: data/img/89314.png      \n",
            "  inflating: data/img/95208.png      \n",
            "  inflating: data/img/16540.png      \n",
            "  inflating: data/img/23419.png      \n",
            "  inflating: data/img/06832.png      \n",
            "  inflating: data/img/64130.png      \n",
            "  inflating: data/img/76423.png      \n",
            "  inflating: data/img/02497.png      \n",
            "  inflating: data/img/13620.png      \n",
            "  inflating: data/img/15043.png      \n",
            "  inflating: data/img/51049.png      \n",
            "  inflating: data/img/35167.png      \n",
            "  inflating: data/img/54397.png      \n",
            "  inflating: data/img/26835.png      \n",
            "  inflating: data/img/30196.png      \n",
            "  inflating: data/img/31294.png      \n",
            "  inflating: data/img/17853.png      \n",
            "  inflating: data/img/17546.png      \n",
            "  inflating: data/img/21075.png      \n",
            "  inflating: data/img/52097.png      \n",
            "  inflating: data/img/59376.png      \n",
            "  inflating: data/img/06873.png      \n",
            "  inflating: data/img/64097.png      \n",
            "  inflating: data/img/85936.png      \n",
            "  inflating: data/img/68309.png      \n",
            "  inflating: data/img/16259.png      \n",
            "  inflating: data/img/98072.png      \n",
            "  inflating: data/img/29547.png      \n",
            "  inflating: data/img/48921.png      \n",
            "  inflating: data/img/43258.png      \n",
            "  inflating: data/img/10725.png      \n",
            "  inflating: data/img/37459.png      \n",
            "  inflating: data/img/72093.png      \n",
            "  inflating: data/img/40159.png      \n",
            "  inflating: data/img/36748.png      \n",
            "  inflating: data/img/95078.png      \n",
            "  inflating: data/img/03841.png      \n",
            "  inflating: data/img/10274.png      \n",
            "  inflating: data/img/74531.png      \n",
            "  inflating: data/img/16780.png      \n",
            "  inflating: data/img/19438.png      \n",
            "  inflating: data/img/29354.png      \n",
            "  inflating: data/img/70284.png      \n",
            "  inflating: data/img/47615.png      \n",
            "  inflating: data/img/23715.png      \n",
            "  inflating: data/img/05782.png      \n",
            "  inflating: data/img/03217.png      \n",
            "  inflating: data/img/06934.png      \n",
            "  inflating: data/img/54639.png      \n",
            "  inflating: data/img/06198.png      \n",
            "  inflating: data/img/40871.png      \n",
            "  inflating: data/img/26910.png      \n",
            "  inflating: data/img/69084.png      \n",
            "  inflating: data/img/24197.png      \n",
            "  inflating: data/img/45198.png      \n",
            "  inflating: data/img/61304.png      \n",
            "  inflating: data/img/27905.png      \n",
            "  inflating: data/img/16254.png      \n",
            "  inflating: data/img/82104.png      \n",
            "  inflating: data/img/61948.png      \n",
            "  inflating: data/img/97402.png      \n",
            "  inflating: data/img/42681.png      \n",
            "  inflating: data/img/82507.png      \n",
            "  inflating: data/img/61823.png      \n",
            "  inflating: data/img/04253.png      \n",
            "  inflating: data/img/79485.png      \n",
            "  inflating: data/img/16720.png      \n",
            "  inflating: data/img/23578.png      \n",
            "  inflating: data/img/90263.png      \n",
            "  inflating: data/img/20673.png      \n",
            "  inflating: data/img/42896.png      \n",
            "  inflating: data/img/98215.png      \n",
            "  inflating: data/img/67021.png      \n",
            "  inflating: data/img/38162.png      \n",
            "  inflating: data/img/40865.png      \n",
            "  inflating: data/img/27604.png      \n",
            "  inflating: data/img/53694.png      \n",
            "  inflating: data/img/75016.png      \n",
            "  inflating: data/img/73914.png      \n",
            "  inflating: data/img/97514.png      \n",
            "  inflating: data/img/91372.png      \n",
            "  inflating: data/img/93614.png      \n",
            "  inflating: data/img/06791.png      \n",
            "  inflating: data/img/48357.png      \n",
            "  inflating: data/img/15269.png      \n",
            "  inflating: data/img/36805.png      \n",
            "  inflating: data/img/80237.png      \n",
            "  inflating: data/img/69081.png      \n",
            "  inflating: data/img/70385.png      \n",
            "  inflating: data/img/46152.png      \n",
            "  inflating: data/img/86032.png      \n",
            "  inflating: data/img/52768.png      \n",
            "  inflating: data/img/05126.png      \n",
            "  inflating: data/img/84927.png      \n",
            "  inflating: data/img/86207.png      \n",
            "  inflating: data/img/23061.png      \n",
            "  inflating: data/img/04971.png      \n",
            "  inflating: data/img/97314.png      \n",
            "  inflating: data/img/14520.png      \n",
            "  inflating: data/img/38410.png      \n",
            "  inflating: data/img/76405.png      \n",
            "  inflating: data/img/59142.png      \n",
            "  inflating: data/img/67541.png      \n",
            "  inflating: data/img/27856.png      \n",
            "  inflating: data/img/73160.png      \n",
            "  inflating: data/img/01295.png      \n",
            "  inflating: data/img/95281.png      \n",
            "  inflating: data/img/34671.png      \n",
            "  inflating: data/img/23897.png      \n",
            "  inflating: data/img/17308.png      \n",
            "  inflating: data/img/67149.png      \n",
            "  inflating: data/img/89106.png      \n",
            "  inflating: data/img/46829.png      \n",
            "  inflating: data/img/13478.png      \n",
            "  inflating: data/img/27139.png      \n",
            "  inflating: data/img/36152.png      \n",
            "  inflating: data/img/74058.png      \n",
            "  inflating: data/img/70628.png      \n",
            "  inflating: data/img/78642.png      \n",
            "  inflating: data/img/61097.png      \n",
            "  inflating: data/img/80154.png      \n",
            "  inflating: data/img/47950.png      \n",
            "  inflating: data/img/62490.png      \n",
            "  inflating: data/img/94251.png      \n",
            "  inflating: data/img/95801.png      \n",
            "  inflating: data/img/64781.png      \n",
            "  inflating: data/img/86031.png      \n",
            "  inflating: data/img/48120.png      \n",
            "  inflating: data/img/73806.png      \n",
            "  inflating: data/img/80759.png      \n",
            "  inflating: data/img/59046.png      \n",
            "  inflating: data/img/35264.png      \n",
            "  inflating: data/img/96270.png      \n",
            "  inflating: data/img/04891.png      \n",
            "  inflating: data/img/27308.png      \n",
            "  inflating: data/img/06237.png      \n",
            "  inflating: data/img/63859.png      \n",
            "  inflating: data/img/29708.png      \n",
            "  inflating: data/img/85976.png      \n",
            "  inflating: data/img/40712.png      \n",
            "  inflating: data/img/20684.png      \n",
            "  inflating: data/img/13764.png      \n",
            "  inflating: data/img/56709.png      \n",
            "  inflating: data/img/28395.png      \n",
            "  inflating: data/img/71084.png      \n",
            "  inflating: data/img/65708.png      \n",
            "  inflating: data/img/40832.png      \n",
            "  inflating: data/img/70849.png      \n",
            "  inflating: data/img/14837.png      \n",
            "  inflating: data/img/95087.png      \n",
            "  inflating: data/img/65814.png      \n",
            "  inflating: data/img/61549.png      \n",
            "  inflating: data/img/24130.png      \n",
            "  inflating: data/img/17052.png      \n",
            "  inflating: data/img/93586.png      \n",
            "  inflating: data/img/12894.png      \n",
            "  inflating: data/img/27169.png      \n",
            "  inflating: data/img/46293.png      \n",
            "  inflating: data/img/98316.png      \n",
            "  inflating: data/img/31857.png      \n",
            "  inflating: data/img/47369.png      \n",
            "  inflating: data/img/93718.png      \n",
            "  inflating: data/img/37592.png      \n",
            "  inflating: data/img/87543.png      \n",
            "  inflating: data/img/62791.png      \n",
            "  inflating: data/img/98754.png      \n",
            "  inflating: data/img/17523.png      \n",
            "  inflating: data/img/62017.png      \n",
            "  inflating: data/img/05936.png      \n",
            "  inflating: data/img/57890.png      \n",
            "  inflating: data/img/52743.png      \n",
            "  inflating: data/img/86172.png      \n",
            "  inflating: data/img/45739.png      \n",
            "  inflating: data/img/42019.png      \n",
            "  inflating: data/img/65320.png      \n",
            "  inflating: data/img/13285.png      \n",
            "  inflating: data/img/26179.png      \n",
            "  inflating: data/img/27836.png      \n",
            "  inflating: data/img/50246.png      \n",
            "  inflating: data/img/48920.png      \n",
            "  inflating: data/img/94175.png      \n",
            "  inflating: data/img/71825.png      \n",
            "  inflating: data/img/70368.png      \n",
            "  inflating: data/img/43092.png      \n",
            "  inflating: data/img/62358.png      \n",
            "  inflating: data/img/36751.png      \n",
            "  inflating: data/img/30851.png      \n",
            "  inflating: data/img/13428.png      \n",
            "  inflating: data/img/15642.png      \n",
            "  inflating: data/img/74583.png      \n",
            "  inflating: data/img/87695.png      \n",
            "  inflating: data/img/57312.png      \n",
            "  inflating: data/img/40857.png      \n",
            "  inflating: data/img/49581.png      \n",
            "  inflating: data/img/94216.png      \n",
            "  inflating: data/img/63709.png      \n",
            "  inflating: data/img/53941.png      \n",
            "  inflating: data/img/81273.png      \n",
            "  inflating: data/img/07596.png      \n",
            "  inflating: data/img/07851.png      \n",
            "  inflating: data/img/04629.png      \n",
            "  inflating: data/img/63745.png      \n",
            "  inflating: data/img/34628.png      \n",
            "  inflating: data/img/96204.png      \n",
            "  inflating: data/img/97643.png      \n",
            "  inflating: data/img/37829.png      \n",
            "  inflating: data/img/21567.png      \n",
            "  inflating: data/img/52379.png      \n",
            "  inflating: data/img/64128.png      \n",
            "  inflating: data/img/40561.png      \n",
            "  inflating: data/img/36109.png      \n",
            "  inflating: data/img/27519.png      \n",
            "  inflating: data/img/84351.png      \n",
            "  inflating: data/img/71340.png      \n",
            "  inflating: data/img/50293.png      \n",
            "  inflating: data/img/96541.png      \n",
            "  inflating: data/img/27180.png      \n",
            "  inflating: data/img/53249.png      \n",
            "  inflating: data/img/54098.png      \n",
            "  inflating: data/img/92743.png      \n",
            "  inflating: data/img/48706.png      \n",
            "  inflating: data/img/61820.png      \n",
            "  inflating: data/img/85670.png      \n",
            "  inflating: data/img/83692.png      \n",
            "  inflating: data/img/12647.png      \n",
            "  inflating: data/img/40862.png      \n",
            "  inflating: data/img/30526.png      \n",
            "  inflating: data/img/01835.png      \n",
            "  inflating: data/img/29401.png      \n",
            "  inflating: data/img/80724.png      \n",
            "  inflating: data/img/84609.png      \n",
            "  inflating: data/img/91836.png      \n",
            "  inflating: data/img/73021.png      \n",
            "  inflating: data/img/36452.png      \n",
            "  inflating: data/img/98571.png      \n",
            "  inflating: data/img/25968.png      \n",
            "  inflating: data/img/94081.png      \n",
            "  inflating: data/img/98307.png      \n",
            "  inflating: data/img/48173.png      \n",
            "  inflating: data/img/42056.png      \n",
            "  inflating: data/img/97860.png      \n",
            "  inflating: data/img/49682.png      \n",
            "  inflating: data/img/04859.png      \n",
            "  inflating: data/img/71692.png      \n",
            "  inflating: data/img/34570.png      \n",
            "  inflating: data/img/90654.png      \n",
            "  inflating: data/img/74509.png      \n",
            "  inflating: data/img/25489.png      \n",
            "  inflating: data/img/18407.png      \n",
            "  inflating: data/img/65931.png      \n",
            "  inflating: data/img/62381.png      \n",
            "  inflating: data/img/40815.png      \n",
            "  inflating: data/img/96240.png      \n",
            "  inflating: data/img/27153.png      \n",
            "  inflating: data/img/68901.png      \n",
            "  inflating: data/img/45382.png      \n",
            "  inflating: data/img/45832.png      \n",
            "  inflating: data/img/10752.png      \n",
            "  inflating: data/img/60893.png      \n",
            "  inflating: data/img/30792.png      \n",
            "  inflating: data/img/45702.png      \n",
            "  inflating: data/img/42039.png      \n",
            "  inflating: data/img/74536.png      \n",
            "  inflating: data/img/95267.png      \n",
            "  inflating: data/img/36075.png      \n",
            "  inflating: data/img/08795.png      \n",
            "  inflating: data/img/06479.png      \n",
            "  inflating: data/img/35740.png      \n",
            "  inflating: data/img/45062.png      \n",
            "  inflating: data/img/26547.png      \n",
            "  inflating: data/img/15280.png      \n",
            "  inflating: data/img/03178.png      \n",
            "  inflating: data/img/56938.png      \n",
            "  inflating: data/img/61289.png      \n",
            "  inflating: data/img/70934.png      \n",
            "  inflating: data/img/84209.png      \n",
            "  inflating: data/img/17093.png      \n",
            "  inflating: data/img/17296.png      \n",
            "  inflating: data/img/32891.png      \n",
            "  inflating: data/img/07865.png      \n",
            "  inflating: data/img/82045.png      \n",
            "  inflating: data/img/94153.png      \n",
            "  inflating: data/img/40529.png      \n",
            "  inflating: data/img/31824.png      \n",
            "  inflating: data/img/69041.png      \n",
            "  inflating: data/img/71259.png      \n",
            "  inflating: data/img/69251.png      \n",
            "  inflating: data/img/47390.png      \n",
            "  inflating: data/img/32947.png      \n",
            "  inflating: data/img/17805.png      \n",
            "  inflating: data/img/19823.png      \n",
            "  inflating: data/img/05734.png      \n",
            "  inflating: data/img/98163.png      \n",
            "  inflating: data/img/37864.png      \n",
            "  inflating: data/img/13067.png      \n",
            "  inflating: data/img/43920.png      \n",
            "  inflating: data/img/61973.png      \n",
            "  inflating: data/img/76584.png      \n",
            "  inflating: data/img/37508.png      \n",
            "  inflating: data/img/78401.png      \n",
            "  inflating: data/img/73164.png      \n",
            "  inflating: data/img/30618.png      \n",
            "  inflating: data/img/17524.png      \n",
            "  inflating: data/img/50789.png      \n",
            "  inflating: data/img/29671.png      \n",
            "  inflating: data/img/31687.png      \n",
            "  inflating: data/img/90583.png      \n",
            "  inflating: data/img/35602.png      \n",
            "  inflating: data/img/59380.png      \n",
            "  inflating: data/img/75928.png      \n",
            "  inflating: data/img/23075.png      \n",
            "  inflating: data/img/49527.png      \n",
            "  inflating: data/img/57180.png      \n",
            "  inflating: data/img/12376.png      \n",
            "  inflating: data/img/03728.png      \n",
            "  inflating: data/img/86142.png      \n",
            "  inflating: data/img/30615.png      \n",
            "  inflating: data/img/34852.png      \n",
            "  inflating: data/img/56387.png      \n",
            "  inflating: data/img/02158.png      \n",
            "  inflating: data/img/67948.png      \n",
            "  inflating: data/img/35472.png      \n",
            "  inflating: data/img/63749.png      \n",
            "  inflating: data/img/50236.png      \n",
            "  inflating: data/img/43198.png      \n",
            "  inflating: data/img/23140.png      \n",
            "  inflating: data/img/75894.png      \n",
            "  inflating: data/img/50783.png      \n",
            "  inflating: data/img/60573.png      \n",
            "  inflating: data/img/37128.png      \n",
            "  inflating: data/img/41573.png      \n",
            "  inflating: data/img/84951.png      \n",
            "  inflating: data/img/46817.png      \n",
            "  inflating: data/img/53968.png      \n",
            "  inflating: data/img/37819.png      \n",
            "  inflating: data/img/46837.png      \n",
            "  inflating: data/img/45219.png      \n",
            "  inflating: data/img/49726.png      \n",
            "  inflating: data/img/61908.png      \n",
            "  inflating: data/img/64283.png      \n",
            "  inflating: data/img/96510.png      \n",
            "  inflating: data/img/84237.png      \n",
            "  inflating: data/img/71863.png      \n",
            "  inflating: data/img/81079.png      \n",
            "  inflating: data/img/58329.png      \n",
            "  inflating: data/img/89367.png      \n",
            "  inflating: data/img/07419.png      \n",
            "  inflating: data/img/59346.png      \n",
            "  inflating: data/img/41527.png      \n",
            "  inflating: data/img/49802.png      \n",
            "  inflating: data/img/58971.png      \n",
            "  inflating: data/img/32617.png      \n",
            "  inflating: data/img/34872.png      \n",
            "  inflating: data/img/90257.png      \n",
            "  inflating: data/img/70582.png      \n",
            "  inflating: data/img/31479.png      \n",
            "  inflating: data/img/32570.png      \n",
            "  inflating: data/img/69304.png      \n",
            "  inflating: data/img/15804.png      \n",
            "  inflating: data/img/31742.png      \n",
            "  inflating: data/img/56470.png      \n",
            "  inflating: data/img/42167.png      \n",
            "  inflating: data/img/96258.png      \n",
            "  inflating: data/img/87352.png      \n",
            "  inflating: data/img/59402.png      \n",
            "  inflating: data/img/80239.png      \n",
            "  inflating: data/img/75410.png      \n",
            "  inflating: data/img/63490.png      \n",
            "  inflating: data/img/52130.png      \n",
            "  inflating: data/img/13074.png      \n",
            "  inflating: data/img/69370.png      \n",
            "  inflating: data/img/93086.png      \n",
            "  inflating: data/img/41875.png      \n",
            "  inflating: data/img/54710.png      \n",
            "  inflating: data/img/14259.png      \n",
            "  inflating: data/img/24316.png      \n",
            "  inflating: data/img/79624.png      \n",
            "  inflating: data/img/59641.png      \n",
            "  inflating: data/img/57386.png      \n",
            "  inflating: data/img/50824.png      \n",
            "  inflating: data/img/16592.png      \n",
            "  inflating: data/img/37542.png      \n",
            "  inflating: data/img/52401.png      \n",
            "  inflating: data/img/56973.png      \n",
            "  inflating: data/img/78214.png      \n",
            "  inflating: data/img/39526.png      \n",
            "  inflating: data/img/36501.png      \n",
            "  inflating: data/img/89532.png      \n",
            "  inflating: data/img/06293.png      \n",
            "  inflating: data/img/02461.png      \n",
            "  inflating: data/img/07634.png      \n",
            "  inflating: data/img/48762.png      \n",
            "  inflating: data/img/67325.png      \n",
            "  inflating: data/img/58079.png      \n",
            "  inflating: data/img/97832.png      \n",
            "  inflating: data/img/80914.png      \n",
            "  inflating: data/img/15427.png      \n",
            "  inflating: data/img/46238.png      \n",
            "  inflating: data/img/35482.png      \n",
            "  inflating: data/img/62485.png      \n",
            "  inflating: data/img/31649.png      \n",
            "  inflating: data/img/06875.png      \n",
            "  inflating: data/img/10345.png      \n",
            "  inflating: data/img/65801.png      \n",
            "  inflating: data/img/42986.png      \n",
            "  inflating: data/img/12450.png      \n",
            "  inflating: data/img/45706.png      \n",
            "  inflating: data/img/65704.png      \n",
            "  inflating: data/img/46239.png      \n",
            "  inflating: data/img/28574.png      \n",
            "  inflating: data/img/49635.png      \n",
            "  inflating: data/img/13902.png      \n",
            "  inflating: data/img/02719.png      \n",
            "  inflating: data/img/90814.png      \n",
            "  inflating: data/img/64821.png      \n",
            "  inflating: data/img/07391.png      \n",
            "  inflating: data/img/25816.png      \n",
            "  inflating: data/img/75602.png      \n",
            "  inflating: data/img/71659.png      \n",
            "  inflating: data/img/41908.png      \n",
            "  inflating: data/img/78914.png      \n",
            "  inflating: data/img/56247.png      \n",
            "  inflating: data/img/13876.png      \n",
            "  inflating: data/img/08654.png      \n",
            "  inflating: data/img/80169.png      \n",
            "  inflating: data/img/36789.png      \n",
            "  inflating: data/img/41086.png      \n",
            "  inflating: data/img/17082.png      \n",
            "  inflating: data/img/25640.png      \n",
            "  inflating: data/img/10649.png      \n",
            "  inflating: data/img/87149.png      \n",
            "  inflating: data/img/02918.png      \n",
            "  inflating: data/img/25401.png      \n",
            "  inflating: data/img/16784.png      \n",
            "  inflating: data/img/86273.png      \n",
            "  inflating: data/img/87953.png      \n",
            "  inflating: data/img/46283.png      \n",
            "  inflating: data/img/57938.png      \n",
            "  inflating: data/img/57821.png      \n",
            "  inflating: data/img/96840.png      \n",
            "  inflating: data/img/49075.png      \n",
            "  inflating: data/img/40932.png      \n",
            "  inflating: data/img/12784.png      \n",
            "  inflating: data/img/74183.png      \n",
            "  inflating: data/img/93164.png      \n",
            "  inflating: data/img/23501.png      \n",
            "  inflating: data/img/24189.png      \n",
            "  inflating: data/img/46173.png      \n",
            "  inflating: data/img/31605.png      \n",
            "  inflating: data/img/15493.png      \n",
            "  inflating: data/img/94571.png      \n",
            "  inflating: data/img/56928.png      \n",
            "  inflating: data/img/13069.png      \n",
            "  inflating: data/img/06823.png      \n",
            "  inflating: data/img/37921.png      \n",
            "  inflating: data/img/94302.png      \n",
            "  inflating: data/img/42309.png      \n",
            "  inflating: data/img/69415.png      \n",
            "  inflating: data/img/07396.png      \n",
            "  inflating: data/img/63097.png      \n",
            "  inflating: data/img/64720.png      \n",
            "  inflating: data/img/71630.png      \n",
            "  inflating: data/img/54083.png      \n",
            "  inflating: data/img/31652.png      \n",
            "  inflating: data/img/16758.png      \n",
            "  inflating: data/img/41852.png      \n",
            "  inflating: data/img/85290.png      \n",
            "  inflating: data/img/56123.png      \n",
            "  inflating: data/img/91562.png      \n",
            "  inflating: data/img/16593.png      \n",
            "  inflating: data/img/43758.png      \n",
            "  inflating: data/img/16842.png      \n",
            "  inflating: data/img/72609.png      \n",
            "  inflating: data/img/85721.png      \n",
            "  inflating: data/img/92483.png      \n",
            "  inflating: data/img/20951.png      \n",
            "  inflating: data/img/89432.png      \n",
            "  inflating: data/img/78215.png      \n",
            "  inflating: data/img/03854.png      \n",
            "  inflating: data/img/69410.png      \n",
            "  inflating: data/img/40592.png      \n",
            "  inflating: data/img/26709.png      \n",
            "  inflating: data/img/74362.png      \n",
            "  inflating: data/img/05376.png      \n",
            "  inflating: data/img/46973.png      \n",
            "  inflating: data/img/94586.png      \n",
            "  inflating: data/img/42510.png      \n",
            "  inflating: data/img/92581.png      \n",
            "  inflating: data/img/79280.png      \n",
            "  inflating: data/img/21387.png      \n",
            "  inflating: data/img/23504.png      \n",
            "  inflating: data/img/98241.png      \n",
            "  inflating: data/img/46529.png      \n",
            "  inflating: data/img/03162.png      \n",
            "  inflating: data/img/60812.png      \n",
            "  inflating: data/img/97826.png      \n",
            "  inflating: data/img/83271.png      \n",
            "  inflating: data/img/15872.png      \n",
            "  inflating: data/img/21354.png      \n",
            "  inflating: data/img/74098.png      \n",
            "  inflating: data/img/67498.png      \n",
            "  inflating: data/img/96354.png      \n",
            "  inflating: data/img/89406.png      \n",
            "  inflating: data/img/81036.png      \n",
            "  inflating: data/img/80965.png      \n",
            "  inflating: data/img/28043.png      \n",
            "  inflating: data/img/56048.png      \n",
            "  inflating: data/img/13894.png      \n",
            "  inflating: data/img/78452.png      \n",
            "  inflating: data/img/06489.png      \n",
            "  inflating: data/img/84913.png      \n",
            "  inflating: data/img/96023.png      \n",
            "  inflating: data/img/06357.png      \n",
            "  inflating: data/img/41672.png      \n",
            "  inflating: data/img/48623.png      \n",
            "  inflating: data/img/08251.png      \n",
            "  inflating: data/img/84103.png      \n",
            "  inflating: data/img/45267.png      \n",
            "  inflating: data/img/52796.png      \n",
            "  inflating: data/img/42983.png      \n",
            "  inflating: data/img/98176.png      \n",
            "  inflating: data/img/93027.png      \n",
            "  inflating: data/img/10362.png      \n",
            "  inflating: data/img/97315.png      \n",
            "  inflating: data/img/38647.png      \n",
            "  inflating: data/img/54962.png      \n",
            "  inflating: data/img/20837.png      \n",
            "  inflating: data/img/07924.png      \n",
            "  inflating: data/img/94850.png      \n",
            "  inflating: data/img/87013.png      \n",
            "  inflating: data/img/74268.png      \n",
            "  inflating: data/img/13570.png      \n",
            "  inflating: data/img/42987.png      \n",
            "  inflating: data/img/59328.png      \n",
            "  inflating: data/img/17280.png      \n",
            "  inflating: data/img/18254.png      \n",
            "  inflating: data/img/12096.png      \n",
            "  inflating: data/img/17385.png      \n",
            "  inflating: data/img/85461.png      \n",
            "  inflating: data/img/97301.png      \n",
            "  inflating: data/img/08471.png      \n",
            "  inflating: data/img/17439.png      \n",
            "  inflating: data/img/64280.png      \n",
            "  inflating: data/img/94086.png      \n",
            "  inflating: data/img/84719.png      \n",
            "  inflating: data/img/92718.png      \n",
            "  inflating: data/img/86251.png      \n",
            "  inflating: data/img/37145.png      \n",
            "  inflating: data/img/34275.png      \n",
            "  inflating: data/img/07259.png      \n",
            "  inflating: data/img/08597.png      \n",
            "  inflating: data/img/51836.png      \n",
            "  inflating: data/img/92847.png      \n",
            "  inflating: data/img/40856.png      \n",
            "  inflating: data/img/34715.png      \n",
            "  inflating: data/img/53147.png      \n",
            "  inflating: data/img/68951.png      \n",
            "  inflating: data/img/91704.png      \n",
            "  inflating: data/img/68594.png      \n",
            "  inflating: data/img/19065.png      \n",
            "  inflating: data/img/27485.png      \n",
            "  inflating: data/img/24098.png      \n",
            "  inflating: data/img/87230.png      \n",
            "  inflating: data/img/27318.png      \n",
            "  inflating: data/img/84729.png      \n",
            "  inflating: data/img/46510.png      \n",
            "  inflating: data/img/68753.png      \n",
            "  inflating: data/img/94615.png      \n",
            "  inflating: data/img/46850.png      \n",
            "  inflating: data/img/09432.png      \n",
            "  inflating: data/img/40137.png      \n",
            "  inflating: data/img/13607.png      \n",
            "  inflating: data/img/29041.png      \n",
            "  inflating: data/img/32781.png      \n",
            "  inflating: data/img/64327.png      \n",
            "  inflating: data/img/70126.png      \n",
            "  inflating: data/img/30782.png      \n",
            "  inflating: data/img/47263.png      \n",
            "  inflating: data/img/79823.png      \n",
            "  inflating: data/img/26043.png      \n",
            "  inflating: data/img/78610.png      \n",
            "  inflating: data/img/59613.png      \n",
            "  inflating: data/img/16895.png      \n",
            "  inflating: data/img/05824.png      \n",
            "  inflating: data/img/89026.png      \n",
            "  inflating: data/img/10867.png      \n",
            "  inflating: data/img/74608.png      \n",
            "  inflating: data/img/45827.png      \n",
            "  inflating: data/img/18526.png      \n",
            "  inflating: data/img/15489.png      \n",
            "  inflating: data/img/14793.png      \n",
            "  inflating: data/img/04719.png      \n",
            "  inflating: data/img/89526.png      \n",
            "  inflating: data/img/79328.png      \n",
            "  inflating: data/img/65248.png      \n",
            "  inflating: data/img/15690.png      \n",
            "  inflating: data/img/70529.png      \n",
            "  inflating: data/img/18059.png      \n",
            "  inflating: data/img/40269.png      \n",
            "  inflating: data/img/31527.png      \n",
            "  inflating: data/img/98731.png      \n",
            "  inflating: data/img/62405.png      \n",
            "  inflating: data/img/21059.png      \n",
            "  inflating: data/img/62305.png      \n",
            "  inflating: data/img/71052.png      \n",
            "  inflating: data/img/93506.png      \n",
            "  inflating: data/img/14086.png      \n",
            "  inflating: data/img/58612.png      \n",
            "  inflating: data/img/24395.png      \n",
            "  inflating: data/img/28315.png      \n",
            "  inflating: data/img/23510.png      \n",
            "  inflating: data/img/62790.png      \n",
            "  inflating: data/img/36541.png      \n",
            "  inflating: data/img/24650.png      \n",
            "  inflating: data/img/98721.png      \n",
            "  inflating: data/img/35160.png      \n",
            "  inflating: data/img/92637.png      \n",
            "  inflating: data/img/93187.png      \n",
            "  inflating: data/img/37918.png      \n",
            "  inflating: data/img/28051.png      \n",
            "  inflating: data/img/68324.png      \n",
            "  inflating: data/img/57849.png      \n",
            "  inflating: data/img/48925.png      \n",
            "  inflating: data/img/63712.png      \n",
            "  inflating: data/img/20971.png      \n",
            "  inflating: data/img/69708.png      \n",
            "  inflating: data/img/23415.png      \n",
            "  inflating: data/img/83127.png      \n",
            "  inflating: data/img/03187.png      \n",
            "  inflating: data/img/91026.png      \n",
            "  inflating: data/img/69720.png      \n",
            "  inflating: data/img/62409.png      \n",
            "  inflating: data/img/65328.png      \n",
            "  inflating: data/img/05398.png      \n",
            "  inflating: data/img/34981.png      \n",
            "  inflating: data/img/19837.png      \n",
            "  inflating: data/img/34057.png      \n",
            "  inflating: data/img/86954.png      \n",
            "  inflating: data/img/58309.png      \n",
            "  inflating: data/img/94067.png      \n",
            "  inflating: data/img/79286.png      \n",
            "  inflating: data/img/35708.png      \n",
            "  inflating: data/img/17802.png      \n",
            "  inflating: data/img/18275.png      \n",
            "  inflating: data/img/71308.png      \n",
            "  inflating: data/img/75430.png      \n",
            "  inflating: data/img/84297.png      \n",
            "  inflating: data/img/54893.png      \n",
            "  inflating: data/img/48291.png      \n",
            "  inflating: data/img/49028.png      \n",
            "  inflating: data/img/59731.png      \n",
            "  inflating: data/img/39785.png      \n",
            "  inflating: data/img/83560.png      \n",
            "  inflating: data/img/68194.png      \n",
            "  inflating: data/img/23014.png      \n",
            "  inflating: data/img/07134.png      \n",
            "  inflating: data/img/98271.png      \n",
            "  inflating: data/img/62035.png      \n",
            "  inflating: data/img/58124.png      \n",
            "  inflating: data/img/07219.png      \n",
            "  inflating: data/img/43675.png      \n",
            "  inflating: data/img/51809.png      \n",
            "  inflating: data/img/28639.png      \n",
            "  inflating: data/img/49173.png      \n",
            "  inflating: data/img/48316.png      \n",
            "  inflating: data/img/40826.png      \n",
            "  inflating: data/img/10236.png      \n",
            "  inflating: data/img/74562.png      \n",
            "  inflating: data/img/67243.png      \n",
            "  inflating: data/img/91750.png      \n",
            "  inflating: data/img/37419.png      \n",
            "  inflating: data/img/78631.png      \n",
            "  inflating: data/img/69470.png      \n",
            "  inflating: data/img/41680.png      \n",
            "  inflating: data/img/09831.png      \n",
            "  inflating: data/img/87024.png      \n",
            "  inflating: data/img/59384.png      \n",
            "  inflating: data/img/94675.png      \n",
            "  inflating: data/img/01395.png      \n",
            "  inflating: data/img/92741.png      \n",
            "  inflating: data/img/51736.png      \n",
            "  inflating: data/img/89126.png      \n",
            "  inflating: data/img/80379.png      \n",
            "  inflating: data/img/38761.png      \n",
            "  inflating: data/img/63451.png      \n",
            "  inflating: data/img/98245.png      \n",
            "  inflating: data/img/98473.png      \n",
            "  inflating: data/img/69520.png      \n",
            "  inflating: data/img/90673.png      \n",
            "  inflating: data/img/71428.png      \n",
            "  inflating: data/img/47286.png      \n",
            "  inflating: data/img/04712.png      \n",
            "  inflating: data/img/09285.png      \n",
            "  inflating: data/img/06794.png      \n",
            "  inflating: data/img/70942.png      \n",
            "  inflating: data/img/76840.png      \n",
            "  inflating: data/img/78104.png      \n",
            "  inflating: data/img/15609.png      \n",
            "  inflating: data/img/09138.png      \n",
            "  inflating: data/img/43815.png      \n",
            "  inflating: data/img/47518.png      \n",
            "  inflating: data/img/65108.png      \n",
            "  inflating: data/img/95830.png      \n",
            "  inflating: data/img/36472.png      \n",
            "  inflating: data/img/16452.png      \n",
            "  inflating: data/img/39675.png      \n",
            "  inflating: data/img/31782.png      \n",
            "  inflating: data/img/76458.png      \n",
            "  inflating: data/img/80197.png      \n",
            "  inflating: data/img/58916.png      \n",
            "  inflating: data/img/73526.png      \n",
            "  inflating: data/img/05387.png      \n",
            "  inflating: data/img/36054.png      \n",
            "  inflating: data/img/03574.png      \n",
            "  inflating: data/img/17290.png      \n",
            "  inflating: data/img/31570.png      \n",
            "  inflating: data/img/29643.png      \n",
            "  inflating: data/img/81902.png      \n",
            "  inflating: data/img/89275.png      \n",
            "  inflating: data/img/02783.png      \n",
            "  inflating: data/img/80256.png      \n",
            "  inflating: data/img/97810.png      \n",
            "  inflating: data/img/80942.png      \n",
            "  inflating: data/img/74380.png      \n",
            "  inflating: data/img/89136.png      \n",
            "  inflating: data/img/39168.png      \n",
            "  inflating: data/img/83965.png      \n",
            "  inflating: data/img/30174.png      \n",
            "  inflating: data/img/48751.png      \n",
            "  inflating: data/img/07413.png      \n",
            "  inflating: data/img/07623.png      \n",
            "  inflating: data/img/03281.png      \n",
            "  inflating: data/img/90471.png      \n",
            "  inflating: data/img/58473.png      \n",
            "  inflating: data/img/81374.png      \n",
            "  inflating: data/img/91430.png      \n",
            "  inflating: data/img/05918.png      \n",
            "  inflating: data/img/83064.png      \n",
            "  inflating: data/img/65794.png      \n",
            "  inflating: data/img/73485.png      \n",
            "  inflating: data/img/01348.png      \n",
            "  inflating: data/img/60739.png      \n",
            "  inflating: data/img/56327.png      \n",
            "  inflating: data/img/82310.png      \n",
            "  inflating: data/img/68019.png      \n",
            "  inflating: data/img/08261.png      \n",
            "  inflating: data/img/42137.png      \n",
            "  inflating: data/img/52071.png      \n",
            "  inflating: data/img/05438.png      \n",
            "  inflating: data/img/34106.png      \n",
            "  inflating: data/img/71823.png      \n",
            "  inflating: data/img/89076.png      \n",
            "  inflating: data/img/89063.png      \n",
            "  inflating: data/img/08632.png      \n",
            "  inflating: data/img/27860.png      \n",
            "  inflating: data/img/13596.png      \n",
            "  inflating: data/img/90345.png      \n",
            "  inflating: data/img/41568.png      \n",
            "  inflating: data/img/06123.png      \n",
            "  inflating: data/img/78164.png      \n",
            "  inflating: data/img/38127.png      \n",
            "  inflating: data/img/28450.png      \n",
            "  inflating: data/img/04263.png      \n",
            "  inflating: data/img/51964.png      \n",
            "  inflating: data/img/24859.png      \n",
            "  inflating: data/img/53280.png      \n",
            "  inflating: data/img/83205.png      \n",
            "  inflating: data/img/75693.png      \n",
            "  inflating: data/img/06514.png      \n",
            "  inflating: data/img/32897.png      \n",
            "  inflating: data/img/35298.png      \n",
            "  inflating: data/img/45136.png      \n",
            "  inflating: data/img/59862.png      \n",
            "  inflating: data/img/57034.png      \n",
            "  inflating: data/img/59478.png      \n",
            "  inflating: data/img/30896.png      \n",
            "  inflating: data/img/52619.png      \n",
            "  inflating: data/img/14652.png      \n",
            "  inflating: data/img/38752.png      \n",
            "  inflating: data/img/25103.png      \n",
            "  inflating: data/img/14275.png      \n",
            "  inflating: data/img/28534.png      \n",
            "  inflating: data/img/89640.png      \n",
            "  inflating: data/img/39054.png      \n",
            "  inflating: data/img/95186.png      \n",
            "  inflating: data/img/71263.png      \n",
            "  inflating: data/img/53827.png      \n",
            "  inflating: data/img/67913.png      \n",
            "  inflating: data/img/47506.png      \n",
            "  inflating: data/img/83257.png      \n",
            "  inflating: data/img/40728.png      \n",
            "  inflating: data/img/64592.png      \n",
            "  inflating: data/img/69380.png      \n",
            "  inflating: data/img/81624.png      \n",
            "  inflating: data/img/82964.png      \n",
            "  inflating: data/img/93087.png      \n",
            "  inflating: data/img/71986.png      \n",
            "  inflating: data/img/24806.png      \n",
            "  inflating: data/img/90256.png      \n",
            "  inflating: data/img/80769.png      \n",
            "  inflating: data/img/79614.png      \n",
            "  inflating: data/img/07658.png      \n",
            "  inflating: data/img/51978.png      \n",
            "  inflating: data/img/09547.png      \n",
            "  inflating: data/img/79652.png      \n",
            "  inflating: data/img/68934.png      \n",
            "  inflating: data/img/78192.png      \n",
            "  inflating: data/img/15647.png      \n",
            "  inflating: data/img/13058.png      \n",
            "  inflating: data/img/78541.png      \n",
            "  inflating: data/img/14976.png      \n",
            "  inflating: data/img/76952.png      \n",
            "  inflating: data/img/09217.png      \n",
            "  inflating: data/img/15603.png      \n",
            "  inflating: data/img/51032.png      \n",
            "  inflating: data/img/52301.png      \n",
            "  inflating: data/img/78325.png      \n",
            "  inflating: data/img/57908.png      \n",
            "  inflating: data/img/50871.png      \n",
            "  inflating: data/img/62391.png      \n",
            "  inflating: data/img/13068.png      \n",
            "  inflating: data/img/75308.png      \n",
            "  inflating: data/img/40839.png      \n",
            "  inflating: data/img/75601.png      \n",
            "  inflating: data/img/72893.png      \n",
            "  inflating: data/img/37802.png      \n",
            "  inflating: data/img/51304.png      \n",
            "  inflating: data/img/59024.png      \n",
            "  inflating: data/img/34920.png      \n",
            "  inflating: data/img/67092.png      \n",
            "  inflating: data/img/47516.png      \n",
            "  inflating: data/img/30489.png      \n",
            "  inflating: data/img/68941.png      \n",
            "  inflating: data/img/48205.png      \n",
            "  inflating: data/img/34862.png      \n",
            "  inflating: data/img/67491.png      \n",
            "  inflating: data/img/56482.png      \n",
            "  inflating: data/img/56378.png      \n",
            "  inflating: data/img/25481.png      \n",
            "  inflating: data/img/72014.png      \n",
            "  inflating: data/img/53618.png      \n",
            "  inflating: data/img/38495.png      \n",
            "  inflating: data/img/86910.png      \n",
            "  inflating: data/img/06147.png      \n",
            "  inflating: data/img/40136.png      \n",
            "  inflating: data/img/87523.png      \n",
            "  inflating: data/img/07652.png      \n",
            "  inflating: data/img/69804.png      \n",
            "  inflating: data/img/26034.png      \n",
            "  inflating: data/img/96284.png      \n",
            "  inflating: data/img/41689.png      \n",
            "  inflating: data/img/32704.png      \n",
            "  inflating: data/img/29163.png      \n",
            "  inflating: data/img/92136.png      \n",
            "  inflating: data/img/70435.png      \n",
            "  inflating: data/img/36190.png      \n",
            "  inflating: data/img/65712.png      \n",
            "  inflating: data/img/36875.png      \n",
            "  inflating: data/img/15283.png      \n",
            "  inflating: data/img/64907.png      \n",
            "  inflating: data/img/87625.png      \n",
            "  inflating: data/img/08962.png      \n",
            "  inflating: data/img/52304.png      \n",
            "  inflating: data/img/73250.png      \n",
            "  inflating: data/img/90618.png      \n",
            "  inflating: data/img/86457.png      \n",
            "  inflating: data/img/61937.png      \n",
            "  inflating: data/img/41873.png      \n",
            "  inflating: data/img/13640.png      \n",
            "  inflating: data/img/16354.png      \n",
            "  inflating: data/img/81279.png      \n",
            "  inflating: data/img/61270.png      \n",
            "  inflating: data/img/31284.png      \n",
            "  inflating: data/img/10659.png      \n",
            "  inflating: data/img/31627.png      \n",
            "  inflating: data/img/43570.png      \n",
            "  inflating: data/img/34015.png      \n",
            "  inflating: data/img/12034.png      \n",
            "  inflating: data/img/57831.png      \n",
            "  inflating: data/img/54930.png      \n",
            "  inflating: data/img/25170.png      \n",
            "  inflating: data/img/82547.png      \n",
            "  inflating: data/img/80627.png      \n",
            "  inflating: data/img/07615.png      \n",
            "  inflating: data/img/62471.png      \n",
            "  inflating: data/img/85709.png      \n",
            "  inflating: data/img/08476.png      \n",
            "  inflating: data/img/65824.png      \n",
            "  inflating: data/img/80591.png      \n",
            "  inflating: data/img/93051.png      \n",
            "  inflating: data/img/09352.png      \n",
            "  inflating: data/img/57084.png      \n",
            "  inflating: data/img/39612.png      \n",
            "  inflating: data/img/72850.png      \n",
            "  inflating: data/img/53219.png      \n",
            "  inflating: data/img/41890.png      \n",
            "  inflating: data/img/84523.png      \n",
            "  inflating: data/img/48260.png      \n",
            "  inflating: data/img/42931.png      \n",
            "  inflating: data/img/34952.png      \n",
            "  inflating: data/img/65187.png      \n",
            "  inflating: data/img/96250.png      \n",
            "  inflating: data/img/46582.png      \n",
            "  inflating: data/img/94361.png      \n",
            "  inflating: data/img/21730.png      \n",
            "  inflating: data/img/54709.png      \n",
            "  inflating: data/img/42351.png      \n",
            "  inflating: data/img/65980.png      \n",
            "  inflating: data/img/87645.png      \n",
            "  inflating: data/img/34295.png      \n",
            "  inflating: data/img/65201.png      \n",
            "  inflating: data/img/46217.png      \n",
            "  inflating: data/img/52986.png      \n",
            "  inflating: data/img/05948.png      \n",
            "  inflating: data/img/65094.png      \n",
            "  inflating: data/img/49806.png      \n",
            "  inflating: data/img/64127.png      \n",
            "  inflating: data/img/24973.png      \n",
            "  inflating: data/img/72489.png      \n",
            "  inflating: data/img/96421.png      \n",
            "  inflating: data/img/13726.png      \n",
            "  inflating: data/img/56130.png      \n",
            "  inflating: data/img/12430.png      \n",
            "  inflating: data/img/46271.png      \n",
            "  inflating: data/img/79043.png      \n",
            "  inflating: data/img/95843.png      \n",
            "  inflating: data/img/85967.png      \n",
            "  inflating: data/img/29463.png      \n",
            "  inflating: data/img/80537.png      \n",
            "  inflating: data/img/52783.png      \n",
            "  inflating: data/img/25904.png      \n",
            "  inflating: data/img/49185.png      \n",
            "  inflating: data/img/98702.png      \n",
            "  inflating: data/img/75136.png      \n",
            "  inflating: data/img/17894.png      \n",
            "  inflating: data/img/97415.png      \n",
            "  inflating: data/img/45176.png      \n",
            "  inflating: data/img/74801.png      \n",
            "  inflating: data/img/43698.png      \n",
            "  inflating: data/img/16870.png      \n",
            "  inflating: data/img/02384.png      \n",
            "  inflating: data/img/87934.png      \n",
            "  inflating: data/img/48015.png      \n",
            "  inflating: data/img/23901.png      \n",
            "  inflating: data/img/45289.png      \n",
            "  inflating: data/img/57208.png      \n",
            "  inflating: data/img/49157.png      \n",
            "  inflating: data/img/30914.png      \n",
            "  inflating: data/img/85716.png      \n",
            "  inflating: data/img/86205.png      \n",
            "  inflating: data/img/37658.png      \n",
            "  inflating: data/img/67835.png      \n",
            "  inflating: data/img/23954.png      \n",
            "  inflating: data/img/45031.png      \n",
            "  inflating: data/img/16098.png      \n",
            "  inflating: data/img/02139.png      \n",
            "  inflating: data/img/28391.png      \n",
            "  inflating: data/img/12876.png      \n",
            "  inflating: data/img/96730.png      \n",
            "  inflating: data/img/60185.png      \n",
            "  inflating: data/img/35618.png      \n",
            "  inflating: data/img/26750.png      \n",
            "  inflating: data/img/56489.png      \n",
            "  inflating: data/img/73945.png      \n",
            "  inflating: data/img/10793.png      \n",
            "  inflating: data/img/62571.png      \n",
            "  inflating: data/img/28160.png      \n",
            "  inflating: data/img/81305.png      \n",
            "  inflating: data/img/21650.png      \n",
            "  inflating: data/img/07469.png      \n",
            "  inflating: data/img/93145.png      \n",
            "  inflating: data/img/95814.png      \n",
            "  inflating: data/img/93761.png      \n",
            "  inflating: data/img/34051.png      \n",
            "  inflating: data/img/25468.png      \n",
            "  inflating: data/img/40653.png      \n",
            "  inflating: data/img/69542.png      \n",
            "  inflating: data/img/57401.png      \n",
            "  inflating: data/img/37681.png      \n",
            "  inflating: data/img/20371.png      \n",
            "  inflating: data/img/54981.png      \n",
            "  inflating: data/img/26537.png      \n",
            "  inflating: data/img/01972.png      \n",
            "  inflating: data/img/96570.png      \n",
            "  inflating: data/img/96230.png      \n",
            "  inflating: data/img/72594.png      \n",
            "  inflating: data/img/62851.png      \n",
            "  inflating: data/img/08941.png      \n",
            "  inflating: data/img/81467.png      \n",
            "  inflating: data/img/70381.png      \n",
            "  inflating: data/img/04267.png      \n",
            "  inflating: data/img/20861.png      \n",
            "  inflating: data/img/54719.png      \n",
            "  inflating: data/img/71846.png      \n",
            "  inflating: data/img/90612.png      \n",
            "  inflating: data/img/31765.png      \n",
            "  inflating: data/img/37091.png      \n",
            "  inflating: data/img/32714.png      \n",
            "  inflating: data/img/40627.png      \n",
            "  inflating: data/img/10483.png      \n",
            "  inflating: data/img/59140.png      \n",
            "  inflating: data/img/36524.png      \n",
            "  inflating: data/img/81249.png      \n",
            "  inflating: data/img/20413.png      \n",
            "  inflating: data/img/92603.png      \n",
            "  inflating: data/img/28319.png      \n",
            "  inflating: data/img/47123.png      \n",
            "  inflating: data/img/54713.png      \n",
            "  inflating: data/img/86714.png      \n",
            "  inflating: data/img/28951.png      \n",
            "  inflating: data/img/27489.png      \n",
            "  inflating: data/img/84072.png      \n",
            "  inflating: data/img/06723.png      \n",
            "  inflating: data/img/19582.png      \n",
            "  inflating: data/img/27803.png      \n",
            "  inflating: data/img/36578.png      \n",
            "  inflating: data/img/39482.png      \n",
            "  inflating: data/img/54290.png      \n",
            "  inflating: data/img/75204.png      \n",
            "  inflating: data/img/04975.png      \n",
            "  inflating: data/img/97250.png      \n",
            "  inflating: data/img/08349.png      \n",
            "  inflating: data/img/47903.png      \n",
            "  inflating: data/img/48031.png      \n",
            "  inflating: data/img/51768.png      \n",
            "  inflating: data/img/10943.png      \n",
            "  inflating: data/img/42153.png      \n",
            "  inflating: data/img/84709.png      \n",
            "  inflating: data/img/85410.png      \n",
            "  inflating: data/img/05379.png      \n",
            "  inflating: data/img/91436.png      \n",
            "  inflating: data/img/03971.png      \n",
            "  inflating: data/img/43207.png      \n",
            "  inflating: data/img/96578.png      \n",
            "  inflating: data/img/31720.png      \n",
            "  inflating: data/img/12843.png      \n",
            "  inflating: data/img/96185.png      \n",
            "  inflating: data/img/72305.png      \n",
            "  inflating: data/img/13459.png      \n",
            "  inflating: data/img/37289.png      \n",
            "  inflating: data/img/81956.png      \n",
            "  inflating: data/img/68342.png      \n",
            "  inflating: data/img/95103.png      \n",
            "  inflating: data/img/84172.png      \n",
            "  inflating: data/img/70264.png      \n",
            "  inflating: data/img/93875.png      \n",
            "  inflating: data/img/49728.png      \n",
            "  inflating: data/img/68947.png      \n",
            "  inflating: data/img/12534.png      \n",
            "  inflating: data/img/93076.png      \n",
            "  inflating: data/img/40629.png      \n",
            "  inflating: data/img/36470.png      \n",
            "  inflating: data/img/53124.png      \n",
            "  inflating: data/img/08624.png      \n",
            "  inflating: data/img/01746.png      \n",
            "  inflating: data/img/20379.png      \n",
            "  inflating: data/img/42503.png      \n",
            "  inflating: data/img/67048.png      \n",
            "  inflating: data/img/81724.png      \n",
            "  inflating: data/img/07653.png      \n",
            "  inflating: data/img/70324.png      \n",
            "  inflating: data/img/78459.png      \n",
            "  inflating: data/img/19372.png      \n",
            "  inflating: data/img/41697.png      \n",
            "  inflating: data/img/57649.png      \n",
            "  inflating: data/img/49136.png      \n",
            "  inflating: data/img/94013.png      \n",
            "  inflating: data/img/68253.png      \n",
            "  inflating: data/img/02816.png      \n",
            "  inflating: data/img/10652.png      \n",
            "  inflating: data/img/74630.png      \n",
            "  inflating: data/img/24537.png      \n",
            "  inflating: data/img/19072.png      \n",
            "  inflating: data/img/40218.png      \n",
            "  inflating: data/img/28154.png      \n",
            "  inflating: data/img/73846.png      \n",
            "  inflating: data/img/40916.png      \n",
            "  inflating: data/img/19537.png      \n",
            "  inflating: data/img/38105.png      \n",
            "  inflating: data/img/07825.png      \n",
            "  inflating: data/img/31876.png      \n",
            "  inflating: data/img/80195.png      \n",
            "  inflating: data/img/38109.png      \n",
            "  inflating: data/img/06945.png      \n",
            "  inflating: data/img/42897.png      \n",
            "  inflating: data/img/94860.png      \n",
            "  inflating: data/img/25493.png      \n",
            "  inflating: data/img/07239.png      \n",
            "  inflating: data/img/91842.png      \n",
            "  inflating: data/img/54376.png      \n",
            "  inflating: data/img/81259.png      \n",
            "  inflating: data/img/98134.png      \n",
            "  inflating: data/img/04782.png      \n",
            "  inflating: data/img/43680.png      \n",
            "  inflating: data/img/79312.png      \n",
            "  inflating: data/img/07456.png      \n",
            "  inflating: data/img/38460.png      \n",
            "  inflating: data/img/93410.png      \n",
            "  inflating: data/img/16045.png      \n",
            "  inflating: data/img/47819.png      \n",
            "  inflating: data/img/74326.png      \n",
            "  inflating: data/img/96704.png      \n",
            "  inflating: data/img/13276.png      \n",
            "  inflating: data/img/61894.png      \n",
            "  inflating: data/img/05741.png      \n",
            "  inflating: data/img/05736.png      \n",
            "  inflating: data/img/56417.png      \n",
            "  inflating: data/img/29467.png      \n",
            "  inflating: data/img/93610.png      \n",
            "  inflating: data/img/43810.png      \n",
            "  inflating: data/img/42751.png      \n",
            "  inflating: data/img/05789.png      \n",
            "  inflating: data/img/04675.png      \n",
            "  inflating: data/img/89465.png      \n",
            "  inflating: data/img/70923.png      \n",
            "  inflating: data/img/62931.png      \n",
            "  inflating: data/img/25179.png      \n",
            "  inflating: data/img/67051.png      \n",
            "  inflating: data/img/49385.png      \n",
            "  inflating: data/img/03715.png      \n",
            "  inflating: data/img/49372.png      \n",
            "  inflating: data/img/12309.png      \n",
            "  inflating: data/img/61754.png      \n",
            "  inflating: data/img/31280.png      \n",
            "  inflating: data/img/83571.png      \n",
            "  inflating: data/img/26598.png      \n",
            "  inflating: data/img/91803.png      \n",
            "  inflating: data/img/51476.png      \n",
            "  inflating: data/img/90826.png      \n",
            "  inflating: data/img/82591.png      \n",
            "  inflating: data/img/02467.png      \n",
            "  inflating: data/img/09312.png      \n",
            "  inflating: data/img/71832.png      \n",
            "  inflating: data/img/18430.png      \n",
            "  inflating: data/img/95426.png      \n",
            "  inflating: data/img/54018.png      \n",
            "  inflating: data/img/92430.png      \n",
            "  inflating: data/img/85327.png      \n",
            "  inflating: data/img/51037.png      \n",
            "  inflating: data/img/15803.png      \n",
            "  inflating: data/img/10548.png      \n",
            "  inflating: data/img/43025.png      \n",
            "  inflating: data/img/21647.png      \n",
            "  inflating: data/img/13609.png      \n",
            "  inflating: data/img/28369.png      \n",
            "  inflating: data/img/21943.png      \n",
            "  inflating: data/img/40829.png      \n",
            "  inflating: data/img/39854.png      \n",
            "  inflating: data/img/70329.png      \n",
            "  inflating: data/img/24106.png      \n",
            "  inflating: data/img/54780.png      \n",
            "  inflating: data/img/79682.png      \n",
            "  inflating: data/img/28567.png      \n",
            "  inflating: data/img/42093.png      \n",
            "  inflating: data/img/19247.png      \n",
            "  inflating: data/img/27059.png      \n",
            "  inflating: data/img/73251.png      \n",
            "  inflating: data/img/36741.png      \n",
            "  inflating: data/img/39540.png      \n",
            "  inflating: data/img/30847.png      \n",
            "  inflating: data/img/52890.png      \n",
            "  inflating: data/img/79085.png      \n",
            "  inflating: data/img/39827.png      \n",
            "  inflating: data/img/69057.png      \n",
            "  inflating: data/img/34619.png      \n",
            "  inflating: data/img/63710.png      \n",
            "  inflating: data/img/51807.png      \n",
            "  inflating: data/img/94287.png      \n",
            "  inflating: data/img/28163.png      \n",
            "  inflating: data/img/47251.png      \n",
            "  inflating: data/img/92705.png      \n",
            "  inflating: data/img/26049.png      \n",
            "  inflating: data/img/01258.png      \n",
            "  inflating: data/img/10567.png      \n",
            "  inflating: data/img/38546.png      \n",
            "  inflating: data/img/40512.png      \n",
            "  inflating: data/img/45976.png      \n",
            "  inflating: data/img/29380.png      \n",
            "  inflating: data/img/42615.png      \n",
            "  inflating: data/img/45819.png      \n",
            "  inflating: data/img/19458.png      \n",
            "  inflating: data/img/38527.png      \n",
            "  inflating: data/img/07968.png      \n",
            "  inflating: data/img/68521.png      \n",
            "  inflating: data/img/21583.png      \n",
            "  inflating: data/img/79406.png      \n",
            "  inflating: data/img/69732.png      \n",
            "  inflating: data/img/48630.png      \n",
            "  inflating: data/img/09152.png      \n",
            "  inflating: data/img/89360.png      \n",
            "  inflating: data/img/20681.png      \n",
            "  inflating: data/img/98716.png      \n",
            "  inflating: data/img/96874.png      \n",
            "  inflating: data/img/74126.png      \n",
            "  inflating: data/img/94132.png      \n",
            "  inflating: data/img/96183.png      \n",
            "  inflating: data/img/25847.png      \n",
            "  inflating: data/img/97804.png      \n",
            "  inflating: data/img/20619.png      \n",
            "  inflating: data/img/60798.png      \n",
            "  inflating: data/img/25478.png      \n",
            "  inflating: data/img/28930.png      \n",
            "  inflating: data/img/39541.png      \n",
            "  inflating: data/img/16049.png      \n",
            "  inflating: data/img/36107.png      \n",
            "  inflating: data/img/41650.png      \n",
            "  inflating: data/img/73609.png      \n",
            "  inflating: data/img/72964.png      \n",
            "  inflating: data/img/15049.png      \n",
            "  inflating: data/img/57142.png      \n",
            "  inflating: data/img/70153.png      \n",
            "  inflating: data/img/82169.png      \n",
            "  inflating: data/img/68597.png      \n",
            "  inflating: data/img/50741.png      \n",
            "  inflating: data/img/04813.png      \n",
            "  inflating: data/img/06973.png      \n",
            "  inflating: data/img/78395.png      \n",
            "  inflating: data/img/34698.png      \n",
            "  inflating: data/img/09457.png      \n",
            "  inflating: data/img/01439.png      \n",
            "  inflating: data/img/08741.png      \n",
            "  inflating: data/img/84537.png      \n",
            "  inflating: data/img/45810.png      \n",
            "  inflating: data/img/93650.png      \n",
            "  inflating: data/img/85426.png      \n",
            "  inflating: data/img/17450.png      \n",
            "  inflating: data/img/67439.png      \n",
            "  inflating: data/img/52710.png      \n",
            "  inflating: data/img/54793.png      \n",
            "  inflating: data/img/86437.png      \n",
            "  inflating: data/img/36981.png      \n",
            "  inflating: data/img/94523.png      \n",
            "  inflating: data/img/25063.png      \n",
            "  inflating: data/img/63952.png      \n",
            "  inflating: data/img/56713.png      \n",
            "  inflating: data/img/82413.png      \n",
            "  inflating: data/img/26410.png      \n",
            "  inflating: data/img/24738.png      \n",
            "  inflating: data/img/01845.png      \n",
            "  inflating: data/img/35249.png      \n",
            "  inflating: data/img/59701.png      \n",
            "  inflating: data/img/40259.png      \n",
            "  inflating: data/img/59123.png      \n",
            "  inflating: data/img/89723.png      \n",
            "  inflating: data/img/52473.png      \n",
            "  inflating: data/img/50461.png      \n",
            "  inflating: data/img/60985.png      \n",
            "  inflating: data/img/80259.png      \n",
            "  inflating: data/img/82460.png      \n",
            "  inflating: data/img/50482.png      \n",
            "  inflating: data/img/05123.png      \n",
            "  inflating: data/img/89156.png      \n",
            "  inflating: data/img/80567.png      \n",
            "  inflating: data/img/48916.png      \n",
            "  inflating: data/img/19358.png      \n",
            "  inflating: data/img/98023.png      \n",
            "  inflating: data/img/59260.png      \n",
            "  inflating: data/img/76145.png      \n",
            "  inflating: data/img/39076.png      \n",
            "  inflating: data/img/71236.png      \n",
            "  inflating: data/img/09516.png      \n",
            "  inflating: data/img/20657.png      \n",
            "  inflating: data/img/01875.png      \n",
            "  inflating: data/img/23047.png      \n",
            "  inflating: data/img/30582.png      \n",
            "  inflating: data/img/74230.png      \n",
            "  inflating: data/img/83150.png      \n",
            "  inflating: data/img/42816.png      \n",
            "  inflating: data/img/96581.png      \n",
            "  inflating: data/img/67924.png      \n",
            "  inflating: data/img/73180.png      \n",
            "  inflating: data/img/80327.png      \n",
            "  inflating: data/img/52974.png      \n",
            "  inflating: data/img/87031.png      \n",
            "  inflating: data/img/53160.png      \n",
            "  inflating: data/img/57012.png      \n",
            "  inflating: data/img/82016.png      \n",
            "  inflating: data/img/92370.png      \n",
            "  inflating: data/img/50184.png      \n",
            "  inflating: data/img/46197.png      \n",
            "  inflating: data/img/04135.png      \n",
            "  inflating: data/img/01459.png      \n",
            "  inflating: data/img/04328.png      \n",
            "  inflating: data/img/24687.png      \n",
            "  inflating: data/img/30591.png      \n",
            "  inflating: data/img/87501.png      \n",
            "  inflating: data/img/62351.png      \n",
            "  inflating: data/img/76034.png      \n",
            "  inflating: data/img/09518.png      \n",
            "  inflating: data/img/29851.png      \n",
            "  inflating: data/img/76348.png      \n",
            "  inflating: data/img/59148.png      \n",
            "  inflating: data/img/81690.png      \n",
            "  inflating: data/img/78964.png      \n",
            "  inflating: data/img/04658.png      \n",
            "  inflating: data/img/18640.png      \n",
            "  inflating: data/img/62504.png      \n",
            "  inflating: data/img/25180.png      \n",
            "  inflating: data/img/35781.png      \n",
            "  inflating: data/img/69813.png      \n",
            "  inflating: data/img/91374.png      \n",
            "  inflating: data/img/24835.png      \n",
            "  inflating: data/img/64207.png      \n",
            "  inflating: data/img/68025.png      \n",
            "  inflating: data/img/53967.png      \n",
            "  inflating: data/img/01293.png      \n",
            "  inflating: data/img/65429.png      \n",
            "  inflating: data/img/28936.png      \n",
            "  inflating: data/img/04597.png      \n",
            "  inflating: data/img/48936.png      \n",
            "  inflating: data/img/32401.png      \n",
            "  inflating: data/img/78306.png      \n",
            "  inflating: data/img/07351.png      \n",
            "  inflating: data/img/52910.png      \n",
            "  inflating: data/img/01529.png      \n",
            "  inflating: data/img/67041.png      \n",
            "  inflating: data/img/56423.png      \n",
            "  inflating: data/img/02364.png      \n",
            "  inflating: data/img/91240.png      \n",
            "  inflating: data/img/84302.png      \n",
            "  inflating: data/img/98627.png      \n",
            "  inflating: data/img/38701.png      \n",
            "  inflating: data/img/84790.png      \n",
            "  inflating: data/img/70429.png      \n",
            "  inflating: data/img/29714.png      \n",
            "  inflating: data/img/60183.png      \n",
            "  inflating: data/img/96701.png      \n",
            "  inflating: data/img/50938.png      \n",
            "  inflating: data/img/52640.png      \n",
            "  inflating: data/img/73256.png      \n",
            "  inflating: data/img/42156.png      \n",
            "  inflating: data/img/68204.png      \n",
            "  inflating: data/img/40573.png      \n",
            "  inflating: data/img/27410.png      \n",
            "  inflating: data/img/34791.png      \n",
            "  inflating: data/img/94325.png      \n",
            "  inflating: data/img/57063.png      \n",
            "  inflating: data/img/69027.png      \n",
            "  inflating: data/img/13798.png      \n",
            "  inflating: data/img/79861.png      \n",
            "  inflating: data/img/74612.png      \n",
            "  inflating: data/img/58371.png      \n",
            "  inflating: data/img/93246.png      \n",
            "  inflating: data/img/87246.png      \n",
            "  inflating: data/img/48567.png      \n",
            "  inflating: data/img/07429.png      \n",
            "  inflating: data/img/39805.png      \n",
            "  inflating: data/img/21509.png      \n",
            "  inflating: data/img/42876.png      \n",
            "  inflating: data/img/90786.png      \n",
            "  inflating: data/img/15243.png      \n",
            "  inflating: data/img/79285.png      \n",
            "  inflating: data/img/80196.png      \n",
            "  inflating: data/img/81294.png      \n",
            "  inflating: data/img/56721.png      \n",
            "  inflating: data/img/24519.png      \n",
            "  inflating: data/img/79042.png      \n",
            "  inflating: data/img/57280.png      \n",
            "  inflating: data/img/52746.png      \n",
            "  inflating: data/img/08924.png      \n",
            "  inflating: data/img/12509.png      \n",
            "  inflating: data/img/41385.png      \n",
            "  inflating: data/img/18945.png      \n",
            "  inflating: data/img/35096.png      \n",
            "  inflating: data/img/57169.png      \n",
            "  inflating: data/img/97245.png      \n",
            "  inflating: data/img/73426.png      \n",
            "  inflating: data/img/76250.png      \n",
            "  inflating: data/img/25416.png      \n",
            "  inflating: data/img/95263.png      \n",
            "  inflating: data/img/17359.png      \n",
            "  inflating: data/img/37058.png      \n",
            "  inflating: data/img/94031.png      \n",
            "  inflating: data/img/92148.png      \n",
            "  inflating: data/img/76293.png      \n",
            "  inflating: data/img/84521.png      \n",
            "  inflating: data/img/40385.png      \n",
            "  inflating: data/img/46301.png      \n",
            "  inflating: data/img/09875.png      \n",
            "  inflating: data/img/21548.png      \n",
            "  inflating: data/img/46375.png      \n",
            "  inflating: data/img/19362.png      \n",
            "  inflating: data/img/60197.png      \n",
            "  inflating: data/img/50149.png      \n",
            "  inflating: data/img/40731.png      \n",
            "  inflating: data/img/85012.png      \n",
            "  inflating: data/img/82693.png      \n",
            "  inflating: data/img/92081.png      \n",
            "  inflating: data/img/93261.png      \n",
            "  inflating: data/img/97568.png      \n",
            "  inflating: data/img/98542.png      \n",
            "  inflating: data/img/70213.png      \n",
            "  inflating: data/img/74053.png      \n",
            "  inflating: data/img/49762.png      \n",
            "  inflating: data/img/01794.png      \n",
            "  inflating: data/img/56834.png      \n",
            "  inflating: data/img/45938.png      \n",
            "  inflating: data/img/63159.png      \n",
            "  inflating: data/img/15893.png      \n",
            "  inflating: data/img/56812.png      \n",
            "  inflating: data/img/01235.png      \n",
            "  inflating: data/img/95174.png      \n",
            "  inflating: data/img/85960.png      \n",
            "  inflating: data/img/04651.png      \n",
            "  inflating: data/img/46021.png      \n",
            "  inflating: data/img/87649.png      \n",
            "  inflating: data/img/52864.png      \n",
            "  inflating: data/img/97823.png      \n",
            "  inflating: data/img/46580.png      \n",
            "  inflating: data/img/79045.png      \n",
            "  inflating: data/img/68503.png      \n",
            "  inflating: data/img/56128.png      \n",
            "  inflating: data/img/64708.png      \n",
            "  inflating: data/img/28739.png      \n",
            "  inflating: data/img/14679.png      \n",
            "  inflating: data/img/07486.png      \n",
            "  inflating: data/img/52847.png      \n",
            "  inflating: data/img/94065.png      \n",
            "  inflating: data/img/68713.png      \n",
            "  inflating: data/img/23459.png      \n",
            "  inflating: data/img/52874.png      \n",
            "  inflating: data/img/71586.png      \n",
            "  inflating: data/img/79325.png      \n",
            "  inflating: data/img/48270.png      \n",
            "  inflating: data/img/03981.png      \n",
            "  inflating: data/img/17609.png      \n",
            "  inflating: data/img/57431.png      \n",
            "  inflating: data/img/60598.png      \n",
            "  inflating: data/img/14893.png      \n",
            "  inflating: data/img/10458.png      \n",
            "  inflating: data/img/05986.png      \n",
            "  inflating: data/img/92357.png      \n",
            "  inflating: data/img/09364.png      \n",
            "  inflating: data/img/63095.png      \n",
            "  inflating: data/img/43782.png      \n",
            "  inflating: data/img/76524.png      \n",
            "  inflating: data/img/26453.png      \n",
            "  inflating: data/img/39475.png      \n",
            "  inflating: data/img/70125.png      \n",
            "  inflating: data/img/61935.png      \n",
            "  inflating: data/img/04827.png      \n",
            "  inflating: data/img/34870.png      \n",
            "  inflating: data/img/97051.png      \n",
            "  inflating: data/img/28971.png      \n",
            "  inflating: data/img/05198.png      \n",
            "  inflating: data/img/65739.png      \n",
            "  inflating: data/img/59048.png      \n",
            "  inflating: data/img/93764.png      \n",
            "  inflating: data/img/21569.png      \n",
            "  inflating: data/img/15298.png      \n",
            "  inflating: data/img/20386.png      \n",
            "  inflating: data/img/64958.png      \n",
            "  inflating: data/img/30546.png      \n",
            "  inflating: data/img/89245.png      \n",
            "  inflating: data/img/15846.png      \n",
            "  inflating: data/img/52716.png      \n",
            "  inflating: data/img/10734.png      \n",
            "  inflating: data/img/56107.png      \n",
            "  inflating: data/img/75382.png      \n",
            "  inflating: data/img/70123.png      \n",
            "  inflating: data/img/80436.png      \n",
            "  inflating: data/img/28476.png      \n",
            "  inflating: data/img/86175.png      \n",
            "  inflating: data/img/56418.png      \n",
            "  inflating: data/img/93481.png      \n",
            "  inflating: data/img/69130.png      \n",
            "  inflating: data/img/80251.png      \n",
            "  inflating: data/img/48091.png      \n",
            "  inflating: data/img/30695.png      \n",
            "  inflating: data/img/25037.png      \n",
            "  inflating: data/img/87364.png      \n",
            "  inflating: data/img/83517.png      \n",
            "  inflating: data/img/78409.png      \n",
            "  inflating: data/img/85093.png      \n",
            "  inflating: data/img/03745.png      \n",
            "  inflating: data/img/56281.png      \n",
            "  inflating: data/img/61280.png      \n",
            "  inflating: data/img/82943.png      \n",
            "  inflating: data/img/19753.png      \n",
            "  inflating: data/img/69205.png      \n",
            "  inflating: data/img/51903.png      \n",
            "  inflating: data/img/91526.png      \n",
            "  inflating: data/img/87923.png      \n",
            "  inflating: data/img/32507.png      \n",
            "  inflating: data/img/69487.png      \n",
            "  inflating: data/img/30465.png      \n",
            "  inflating: data/img/75943.png      \n",
            "  inflating: data/img/42160.png      \n",
            "  inflating: data/img/06431.png      \n",
            "  inflating: data/img/57042.png      \n",
            "  inflating: data/img/71905.png      \n",
            "  inflating: data/img/31625.png      \n",
            "  inflating: data/img/95780.png      \n",
            "  inflating: data/img/31450.png      \n",
            "  inflating: data/img/97384.png      \n",
            "  inflating: data/img/16502.png      \n",
            "  inflating: data/img/40167.png      \n",
            "  inflating: data/img/49861.png      \n",
            "  inflating: data/img/96502.png      \n",
            "  inflating: data/img/62745.png      \n",
            "  inflating: data/img/73465.png      \n",
            "  inflating: data/img/09785.png      \n",
            "  inflating: data/img/49607.png      \n",
            "  inflating: data/img/39452.png      \n",
            "  inflating: data/img/26947.png      \n",
            "  inflating: data/img/96301.png      \n",
            "  inflating: data/img/13968.png      \n",
            "  inflating: data/img/76098.png      \n",
            "  inflating: data/img/51407.png      \n",
            "  inflating: data/img/47016.png      \n",
            "  inflating: data/img/85409.png      \n",
            "  inflating: data/img/62198.png      \n",
            "  inflating: data/img/70534.png      \n",
            "  inflating: data/img/23851.png      \n",
            "  inflating: data/img/72561.png      \n",
            "  inflating: data/img/51462.png      \n",
            "  inflating: data/img/57638.png      \n",
            "  inflating: data/img/78094.png      \n",
            "  inflating: data/img/75649.png      \n",
            "  inflating: data/img/19075.png      \n",
            "  inflating: data/img/93471.png      \n",
            "  inflating: data/img/45167.png      \n",
            "  inflating: data/img/53948.png      \n",
            "  inflating: data/img/38057.png      \n",
            "  inflating: data/img/47612.png      \n",
            "  inflating: data/img/51683.png      \n",
            "  inflating: data/img/78534.png      \n",
            "  inflating: data/img/18394.png      \n",
            "  inflating: data/img/89307.png      \n",
            "  inflating: data/img/59627.png      \n",
            "  inflating: data/img/95214.png      \n",
            "  inflating: data/img/37895.png      \n",
            "  inflating: data/img/34816.png      \n",
            "  inflating: data/img/34582.png      \n",
            "  inflating: data/img/74083.png      \n",
            "  inflating: data/img/39427.png      \n",
            "  inflating: data/img/42105.png      \n",
            "  inflating: data/img/46910.png      \n",
            "  inflating: data/img/19604.png      \n",
            "  inflating: data/img/45297.png      \n",
            "  inflating: data/img/70452.png      \n",
            "  inflating: data/img/46318.png      \n",
            "  inflating: data/img/51284.png      \n",
            "  inflating: data/img/26318.png      \n",
            "  inflating: data/img/27563.png      \n",
            "  inflating: data/img/27635.png      \n",
            "  inflating: data/img/50176.png      \n",
            "  inflating: data/img/01329.png      \n",
            "  inflating: data/img/73198.png      \n",
            "  inflating: data/img/12483.png      \n",
            "  inflating: data/img/76341.png      \n",
            "  inflating: data/img/36781.png      \n",
            "  inflating: data/img/86024.png      \n",
            "  inflating: data/img/30296.png      \n",
            "  inflating: data/img/09561.png      \n",
            "  inflating: data/img/72689.png      \n",
            "  inflating: data/img/17025.png      \n",
            "  inflating: data/img/86314.png      \n",
            "  inflating: data/img/03256.png      \n",
            "  inflating: data/img/21035.png      \n",
            "  inflating: data/img/96857.png      \n",
            "  inflating: data/img/03681.png      \n",
            "  inflating: data/img/27360.png      \n",
            "  inflating: data/img/58974.png      \n",
            "  inflating: data/img/65301.png      \n",
            "  inflating: data/img/52613.png      \n",
            "  inflating: data/img/52349.png      \n",
            "  inflating: data/img/92814.png      \n",
            "  inflating: data/img/23416.png      \n",
            "  inflating: data/img/80945.png      \n",
            "  inflating: data/img/29483.png      \n",
            "  inflating: data/img/70214.png      \n",
            "  inflating: data/img/92547.png      \n",
            "  inflating: data/img/84673.png      \n",
            "  inflating: data/img/84057.png      \n",
            "  inflating: data/img/54983.png      \n",
            "  inflating: data/img/67214.png      \n",
            "  inflating: data/img/39102.png      \n",
            "  inflating: data/img/79423.png      \n",
            "  inflating: data/img/75846.png      \n",
            "  inflating: data/img/50328.png      \n",
            "  inflating: data/img/72839.png      \n",
            "  inflating: data/img/52914.png      \n",
            "  inflating: data/img/95817.png      \n",
            "  inflating: data/img/31092.png      \n",
            "  inflating: data/img/92876.png      \n",
            "  inflating: data/img/23096.png      \n",
            "  inflating: data/img/02571.png      \n",
            "  inflating: data/img/45371.png      \n",
            "  inflating: data/img/42736.png      \n",
            "  inflating: data/img/83170.png      \n",
            "  inflating: data/img/32915.png      \n",
            "  inflating: data/img/52036.png      \n",
            "  inflating: data/img/01389.png      \n",
            "  inflating: data/img/50867.png      \n",
            "  inflating: data/img/19630.png      \n",
            "  inflating: data/img/36842.png      \n",
            "  inflating: data/img/45301.png      \n",
            "  inflating: data/img/78492.png      \n",
            "  inflating: data/img/47205.png      \n",
            "  inflating: data/img/80231.png      \n",
            "  inflating: data/img/07935.png      \n",
            "  inflating: data/img/31085.png      \n",
            "  inflating: data/img/15839.png      \n",
            "  inflating: data/img/84639.png      \n",
            "  inflating: data/img/24958.png      \n",
            "  inflating: data/img/96402.png      \n",
            "  inflating: data/img/91402.png      \n",
            "  inflating: data/img/68374.png      \n",
            "  inflating: data/img/62840.png      \n",
            "  inflating: data/img/52706.png      \n",
            "  inflating: data/img/18397.png      \n",
            "  inflating: data/img/38674.png      \n",
            "  inflating: data/img/86021.png      \n",
            "  inflating: data/img/31429.png      \n",
            "  inflating: data/img/03962.png      \n",
            "  inflating: data/img/25731.png      \n",
            "  inflating: data/img/78904.png      \n",
            "  inflating: data/img/39504.png      \n",
            "  inflating: data/img/83402.png      \n",
            "  inflating: data/img/38549.png      \n",
            "  inflating: data/img/15796.png      \n",
            "  inflating: data/img/46870.png      \n",
            "  inflating: data/img/60758.png      \n",
            "  inflating: data/img/67283.png      \n",
            "  inflating: data/img/08317.png      \n",
            "  inflating: data/img/34097.png      \n",
            "  inflating: data/img/57986.png      \n",
            "  inflating: data/img/81436.png      \n",
            "  inflating: data/img/40681.png      \n",
            "  inflating: data/img/20149.png      \n",
            "  inflating: data/img/27048.png      \n",
            "  inflating: data/img/76495.png      \n",
            "  inflating: data/img/25849.png      \n",
            "  inflating: data/img/13679.png      \n",
            "  inflating: data/img/46815.png      \n",
            "  inflating: data/img/91245.png      \n",
            "  inflating: data/img/84219.png      \n",
            "  inflating: data/img/96735.png      \n",
            "  inflating: data/img/72160.png      \n",
            "  inflating: data/img/31076.png      \n",
            "  inflating: data/img/26980.png      \n",
            "  inflating: data/img/36294.png      \n",
            "  inflating: data/img/12958.png      \n",
            "  inflating: data/img/08695.png      \n",
            "  inflating: data/img/76259.png      \n",
            "  inflating: data/img/42673.png      \n",
            "  inflating: data/img/07126.png      \n",
            "  inflating: data/img/01247.png      \n",
            "  inflating: data/img/28396.png      \n",
            "  inflating: data/img/15480.png      \n",
            "  inflating: data/img/16084.png      \n",
            "  inflating: data/img/52483.png      \n",
            "  inflating: data/img/90342.png      \n",
            "  inflating: data/img/73514.png      \n",
            "  inflating: data/img/70594.png      \n",
            "  inflating: data/img/97213.png      \n",
            "  inflating: data/img/86329.png      \n",
            "  inflating: data/img/25341.png      \n",
            "  inflating: data/img/75823.png      \n",
            "  inflating: data/img/96802.png      \n",
            "  inflating: data/img/87594.png      \n",
            "  inflating: data/img/37426.png      \n",
            "  inflating: data/img/57984.png      \n",
            "  inflating: data/img/35794.png      \n",
            "  inflating: data/img/39085.png      \n",
            "  inflating: data/img/24751.png      \n",
            "  inflating: data/img/31042.png      \n",
            "  inflating: data/img/02385.png      \n",
            "  inflating: data/img/07286.png      \n",
            "  inflating: data/img/92016.png      \n",
            "  inflating: data/img/91863.png      \n",
            "  inflating: data/img/48079.png      \n",
            "  inflating: data/img/02568.png      \n",
            "  inflating: data/img/95401.png      \n",
            "  inflating: data/img/36598.png      \n",
            "  inflating: data/img/87521.png      \n",
            "  inflating: data/img/67029.png      \n",
            "  inflating: data/img/67319.png      \n",
            "  inflating: data/img/97403.png      \n",
            "  inflating: data/img/97824.png      \n",
            "  inflating: data/img/76083.png      \n",
            "  inflating: data/img/30986.png      \n",
            "  inflating: data/img/12967.png      \n",
            "  inflating: data/img/35182.png      \n",
            "  inflating: data/img/57430.png      \n",
            "  inflating: data/img/08375.png      \n",
            "  inflating: data/img/60592.png      \n",
            "  inflating: data/img/27605.png      \n",
            "  inflating: data/img/95402.png      \n",
            "  inflating: data/img/14083.png      \n",
            "  inflating: data/img/81450.png      \n",
            "  inflating: data/img/08452.png      \n",
            "  inflating: data/img/17645.png      \n",
            "  inflating: data/img/83670.png      \n",
            "  inflating: data/img/12834.png      \n",
            "  inflating: data/img/54912.png      \n",
            "  inflating: data/img/43517.png      \n",
            "  inflating: data/img/84152.png      \n",
            "  inflating: data/img/93740.png      \n",
            "  inflating: data/img/31295.png      \n",
            "  inflating: data/img/53806.png      \n",
            "  inflating: data/img/02518.png      \n",
            "  inflating: data/img/50248.png      \n",
            "  inflating: data/img/19768.png      \n",
            "  inflating: data/img/03246.png      \n",
            "  inflating: data/img/10693.png      \n",
            "  inflating: data/img/03271.png      \n",
            "  inflating: data/img/81370.png      \n",
            "  inflating: data/img/40217.png      \n",
            "  inflating: data/img/75603.png      \n",
            "  inflating: data/img/02367.png      \n",
            "  inflating: data/img/51089.png      \n",
            "  inflating: data/img/92806.png      \n",
            "  inflating: data/img/76920.png      \n",
            "  inflating: data/img/34028.png      \n",
            "  inflating: data/img/76549.png      \n",
            "  inflating: data/img/17829.png      \n",
            "  inflating: data/img/51694.png      \n",
            "  inflating: data/img/82014.png      \n",
            "  inflating: data/img/32706.png      \n",
            "  inflating: data/img/29678.png      \n",
            "  inflating: data/img/64291.png      \n",
            "  inflating: data/img/12750.png      \n",
            "  inflating: data/img/90145.png      \n",
            "  inflating: data/img/63915.png      \n",
            "  inflating: data/img/40791.png      \n",
            "  inflating: data/img/31278.png      \n",
            "  inflating: data/img/38702.png      \n",
            "  inflating: data/img/51094.png      \n",
            "  inflating: data/img/78125.png      \n",
            "  inflating: data/img/05174.png      \n",
            "  inflating: data/img/50187.png      \n",
            "  inflating: data/img/76108.png      \n",
            "  inflating: data/img/71245.png      \n",
            "  inflating: data/img/29841.png      \n",
            "  inflating: data/img/71482.png      \n",
            "  inflating: data/img/59170.png      \n",
            "  inflating: data/img/65819.png      \n",
            "  inflating: data/img/01672.png      \n",
            "  inflating: data/img/91806.png      \n",
            "  inflating: data/img/02165.png      \n",
            "  inflating: data/img/21078.png      \n",
            "  inflating: data/img/14378.png      \n",
            "  inflating: data/img/94602.png      \n",
            "  inflating: data/img/78163.png      \n",
            "  inflating: data/img/03947.png      \n",
            "  inflating: data/img/73984.png      \n",
            "  inflating: data/img/65234.png      \n",
            "  inflating: data/img/85167.png      \n",
            "  inflating: data/img/08649.png      \n",
            "  inflating: data/img/87239.png      \n",
            "  inflating: data/img/18943.png      \n",
            "  inflating: data/img/81950.png      \n",
            "  inflating: data/img/50186.png      \n",
            "  inflating: data/img/65092.png      \n",
            "  inflating: data/img/34018.png      \n",
            "  inflating: data/img/71450.png      \n",
            "  inflating: data/img/14879.png      \n",
            "  inflating: data/img/90752.png      \n",
            "  inflating: data/img/98351.png      \n",
            "  inflating: data/img/39465.png      \n",
            "  inflating: data/img/21348.png      \n",
            "  inflating: data/img/82134.png      \n",
            "  inflating: data/img/02984.png      \n",
            "  inflating: data/img/69123.png      \n",
            "  inflating: data/img/85317.png      \n",
            "  inflating: data/img/17438.png      \n",
            "  inflating: data/img/26137.png      \n",
            "  inflating: data/img/26091.png      \n",
            "  inflating: data/img/51928.png      \n",
            "  inflating: data/img/68409.png      \n",
            "  inflating: data/img/25097.png      \n",
            "  inflating: data/img/34806.png      \n",
            "  inflating: data/img/62510.png      \n",
            "  inflating: data/img/76892.png      \n",
            "  inflating: data/img/78693.png      \n",
            "  inflating: data/img/28695.png      \n",
            "  inflating: data/img/31408.png      \n",
            "  inflating: data/img/12460.png      \n",
            "  inflating: data/img/40758.png      \n",
            "  inflating: data/img/76321.png      \n",
            "  inflating: data/img/52610.png      \n",
            "  inflating: data/img/14953.png      \n",
            "  inflating: data/img/01925.png      \n",
            "  inflating: data/img/52034.png      \n",
            "  inflating: data/img/03957.png      \n",
            "  inflating: data/img/20945.png      \n",
            "  inflating: data/img/18356.png      \n",
            "  inflating: data/img/86293.png      \n",
            "  inflating: data/img/05483.png      \n",
            "  inflating: data/img/58609.png      \n",
            "  inflating: data/img/97305.png      \n",
            "  inflating: data/img/19672.png      \n",
            "  inflating: data/img/86394.png      \n",
            "  inflating: data/img/38427.png      \n",
            "  inflating: data/img/87130.png      \n",
            "  inflating: data/img/67205.png      \n",
            "  inflating: data/img/46218.png      \n",
            "  inflating: data/img/71568.png      \n",
            "  inflating: data/img/20395.png      \n",
            "  inflating: data/img/31629.png      \n",
            "  inflating: data/img/28017.png      \n",
            "  inflating: data/img/58479.png      \n",
            "  inflating: data/img/62081.png      \n",
            "  inflating: data/img/29813.png      \n",
            "  inflating: data/img/58247.png      \n",
            "  inflating: data/img/17596.png      \n",
            "  inflating: data/img/63074.png      \n",
            "  inflating: data/img/15064.png      \n",
            "  inflating: data/img/52698.png      \n",
            "  inflating: data/img/26489.png      \n",
            "  inflating: data/img/61037.png      \n",
            "  inflating: data/img/10956.png      \n",
            "  inflating: data/img/51497.png      \n",
            "  inflating: data/img/97103.png      \n",
            "  inflating: data/img/47098.png      \n",
            "  inflating: data/img/72506.png      \n",
            "  inflating: data/img/63150.png      \n",
            "  inflating: data/img/03847.png      \n",
            "  inflating: data/img/07392.png      \n",
            "  inflating: data/img/97026.png      \n",
            "  inflating: data/img/03756.png      \n",
            "  inflating: data/img/50962.png      \n",
            "  inflating: data/img/95062.png      \n",
            "  inflating: data/img/51730.png      \n",
            "  inflating: data/img/75104.png      \n",
            "  inflating: data/img/23759.png      \n",
            "  inflating: data/img/67481.png      \n",
            "  inflating: data/img/35840.png      \n",
            "  inflating: data/img/37204.png      \n",
            "  inflating: data/img/34178.png      \n",
            "  inflating: data/img/70315.png      \n",
            "  inflating: data/img/57861.png      \n",
            "  inflating: data/img/48361.png      \n",
            "  inflating: data/img/06491.png      \n",
            "  inflating: data/img/86013.png      \n",
            "  inflating: data/img/94018.png      \n",
            "  inflating: data/img/90126.png      \n",
            "  inflating: data/img/27058.png      \n",
            "  inflating: data/img/79068.png      \n",
            "  inflating: data/img/98573.png      \n",
            "  inflating: data/img/87426.png      \n",
            "  inflating: data/img/51924.png      \n",
            "  inflating: data/img/41607.png      \n",
            "  inflating: data/img/90538.png      \n",
            "  inflating: data/img/02514.png      \n",
            "  inflating: data/img/12045.png      \n",
            "  inflating: data/img/84091.png      \n",
            "  inflating: data/img/72654.png      \n",
            "  inflating: data/img/02789.png      \n",
            "  inflating: data/img/20567.png      \n",
            "  inflating: data/img/75461.png      \n",
            "  inflating: data/img/02613.png      \n",
            "  inflating: data/img/98071.png      \n",
            "  inflating: data/img/23485.png      \n",
            "  inflating: data/img/70925.png      \n",
            "  inflating: data/img/82346.png      \n",
            "  inflating: data/img/45316.png      \n",
            "  inflating: data/img/13548.png      \n",
            "  inflating: data/img/03482.png      \n",
            "  inflating: data/img/41296.png      \n",
            "  inflating: data/img/49673.png      \n",
            "  inflating: data/img/96873.png      \n",
            "  inflating: data/img/35647.png      \n",
            "  inflating: data/img/65843.png      \n",
            "  inflating: data/img/71689.png      \n",
            "  inflating: data/img/64190.png      \n",
            "  inflating: data/img/20854.png      \n",
            "  inflating: data/img/78925.png      \n",
            "  inflating: data/img/25370.png      \n",
            "  inflating: data/img/45396.png      \n",
            "  inflating: data/img/01327.png      \n",
            "  inflating: data/img/46380.png      \n",
            "  inflating: data/img/80947.png      \n",
            "  inflating: data/img/04239.png      \n",
            "  inflating: data/img/94578.png      \n",
            "  inflating: data/img/51892.png      \n",
            "  inflating: data/img/34078.png      \n",
            "  inflating: data/img/28374.png      \n",
            "  inflating: data/img/09731.png      \n",
            "  inflating: data/img/08439.png      \n",
            "  inflating: data/img/36895.png      \n",
            "  inflating: data/img/29508.png      \n",
            "  inflating: data/img/72634.png      \n",
            "  inflating: data/img/46123.png      \n",
            "  inflating: data/img/19567.png      \n",
            "  inflating: data/img/87209.png      \n",
            "  inflating: data/img/64825.png      \n",
            "  inflating: data/img/48317.png      \n",
            "  inflating: data/img/38029.png      \n",
            "  inflating: data/img/63741.png      \n",
            "  inflating: data/img/10478.png      \n",
            "  inflating: data/img/45630.png      \n",
            "  inflating: data/img/46971.png      \n",
            "  inflating: data/img/01392.png      \n",
            "  inflating: data/img/13269.png      \n",
            "  inflating: data/img/24980.png      \n",
            "  inflating: data/img/75810.png      \n",
            "  inflating: data/img/64089.png      \n",
            "  inflating: data/img/10948.png      \n",
            "  inflating: data/img/79681.png      \n",
            "  inflating: data/img/90734.png      \n",
            "  inflating: data/img/51602.png      \n",
            "  inflating: data/img/70193.png      \n",
            "  inflating: data/img/95102.png      \n",
            "  inflating: data/img/54602.png      \n",
            "  inflating: data/img/08376.png      \n",
            "  inflating: data/img/41678.png      \n",
            "  inflating: data/img/34871.png      \n",
            "  inflating: data/img/48012.png      \n",
            "  inflating: data/img/34978.png      \n",
            "  inflating: data/img/60541.png      \n",
            "  inflating: data/img/43690.png      \n",
            "  inflating: data/img/49260.png      \n",
            "  inflating: data/img/35628.png      \n",
            "  inflating: data/img/25914.png      \n",
            "  inflating: data/img/06859.png      \n",
            "  inflating: data/img/89107.png      \n",
            "  inflating: data/img/47831.png      \n",
            "  inflating: data/img/54682.png      \n",
            "  inflating: data/img/05813.png      \n",
            "  inflating: data/img/02975.png      \n",
            "  inflating: data/img/76320.png      \n",
            "  inflating: data/img/15374.png      \n",
            "  inflating: data/img/92780.png      \n",
            "  inflating: data/img/65829.png      \n",
            "  inflating: data/img/01487.png      \n",
            "  inflating: data/img/78139.png      \n",
            "  inflating: data/img/05917.png      \n",
            "  inflating: data/img/56287.png      \n",
            "  inflating: data/img/08534.png      \n",
            "  inflating: data/img/09251.png      \n",
            "  inflating: data/img/16948.png      \n",
            "  inflating: data/img/54823.png      \n",
            "  inflating: data/img/70928.png      \n",
            "  inflating: data/img/17405.png      \n",
            "  inflating: data/img/16920.png      \n",
            "  inflating: data/img/13674.png      \n",
            "  inflating: data/img/09827.png      \n",
            "  inflating: data/img/48617.png      \n",
            "  inflating: data/img/46310.png      \n",
            "  inflating: data/img/28437.png      \n",
            "  inflating: data/img/54319.png      \n",
            "  inflating: data/img/12957.png      \n",
            "  inflating: data/img/43826.png      \n",
            "  inflating: data/img/03874.png      \n",
            "  inflating: data/img/19650.png      \n",
            "  inflating: data/img/94732.png      \n",
            "  inflating: data/img/49531.png      \n",
            "  inflating: data/img/45207.png      \n",
            "  inflating: data/img/98412.png      \n",
            "  inflating: data/img/04729.png      \n",
            "  inflating: data/img/25913.png      \n",
            "  inflating: data/img/32789.png      \n",
            "  inflating: data/img/36187.png      \n",
            "  inflating: data/img/80736.png      \n",
            "  inflating: data/img/24178.png      \n",
            "  inflating: data/img/94508.png      \n",
            "  inflating: data/img/04583.png      \n",
            "  inflating: data/img/84736.png      \n",
            "  inflating: data/img/58462.png      \n",
            "  inflating: data/img/61482.png      \n",
            "  inflating: data/img/73842.png      \n",
            "  inflating: data/img/39816.png      \n",
            "  inflating: data/img/97365.png      \n",
            "  inflating: data/img/19367.png      \n",
            "  inflating: data/img/50491.png      \n",
            "  inflating: data/img/28957.png      \n",
            "  inflating: data/img/74390.png      \n",
            "  inflating: data/img/74062.png      \n",
            "  inflating: data/img/87065.png      \n",
            "  inflating: data/img/27895.png      \n",
            "  inflating: data/img/08719.png      \n",
            "  inflating: data/img/91208.png      \n",
            "  inflating: data/img/51496.png      \n",
            "  inflating: data/img/32468.png      \n",
            "  inflating: data/img/34687.png      \n",
            "  inflating: data/img/31657.png      \n",
            "  inflating: data/img/31640.png      \n",
            "  inflating: data/img/86125.png      \n",
            "  inflating: data/img/69710.png      \n",
            "  inflating: data/img/73105.png      \n",
            "  inflating: data/img/53812.png      \n",
            "  inflating: data/img/02145.png      \n",
            "  inflating: data/img/09718.png      \n",
            "  inflating: data/img/42861.png      \n",
            "  inflating: data/img/41562.png      \n",
            "  inflating: data/img/30785.png      \n",
            "  inflating: data/img/69751.png      \n",
            "  inflating: data/img/45209.png      \n",
            "  inflating: data/img/07826.png      \n",
            "  inflating: data/img/16874.png      \n",
            "  inflating: data/img/41376.png      \n",
            "  inflating: data/img/70281.png      \n",
            "  inflating: data/img/76094.png      \n",
            "  inflating: data/img/54679.png      \n",
            "  inflating: data/img/15420.png      \n",
            "  inflating: data/img/97045.png      \n",
            "  inflating: data/img/25791.png      \n",
            "  inflating: data/img/49280.png      \n",
            "  inflating: data/img/26378.png      \n",
            "  inflating: data/img/28659.png      \n",
            "  inflating: data/img/36028.png      \n",
            "  inflating: data/img/80916.png      \n",
            "  inflating: data/img/63805.png      \n",
            "  inflating: data/img/19472.png      \n",
            "  inflating: data/img/24165.png      \n",
            "  inflating: data/img/58374.png      \n",
            "  inflating: data/img/69327.png      \n",
            "  inflating: data/img/76021.png      \n",
            "  inflating: data/img/41983.png      \n",
            "  inflating: data/img/96307.png      \n",
            "  inflating: data/img/28973.png      \n",
            "  inflating: data/img/92645.png      \n",
            "  inflating: data/img/49170.png      \n",
            "  inflating: data/img/31809.png      \n",
            "  inflating: data/img/26490.png      \n",
            "  inflating: data/img/19275.png      \n",
            "  inflating: data/img/67521.png      \n",
            "  inflating: data/img/74029.png      \n",
            "  inflating: data/img/20195.png      \n",
            "  inflating: data/img/76104.png      \n",
            "  inflating: data/img/48213.png      \n",
            "  inflating: data/img/18427.png      \n",
            "  inflating: data/img/29485.png      \n",
            "  inflating: data/img/52863.png      \n",
            "  inflating: data/img/47582.png      \n",
            "  inflating: data/img/49023.png      \n",
            "  inflating: data/img/98762.png      \n",
            "  inflating: data/img/46193.png      \n",
            "  inflating: data/img/47608.png      \n",
            "  inflating: data/img/41062.png      \n",
            "  inflating: data/img/41032.png      \n",
            "  inflating: data/img/27468.png      \n",
            "  inflating: data/img/83046.png      \n",
            "  inflating: data/img/02971.png      \n",
            "  inflating: data/img/14908.png      \n",
            "  inflating: data/img/67091.png      \n",
            "  inflating: data/img/47215.png      \n",
            "  inflating: data/img/75918.png      \n",
            "  inflating: data/img/50839.png      \n",
            "  inflating: data/img/26803.png      \n",
            "  inflating: data/img/82609.png      \n",
            "  inflating: data/img/96235.png      \n",
            "  inflating: data/img/58793.png      \n",
            "  inflating: data/img/15394.png      \n",
            "  inflating: data/img/96845.png      \n",
            "  inflating: data/img/91320.png      \n",
            "  inflating: data/img/83415.png      \n",
            "  inflating: data/img/84360.png      \n",
            "  inflating: data/img/43265.png      \n",
            "  inflating: data/img/53407.png      \n",
            "  inflating: data/img/03257.png      \n",
            "  inflating: data/img/57469.png      \n",
            "  inflating: data/img/20317.png      \n",
            "  inflating: data/img/72608.png      \n",
            "  inflating: data/img/45231.png      \n",
            "  inflating: data/img/24069.png      \n",
            "  inflating: data/img/96820.png      \n",
            "  inflating: data/img/86514.png      \n",
            "  inflating: data/img/96471.png      \n",
            "  inflating: data/img/53917.png      \n",
            "  inflating: data/img/87390.png      \n",
            "  inflating: data/img/07983.png      \n",
            "  inflating: data/img/74091.png      \n",
            "  inflating: data/img/83941.png      \n",
            "  inflating: data/img/58310.png      \n",
            "  inflating: data/img/05479.png      \n",
            "  inflating: data/img/02846.png      \n",
            "  inflating: data/img/72094.png      \n",
            "  inflating: data/img/72946.png      \n",
            "  inflating: data/img/74920.png      \n",
            "  inflating: data/img/58093.png      \n",
            "  inflating: data/img/01578.png      \n",
            "  inflating: data/img/96058.png      \n",
            "  inflating: data/img/80297.png      \n",
            "  inflating: data/img/07291.png      \n",
            "  inflating: data/img/93102.png      \n",
            "  inflating: data/img/57120.png      \n",
            "  inflating: data/img/07481.png      \n",
            "  inflating: data/img/09156.png      \n",
            "  inflating: data/img/90463.png      \n",
            "  inflating: data/img/43859.png      \n",
            "  inflating: data/img/26307.png      \n",
            "  inflating: data/img/15847.png      \n",
            "  inflating: data/img/27548.png      \n",
            "  inflating: data/img/84016.png      \n",
            "  inflating: data/img/12589.png      \n",
            "  inflating: data/img/13579.png      \n",
            "  inflating: data/img/53924.png      \n",
            "  inflating: data/img/27068.png      \n",
            "  inflating: data/img/92015.png      \n",
            "  inflating: data/img/93528.png      \n",
            "  inflating: data/img/47510.png      \n",
            "  inflating: data/img/69324.png      \n",
            "  inflating: data/img/69357.png      \n",
            "  inflating: data/img/09715.png      \n",
            "  inflating: data/img/20187.png      \n",
            "  inflating: data/img/75941.png      \n",
            "  inflating: data/img/23761.png      \n",
            "  inflating: data/img/63547.png      \n",
            "  inflating: data/img/38046.png      \n",
            "  inflating: data/img/25901.png      \n",
            "  inflating: data/img/42786.png      \n",
            "  inflating: data/img/67342.png      \n",
            "  inflating: data/img/08137.png      \n",
            "  inflating: data/img/32160.png      \n",
            "  inflating: data/img/63275.png      \n",
            "  inflating: data/img/51092.png      \n",
            "  inflating: data/img/81720.png      \n",
            "  inflating: data/img/24039.png      \n",
            "  inflating: data/img/42618.png      \n",
            "  inflating: data/img/56134.png      \n",
            "  inflating: data/img/52603.png      \n",
            "  inflating: data/img/35087.png      \n",
            "  inflating: data/img/70432.png      \n",
            "  inflating: data/img/48320.png      \n",
            "  inflating: data/img/68159.png      \n",
            "  inflating: data/img/28613.png      \n",
            "  inflating: data/img/09467.png      \n",
            "  inflating: data/img/35674.png      \n",
            "  inflating: data/img/42813.png      \n",
            "  inflating: data/img/28470.png      \n",
            "  inflating: data/img/01527.png      \n",
            "  inflating: data/img/15076.png      \n",
            "  inflating: data/img/24905.png      \n",
            "  inflating: data/img/63129.png      \n",
            "  inflating: data/img/34170.png      \n",
            "  inflating: data/img/48930.png      \n",
            "  inflating: data/img/10528.png      \n",
            "  inflating: data/img/25107.png      \n",
            "  inflating: data/img/84762.png      \n",
            "  inflating: data/img/18942.png      \n",
            "  inflating: data/img/18524.png      \n",
            "  inflating: data/img/68520.png      \n",
            "  inflating: data/img/29785.png      \n",
            "  inflating: data/img/04397.png      \n",
            "  inflating: data/img/70598.png      \n",
            "  inflating: data/img/83216.png      \n",
            "  inflating: data/img/31497.png      \n",
            "  inflating: data/img/36450.png      \n",
            "  inflating: data/img/74198.png      \n",
            "  inflating: data/img/92157.png      \n",
            "  inflating: data/img/21560.png      \n",
            "  inflating: data/img/36185.png      \n",
            "  inflating: data/img/58917.png      \n",
            "  inflating: data/img/48570.png      \n",
            "  inflating: data/img/28657.png      \n",
            "  inflating: data/img/94305.png      \n",
            "  inflating: data/img/53820.png      \n",
            "  inflating: data/img/02935.png      \n",
            "  inflating: data/img/91462.png      \n",
            "  inflating: data/img/95130.png      \n",
            "  inflating: data/img/17392.png      \n",
            "  inflating: data/img/17386.png      \n",
            "  inflating: data/img/73480.png      \n",
            "  inflating: data/img/14037.png      \n",
            "  inflating: data/img/65432.png      \n",
            "  inflating: data/img/23817.png      \n",
            "  inflating: data/img/38417.png      \n",
            "  inflating: data/img/16837.png      \n",
            "  inflating: data/img/48679.png      \n",
            "  inflating: data/img/85413.png      \n",
            "  inflating: data/img/51046.png      \n",
            "  inflating: data/img/89750.png      \n",
            "  inflating: data/img/37294.png      \n",
            "  inflating: data/img/42598.png      \n",
            "  inflating: data/img/47180.png      \n",
            "  inflating: data/img/06897.png      \n",
            "  inflating: data/img/17965.png      \n",
            "  inflating: data/img/41506.png      \n",
            "  inflating: data/img/61503.png      \n",
            "  inflating: data/img/15824.png      \n",
            "  inflating: data/img/92364.png      \n",
            "  inflating: data/img/57812.png      \n",
            "  inflating: data/img/16097.png      \n",
            "  inflating: data/img/85621.png      \n",
            "  inflating: data/img/47920.png      \n",
            "  inflating: data/img/42860.png      \n",
            "  inflating: data/img/94560.png      \n",
            "  inflating: data/img/28597.png      \n",
            "  inflating: data/img/70136.png      \n",
            "  inflating: data/img/17843.png      \n",
            "  inflating: data/img/07521.png      \n",
            "  inflating: data/img/73204.png      \n",
            "  inflating: data/img/45930.png      \n",
            "  inflating: data/img/61485.png      \n",
            "  inflating: data/img/30249.png      \n",
            "  inflating: data/img/24651.png      \n",
            "  inflating: data/img/97058.png      \n",
            "  inflating: data/img/84527.png      \n",
            "  inflating: data/img/27163.png      \n",
            "  inflating: data/img/95247.png      \n",
            "  inflating: data/img/95302.png      \n",
            "  inflating: data/img/30762.png      \n",
            "  inflating: data/img/36870.png      \n",
            "  inflating: data/img/61093.png      \n",
            "  inflating: data/img/94150.png      \n",
            "  inflating: data/img/53678.png      \n",
            "  inflating: data/img/53628.png      \n",
            "  inflating: data/img/90183.png      \n",
            "  inflating: data/img/15738.png      \n",
            "  inflating: data/img/90632.png      \n",
            "  inflating: data/img/63714.png      \n",
            "  inflating: data/img/01682.png      \n",
            "  inflating: data/img/38697.png      \n",
            "  inflating: data/img/80532.png      \n",
            "  inflating: data/img/95684.png      \n",
            "  inflating: data/img/57840.png      \n",
            "  inflating: data/img/53187.png      \n",
            "  inflating: data/img/83765.png      \n",
            "  inflating: data/img/20154.png      \n",
            "  inflating: data/img/82531.png      \n",
            "  inflating: data/img/71045.png      \n",
            "  inflating: data/img/10675.png      \n",
            "  inflating: data/img/08537.png      \n",
            "  inflating: data/img/24319.png      \n",
            "  inflating: data/img/60578.png      \n",
            "  inflating: data/img/72146.png      \n",
            "  inflating: data/img/49752.png      \n",
            "  inflating: data/img/18379.png      \n",
            "  inflating: data/img/57602.png      \n",
            "  inflating: data/img/13470.png      \n",
            "  inflating: data/img/34780.png      \n",
            "  inflating: data/img/76142.png      \n",
            "  inflating: data/img/18306.png      \n",
            "  inflating: data/img/84901.png      \n",
            "  inflating: data/img/94256.png      \n",
            "  inflating: data/img/84652.png      \n",
            "  inflating: data/img/92453.png      \n",
            "  inflating: data/img/86203.png      \n",
            "  inflating: data/img/29574.png      \n",
            "  inflating: data/img/09572.png      \n",
            "  inflating: data/img/92734.png      \n",
            "  inflating: data/img/30756.png      \n",
            "  inflating: data/img/91724.png      \n",
            "  inflating: data/img/14657.png      \n",
            "  inflating: data/img/46853.png      \n",
            "  inflating: data/img/01827.png      \n",
            "  inflating: data/img/82749.png      \n",
            "  inflating: data/img/86075.png      \n",
            "  inflating: data/img/70624.png      \n",
            "  inflating: data/img/36729.png      \n",
            "  inflating: data/img/69042.png      \n",
            "  inflating: data/img/43657.png      \n",
            "  inflating: data/img/35691.png      \n",
            "  inflating: data/img/71452.png      \n",
            "  inflating: data/img/47982.png      \n",
            "  inflating: data/img/17369.png      \n",
            "  inflating: data/img/05712.png      \n",
            "  inflating: data/img/01734.png      \n",
            "  inflating: data/img/87241.png      \n",
            "  inflating: data/img/75618.png      \n",
            "  inflating: data/img/89475.png      \n",
            "  inflating: data/img/27630.png      \n",
            "  inflating: data/img/50398.png      \n",
            "  inflating: data/img/74352.png      \n",
            "  inflating: data/img/89407.png      \n",
            "  inflating: data/img/06781.png      \n",
            "  inflating: data/img/57094.png      \n",
            "  inflating: data/img/34095.png      \n",
            "  inflating: data/img/02169.png      \n",
            "  inflating: data/img/87413.png      \n",
            "  inflating: data/img/38271.png      \n",
            "  inflating: data/img/82367.png      \n",
            "  inflating: data/img/78420.png      \n",
            "  inflating: data/img/16537.png      \n",
            "  inflating: data/img/67580.png      \n",
            "  inflating: data/img/31604.png      \n",
            "  inflating: data/img/67241.png      \n",
            "  inflating: data/img/92187.png      \n",
            "  inflating: data/img/16573.png      \n",
            "  inflating: data/img/64385.png      \n",
            "  inflating: data/img/23748.png      \n",
            "  inflating: data/img/28765.png      \n",
            "  inflating: data/img/17209.png      \n",
            "  inflating: data/img/32650.png      \n",
            "  inflating: data/img/45036.png      \n",
            "  inflating: data/img/76091.png      \n",
            "  inflating: data/img/65978.png      \n",
            "  inflating: data/img/67208.png      \n",
            "  inflating: data/img/79806.png      \n",
            "  inflating: data/img/03794.png      \n",
            "  inflating: data/img/60732.png      \n",
            "  inflating: data/img/04185.png      \n",
            "  inflating: data/img/97021.png      \n",
            "  inflating: data/img/45831.png      \n",
            "  inflating: data/img/87436.png      \n",
            "  inflating: data/img/03871.png      \n",
            "  inflating: data/img/25768.png      \n",
            "  inflating: data/img/69175.png      \n",
            "  inflating: data/img/16058.png      \n",
            "  inflating: data/img/49021.png      \n",
            "  inflating: data/img/49650.png      \n",
            "  inflating: data/img/51469.png      \n",
            "  inflating: data/img/46920.png      \n",
            "  inflating: data/img/01379.png      \n",
            "  inflating: data/img/32076.png      \n",
            "  inflating: data/img/51839.png      \n",
            "  inflating: data/img/53291.png      \n",
            "  inflating: data/img/31684.png      \n",
            "  inflating: data/img/17054.png      \n",
            "  inflating: data/img/35640.png      \n",
            "  inflating: data/img/07893.png      \n",
            "  inflating: data/img/64839.png      \n",
            "  inflating: data/img/14950.png      \n",
            "  inflating: data/img/83256.png      \n",
            "  inflating: data/img/56421.png      \n",
            "  inflating: data/img/81529.png      \n",
            "  inflating: data/img/83960.png      \n",
            "  inflating: data/img/59061.png      \n",
            "  inflating: data/img/91765.png      \n",
            "  inflating: data/img/48625.png      \n",
            "  inflating: data/img/85912.png      \n",
            "  inflating: data/img/39285.png      \n",
            "  inflating: data/img/30162.png      \n",
            "  inflating: data/img/46509.png      \n",
            "  inflating: data/img/82973.png      \n",
            "  inflating: data/img/45108.png      \n",
            "  inflating: data/img/16749.png      \n",
            "  inflating: data/img/69450.png      \n",
            "  inflating: data/img/96513.png      \n",
            "  inflating: data/img/36179.png      \n",
            "  inflating: data/img/49725.png      \n",
            "  inflating: data/img/93042.png      \n",
            "  inflating: data/img/53768.png      \n",
            "  inflating: data/img/57160.png      \n",
            "  inflating: data/img/62849.png      \n",
            "  inflating: data/img/36418.png      \n",
            "  inflating: data/img/64935.png      \n",
            "  inflating: data/img/20146.png      \n",
            "  inflating: data/img/13590.png      \n",
            "  inflating: data/img/17356.png      \n",
            "  inflating: data/img/39524.png      \n",
            "  inflating: data/img/34756.png      \n",
            "  inflating: data/img/72604.png      \n",
            "  inflating: data/img/84276.png      \n",
            "  inflating: data/img/63025.png      \n",
            "  inflating: data/img/08742.png      \n",
            "  inflating: data/img/71086.png      \n",
            "  inflating: data/img/47692.png      \n",
            "  inflating: data/img/15674.png      \n",
            "  inflating: data/img/69258.png      \n",
            "  inflating: data/img/93568.png      \n",
            "  inflating: data/img/21096.png      \n",
            "  inflating: data/img/50487.png      \n",
            "  inflating: data/img/01324.png      \n",
            "  inflating: data/img/46895.png      \n",
            "  inflating: data/img/50142.png      \n",
            "  inflating: data/img/43519.png      \n",
            "  inflating: data/img/31025.png      \n",
            "  inflating: data/img/85941.png      \n",
            "  inflating: data/img/39182.png      \n",
            "  inflating: data/img/17624.png      \n",
            "  inflating: data/img/10325.png      \n",
            "  inflating: data/img/92513.png      \n",
            "  inflating: data/img/20568.png      \n",
            "  inflating: data/img/31495.png      \n",
            "  inflating: data/img/93081.png      \n",
            "  inflating: data/img/07916.png      \n",
            "  inflating: data/img/04798.png      \n",
            "  inflating: data/img/16702.png      \n",
            "  inflating: data/img/31409.png      \n",
            "  inflating: data/img/96720.png      \n",
            "  inflating: data/img/45320.png      \n",
            "  inflating: data/img/91056.png      \n",
            "  inflating: data/img/43082.png      \n",
            "  inflating: data/img/63078.png      \n",
            "  inflating: data/img/47012.png      \n",
            "  inflating: data/img/59602.png      \n",
            "  inflating: data/img/36571.png      \n",
            "  inflating: data/img/05793.png      \n",
            "  inflating: data/img/59206.png      \n",
            "  inflating: data/img/97845.png      \n",
            "  inflating: data/img/86123.png      \n",
            "  inflating: data/img/07465.png      \n",
            "  inflating: data/img/74286.png      \n",
            "  inflating: data/img/34091.png      \n",
            "  inflating: data/img/75698.png      \n",
            "  inflating: data/img/15478.png      \n",
            "  inflating: data/img/81059.png      \n",
            "  inflating: data/img/27185.png      \n",
            "  inflating: data/img/83456.png      \n",
            "  inflating: data/img/69253.png      \n",
            "  inflating: data/img/49613.png      \n",
            "  inflating: data/img/35971.png      \n",
            "  inflating: data/img/09615.png      \n",
            "  inflating: data/img/35701.png      \n",
            "  inflating: data/img/64982.png      \n",
            "  inflating: data/img/08126.png      \n",
            "  inflating: data/img/12063.png      \n",
            "  inflating: data/img/50964.png      \n",
            "  inflating: data/img/89072.png      \n",
            "  inflating: data/img/03267.png      \n",
            "  inflating: data/img/07693.png      \n",
            "  inflating: data/img/25109.png      \n",
            "  inflating: data/img/79856.png      \n",
            "  inflating: data/img/91385.png      \n",
            "  inflating: data/img/87126.png      \n",
            "  inflating: data/img/26381.png      \n",
            "  inflating: data/img/43218.png      \n",
            "  inflating: data/img/34127.png      \n",
            "  inflating: data/img/54690.png      \n",
            "  inflating: data/img/72396.png      \n",
            "  inflating: data/img/86372.png      \n",
            "  inflating: data/img/63289.png      \n",
            "  inflating: data/img/54016.png      \n",
            "  inflating: data/img/83914.png      \n",
            "  inflating: data/img/85072.png      \n",
            "  inflating: data/img/37059.png      \n",
            "  inflating: data/img/46315.png      \n",
            "  inflating: data/img/76582.png      \n",
            "  inflating: data/img/76542.png      \n",
            "  inflating: data/img/84120.png      \n",
            "  inflating: data/img/68903.png      \n",
            "  inflating: data/img/19430.png      \n",
            "  inflating: data/img/73152.png      \n",
            "  inflating: data/img/64138.png      \n",
            "  inflating: data/img/23048.png      \n",
            "  inflating: data/img/20913.png      \n",
            "  inflating: data/img/82704.png      \n",
            "  inflating: data/img/64025.png      \n",
            "  inflating: data/img/24038.png      \n",
            "  inflating: data/img/87125.png      \n",
            "  inflating: data/img/39206.png      \n",
            "  inflating: data/img/27503.png      \n",
            "  inflating: data/img/32841.png      \n",
            "  inflating: data/img/49168.png      \n",
            "  inflating: data/img/26735.png      \n",
            "  inflating: data/img/58497.png      \n",
            "  inflating: data/img/28074.png      \n",
            "  inflating: data/img/76519.png      \n",
            "  inflating: data/img/92308.png      \n",
            "  inflating: data/img/64723.png      \n",
            "  inflating: data/img/36158.png      \n",
            "  inflating: data/img/40182.png      \n",
            "  inflating: data/img/41860.png      \n",
            "  inflating: data/img/48523.png      \n",
            "  inflating: data/img/05429.png      \n",
            "  inflating: data/img/52486.png      \n",
            "  inflating: data/img/43956.png      \n",
            "  inflating: data/img/98603.png      \n",
            "  inflating: data/img/62843.png      \n",
            "  inflating: data/img/14306.png      \n",
            "  inflating: data/img/74359.png      \n",
            "  inflating: data/img/76528.png      \n",
            "  inflating: data/img/96120.png      \n",
            "  inflating: data/img/96872.png      \n",
            "  inflating: data/img/89612.png      \n",
            "  inflating: data/img/20873.png      \n",
            "  inflating: data/img/32640.png      \n",
            "  inflating: data/img/08196.png      \n",
            "  inflating: data/img/86351.png      \n",
            "  inflating: data/img/41250.png      \n",
            "  inflating: data/img/68327.png      \n",
            "  inflating: data/img/42851.png      \n",
            "  inflating: data/img/67951.png      \n",
            "  inflating: data/img/75231.png      \n",
            "  inflating: data/img/50397.png      \n",
            "  inflating: data/img/57962.png      \n",
            "  inflating: data/img/01589.png      \n",
            "  inflating: data/img/81630.png      \n",
            "  inflating: data/img/67548.png      \n",
            "  inflating: data/img/23657.png      \n",
            "  inflating: data/img/89061.png      \n",
            "  inflating: data/img/21063.png      \n",
            "  inflating: data/img/64325.png      \n",
            "  inflating: data/img/23109.png      \n",
            "  inflating: data/img/49105.png      \n",
            "  inflating: data/img/70352.png      \n",
            "  inflating: data/img/42650.png      \n",
            "  inflating: data/img/17869.png      \n",
            "  inflating: data/img/92837.png      \n",
            "  inflating: data/img/51360.png      \n",
            "  inflating: data/img/38509.png      \n",
            "  inflating: data/img/94026.png      \n",
            "  inflating: data/img/15920.png      \n",
            "  inflating: data/img/31082.png      \n",
            "  inflating: data/img/08275.png      \n",
            "  inflating: data/img/32907.png      \n",
            "  inflating: data/img/42601.png      \n",
            "  inflating: data/img/92068.png      \n",
            "  inflating: data/img/76139.png      \n",
            "  inflating: data/img/31752.png      \n",
            "  inflating: data/img/07312.png      \n",
            "  inflating: data/img/79843.png      \n",
            "  inflating: data/img/57482.png      \n",
            "  inflating: data/img/45931.png      \n",
            "  inflating: data/img/23071.png      \n",
            "  inflating: data/img/86597.png      \n",
            "  inflating: data/img/71562.png      \n",
            "  inflating: data/img/06158.png      \n",
            "  inflating: data/img/93827.png      \n",
            "  inflating: data/img/12549.png      \n",
            "  inflating: data/img/09213.png      \n",
            "  inflating: data/img/80179.png      \n",
            "  inflating: data/img/96517.png      \n",
            "  inflating: data/img/40312.png      \n",
            "  inflating: data/img/94178.png      \n",
            "  inflating: data/img/14965.png      \n",
            "  inflating: data/img/12983.png      \n",
            "  inflating: data/img/60254.png      \n",
            "  inflating: data/img/19602.png      \n",
            "  inflating: data/img/56219.png      \n",
            "  inflating: data/img/38251.png      \n",
            "  inflating: data/img/96435.png      \n",
            "  inflating: data/img/87691.png      \n",
            "  inflating: data/img/92541.png      \n",
            "  inflating: data/img/74013.png      \n",
            "  inflating: data/img/05987.png      \n",
            "  inflating: data/img/07523.png      \n",
            "  inflating: data/img/71492.png      \n",
            "  inflating: data/img/56921.png      \n",
            "  inflating: data/img/26405.png      \n",
            "  inflating: data/img/51263.png      \n",
            "  inflating: data/img/91763.png      \n",
            "  inflating: data/img/17348.png      \n",
            "  inflating: data/img/98203.png      \n",
            "  inflating: data/img/18462.png      \n",
            "  inflating: data/img/80321.png      \n",
            "  inflating: data/img/98624.png      \n",
            "  inflating: data/img/79853.png      \n",
            "  inflating: data/img/70183.png      \n",
            "  inflating: data/img/35186.png      \n",
            "  inflating: data/img/16423.png      \n",
            "  inflating: data/img/19206.png      \n",
            "  inflating: data/img/41623.png      \n",
            "  inflating: data/img/74259.png      \n",
            "  inflating: data/img/30486.png      \n",
            "  inflating: data/img/21385.png      \n",
            "  inflating: data/img/46872.png      \n",
            "  inflating: data/img/14397.png      \n",
            "  inflating: data/img/23570.png      \n",
            "  inflating: data/img/84910.png      \n",
            "  inflating: data/img/70318.png      \n",
            "  inflating: data/img/39860.png      \n",
            "  inflating: data/img/43275.png      \n",
            "  inflating: data/img/15308.png      \n",
            "  inflating: data/img/84291.png      \n",
            "  inflating: data/img/38490.png      \n",
            "  inflating: data/img/75908.png      \n",
            "  inflating: data/img/48926.png      \n",
            "  inflating: data/img/02435.png      \n",
            "  inflating: data/img/97321.png      \n",
            "  inflating: data/img/84107.png      \n",
            "  inflating: data/img/21403.png      \n",
            "  inflating: data/img/26538.png      \n",
            "  inflating: data/img/71253.png      \n",
            "  inflating: data/img/87510.png      \n",
            "  inflating: data/img/41796.png      \n",
            "  inflating: data/img/38095.png      \n",
            "  inflating: data/img/68971.png      \n",
            "  inflating: data/img/02146.png      \n",
            "  inflating: data/img/68950.png      \n",
            "  inflating: data/img/91064.png      \n",
            "  inflating: data/img/36014.png      \n",
            "  inflating: data/img/16320.png      \n",
            "  inflating: data/img/71389.png      \n",
            "  inflating: data/img/61234.png      \n",
            "  inflating: data/img/57426.png      \n",
            "  inflating: data/img/37198.png      \n",
            "  inflating: data/img/36458.png      \n",
            "  inflating: data/img/54831.png      \n",
            "  inflating: data/img/20374.png      \n",
            "  inflating: data/img/09514.png      \n",
            "  inflating: data/img/58321.png      \n",
            "  inflating: data/img/28935.png      \n",
            "  inflating: data/img/76592.png      \n",
            "  inflating: data/img/41037.png      \n",
            "  inflating: data/img/12054.png      \n",
            "  inflating: data/img/58947.png      \n",
            "  inflating: data/img/01962.png      \n",
            "  inflating: data/img/52437.png      \n",
            "  inflating: data/img/60895.png      \n",
            "  inflating: data/img/48651.png      \n",
            "  inflating: data/img/54318.png      \n",
            "  inflating: data/img/46205.png      \n",
            "  inflating: data/img/90657.png      \n",
            "  inflating: data/img/91236.png      \n",
            "  inflating: data/img/07628.png      \n",
            "  inflating: data/img/63524.png      \n",
            "  inflating: data/img/71485.png      \n",
            "  inflating: data/img/01967.png      \n",
            "  inflating: data/img/15983.png      \n",
            "  inflating: data/img/72903.png      \n",
            "  inflating: data/img/27953.png      \n",
            "  inflating: data/img/09587.png      \n",
            "  inflating: data/img/08743.png      \n",
            "  inflating: data/img/70269.png      \n",
            "  inflating: data/img/37251.png      \n",
            "  inflating: data/img/90728.png      \n",
            "  inflating: data/img/37284.png      \n",
            "  inflating: data/img/18453.png      \n",
            "  inflating: data/img/12389.png      \n",
            "  inflating: data/img/79138.png      \n",
            "  inflating: data/img/58264.png      \n",
            "  inflating: data/img/20539.png      \n",
            "  inflating: data/img/61702.png      \n",
            "  inflating: data/img/08164.png      \n",
            "  inflating: data/img/06714.png      \n",
            "  inflating: data/img/81934.png      \n",
            "  inflating: data/img/60927.png      \n",
            "  inflating: data/img/41362.png      \n",
            "  inflating: data/img/53172.png      \n",
            "  inflating: data/img/95803.png      \n",
            "  inflating: data/img/67915.png      \n",
            "  inflating: data/img/94108.png      \n",
            "  inflating: data/img/56791.png      \n",
            "  inflating: data/img/62315.png      \n",
            "  inflating: data/img/36481.png      \n",
            "  inflating: data/img/04183.png      \n",
            "  inflating: data/img/96082.png      \n",
            "  inflating: data/img/94372.png      \n",
            "  inflating: data/img/89421.png      \n",
            "  inflating: data/img/83745.png      \n",
            "  inflating: data/img/18507.png      \n",
            "  inflating: data/img/36072.png      \n",
            "  inflating: data/img/26507.png      \n",
            "  inflating: data/img/09286.png      \n",
            "  inflating: data/img/42509.png      \n",
            "  inflating: data/img/90574.png      \n",
            "  inflating: data/img/23058.png      \n",
            "  inflating: data/img/78429.png      \n",
            "  inflating: data/img/02761.png      \n",
            "  inflating: data/img/04152.png      \n",
            "  inflating: data/img/89324.png      \n",
            "  inflating: data/img/17643.png      \n",
            "  inflating: data/img/26940.png      \n",
            "  inflating: data/img/47592.png      \n",
            "  inflating: data/img/01892.png      \n",
            "  inflating: data/img/87416.png      \n",
            "  inflating: data/img/42635.png      \n",
            "  inflating: data/img/59107.png      \n",
            "  inflating: data/img/30684.png      \n",
            "  inflating: data/img/78659.png      \n",
            "  inflating: data/img/64198.png      \n",
            "  inflating: data/img/50413.png      \n",
            "  inflating: data/img/63921.png      \n",
            "  inflating: data/img/35784.png      \n",
            "  inflating: data/img/81764.png      \n",
            "  inflating: data/img/82156.png      \n",
            "  inflating: data/img/32405.png      \n",
            "  inflating: data/img/17892.png      \n",
            "  inflating: data/img/38047.png      \n",
            "  inflating: data/img/27843.png      \n",
            "  inflating: data/img/20918.png      \n",
            "  inflating: data/img/37951.png      \n",
            "  inflating: data/img/65473.png      \n",
            "  inflating: data/img/40982.png      \n",
            "  inflating: data/img/07694.png      \n",
            "  inflating: data/img/41308.png      \n",
            "  inflating: data/img/91730.png      \n",
            "  inflating: data/img/21605.png      \n",
            "  inflating: data/img/34510.png      \n",
            "  inflating: data/img/38072.png      \n",
            "  inflating: data/img/86045.png      \n",
            "  inflating: data/img/56231.png      \n",
            "  inflating: data/img/92407.png      \n",
            "  inflating: data/img/64071.png      \n",
            "  inflating: data/img/47596.png      \n",
            "  inflating: data/img/18935.png      \n",
            "  inflating: data/img/61205.png      \n",
            "  inflating: data/img/17956.png      \n",
            "  inflating: data/img/27438.png      \n",
            "  inflating: data/img/06295.png      \n",
            "  inflating: data/img/79351.png      \n",
            "  inflating: data/img/52089.png      \n",
            "  inflating: data/img/95714.png      \n",
            "  inflating: data/img/90386.png      \n",
            "  inflating: data/img/04591.png      \n",
            "  inflating: data/img/05489.png      \n",
            "  inflating: data/img/94731.png      \n",
            "  inflating: data/img/97108.png      \n",
            "  inflating: data/img/61082.png      \n",
            "  inflating: data/img/19824.png      \n",
            "  inflating: data/img/80597.png      \n",
            "  inflating: data/img/97256.png      \n",
            "  inflating: data/img/79864.png      \n",
            "  inflating: data/img/06248.png      \n",
            "  inflating: data/img/29517.png      \n",
            "  inflating: data/img/71083.png      \n",
            "  inflating: data/img/78035.png      \n",
            "  inflating: data/img/61342.png      \n",
            "  inflating: data/img/58349.png      \n",
            "  inflating: data/img/05437.png      \n",
            "  inflating: data/img/83607.png      \n",
            "  inflating: data/img/56723.png      \n",
            "  inflating: data/img/34982.png      \n",
            "  inflating: data/img/18094.png      \n",
            "  inflating: data/img/31794.png      \n",
            "  inflating: data/img/63295.png      \n",
            "  inflating: data/img/46178.png      \n",
            "  inflating: data/img/62597.png      \n",
            "  inflating: data/img/35692.png      \n",
            "  inflating: data/img/41370.png      \n",
            "  inflating: data/img/70294.png      \n",
            "  inflating: data/img/74261.png      \n",
            "  inflating: data/img/59738.png      \n",
            "  inflating: data/img/78069.png      \n",
            "  inflating: data/img/75023.png      \n",
            "  inflating: data/img/05972.png      \n",
            "  inflating: data/img/47829.png      \n",
            "  inflating: data/img/54069.png      \n",
            "  inflating: data/img/06513.png      \n",
            "  inflating: data/img/81265.png      \n",
            "  inflating: data/img/25687.png      \n",
            "  inflating: data/img/63581.png      \n",
            "  inflating: data/img/19726.png      \n",
            "  inflating: data/img/24368.png      \n",
            "  inflating: data/img/76429.png      \n",
            "  inflating: data/img/57926.png      \n",
            "  inflating: data/img/63871.png      \n",
            "  inflating: data/img/02194.png      \n",
            "  inflating: data/img/75340.png      \n",
            "  inflating: data/img/97628.png      \n",
            "  inflating: data/img/74206.png      \n",
            "  inflating: data/img/28045.png      \n",
            "  inflating: data/img/50162.png      \n",
            "  inflating: data/img/21534.png      \n",
            "  inflating: data/img/61570.png      \n",
            "  inflating: data/img/59684.png      \n",
            "  inflating: data/img/21589.png      \n",
            "  inflating: data/img/98034.png      \n",
            "  inflating: data/img/42170.png      \n",
            "  inflating: data/img/37260.png      \n",
            "  inflating: data/img/06352.png      \n",
            "  inflating: data/img/79405.png      \n",
            "  inflating: data/img/04529.png      \n",
            "  inflating: data/img/89632.png      \n",
            "  inflating: data/img/63104.png      \n",
            "  inflating: data/img/53682.png      \n",
            "  inflating: data/img/81943.png      \n",
            "  inflating: data/img/93648.png      \n",
            "  inflating: data/img/68254.png      \n",
            "  inflating: data/img/19346.png      \n",
            "  inflating: data/img/63795.png      \n",
            "  inflating: data/img/21345.png      \n",
            "  inflating: data/img/12093.png      \n",
            "  inflating: data/img/53062.png      \n",
            "  inflating: data/img/48309.png      \n",
            "  inflating: data/img/24975.png      \n",
            "  inflating: data/img/53942.png      \n",
            "  inflating: data/img/54632.png      \n",
            "  inflating: data/img/61379.png      \n",
            "  inflating: data/img/21753.png      \n",
            "  inflating: data/img/90251.png      \n",
            "  inflating: data/img/63192.png      \n",
            "  inflating: data/img/52641.png      \n",
            "  inflating: data/img/43051.png      \n",
            "  inflating: data/img/74906.png      \n",
            "  inflating: data/img/75041.png      \n",
            "  inflating: data/img/73816.png      \n",
            "  inflating: data/img/32067.png      \n",
            "  inflating: data/img/29084.png      \n",
            "  inflating: data/img/23519.png      \n",
            "  inflating: data/img/64059.png      \n",
            "  inflating: data/img/75839.png      \n",
            "  inflating: data/img/05614.png      \n",
            "  inflating: data/img/60823.png      \n",
            "  inflating: data/img/91654.png      \n",
            "  inflating: data/img/32750.png      \n",
            "  inflating: data/img/10749.png      \n",
            "  inflating: data/img/61802.png      \n",
            "  inflating: data/img/97486.png      \n",
            "  inflating: data/img/49583.png      \n",
            "  inflating: data/img/83760.png      \n",
            "  inflating: data/img/81096.png      \n",
            "  inflating: data/img/67350.png      \n",
            "  inflating: data/img/08693.png      \n",
            "  inflating: data/img/37408.png      \n",
            "  inflating: data/img/05798.png      \n",
            "  inflating: data/img/74908.png      \n",
            "  inflating: data/img/21749.png      \n",
            "  inflating: data/img/48103.png      \n",
            "  inflating: data/img/96104.png      \n",
            "  inflating: data/img/29137.png      \n",
            "  inflating: data/img/09687.png      \n",
            "  inflating: data/img/27854.png      \n",
            "  inflating: data/img/03524.png      \n",
            "  inflating: data/img/72945.png      \n",
            "  inflating: data/img/80317.png      \n",
            "  inflating: data/img/46823.png      \n",
            "  inflating: data/img/95872.png      \n",
            "  inflating: data/img/05716.png      \n",
            "  inflating: data/img/98246.png      \n",
            "  inflating: data/img/49723.png      \n",
            "  inflating: data/img/56419.png      \n",
            "  inflating: data/img/18632.png      \n",
            "  inflating: data/img/23815.png      \n",
            "  inflating: data/img/83246.png      \n",
            "  inflating: data/img/98653.png      \n",
            "  inflating: data/img/09821.png      \n",
            "  inflating: data/img/80742.png      \n",
            "  inflating: data/img/59462.png      \n",
            "  inflating: data/img/47618.png      \n",
            "  inflating: data/img/90247.png      \n",
            "  inflating: data/img/09851.png      \n",
            "  inflating: data/img/32615.png      \n",
            "  inflating: data/img/07592.png      \n",
            "  inflating: data/img/97805.png      \n",
            "  inflating: data/img/16438.png      \n",
            "  inflating: data/img/47053.png      \n",
            "  inflating: data/img/59240.png      \n",
            "  inflating: data/img/92158.png      \n",
            "  inflating: data/img/34217.png      \n",
            "  inflating: data/img/42568.png      \n",
            "  inflating: data/img/62590.png      \n",
            "  inflating: data/img/13809.png      \n",
            "  inflating: data/img/61927.png      \n",
            "  inflating: data/img/58327.png      \n",
            "  inflating: data/img/51726.png      \n",
            "  inflating: data/img/35016.png      \n",
            "  inflating: data/img/45612.png      \n",
            "  inflating: data/img/04735.png      \n",
            "  inflating: data/img/17953.png      \n",
            "  inflating: data/img/07192.png      \n",
            "  inflating: data/img/27398.png      \n",
            "  inflating: data/img/67485.png      \n",
            "  inflating: data/img/61274.png      \n",
            "  inflating: data/img/93057.png      \n",
            "  inflating: data/img/81349.png      \n",
            "  inflating: data/img/98564.png      \n",
            "  inflating: data/img/85023.png      \n",
            "  inflating: data/img/23091.png      \n",
            "  inflating: data/img/87234.png      \n",
            "  inflating: data/img/67809.png      \n",
            "  inflating: data/img/65189.png      \n",
            "  inflating: data/img/82947.png      \n",
            "  inflating: data/img/64350.png      \n",
            "  inflating: data/img/32451.png      \n",
            "  inflating: data/img/95467.png      \n",
            "  inflating: data/img/45379.png      \n",
            "  inflating: data/img/52904.png      \n",
            "  inflating: data/img/10732.png      \n",
            "  inflating: data/img/47561.png      \n",
            "  inflating: data/img/57631.png      \n",
            "  inflating: data/img/83726.png      \n",
            "  inflating: data/img/52164.png      \n",
            "  inflating: data/img/43906.png      \n",
            "  inflating: data/img/09756.png      \n",
            "  inflating: data/img/80425.png      \n",
            "  inflating: data/img/73604.png      \n",
            "  inflating: data/img/61349.png      \n",
            "  inflating: data/img/98350.png      \n",
            "  inflating: data/img/31267.png      \n",
            "  inflating: data/img/30569.png      \n",
            "  inflating: data/img/71502.png      \n",
            "  inflating: data/img/54321.png      \n",
            "  inflating: data/img/72356.png      \n",
            "  inflating: data/img/06541.png      \n",
            "  inflating: data/img/23158.png      \n",
            "  inflating: data/img/60532.png      \n",
            "  inflating: data/img/60427.png      \n",
            "  inflating: data/img/52670.png      \n",
            "  inflating: data/img/47528.png      \n",
            "  inflating: data/img/53901.png      \n",
            "  inflating: data/img/65217.png      \n",
            "  inflating: data/img/61204.png      \n",
            "  inflating: data/img/30697.png      \n",
            "  inflating: data/img/67859.png      \n",
            "  inflating: data/img/72805.png      \n",
            "  inflating: data/img/84592.png      \n",
            "  inflating: data/img/17354.png      \n",
            "  inflating: data/img/64273.png      \n",
            "  inflating: data/img/34567.png      \n",
            "  inflating: data/img/92473.png      \n",
            "  inflating: data/img/61873.png      \n",
            "  inflating: data/img/16934.png      \n",
            "  inflating: data/img/40567.png      \n",
            "  inflating: data/img/48370.png      \n",
            "  inflating: data/img/72058.png      \n",
            "  inflating: data/img/05826.png      \n",
            "  inflating: data/img/51269.png      \n",
            "  inflating: data/img/89035.png      \n",
            "  inflating: data/img/65817.png      \n",
            "  inflating: data/img/82450.png      \n",
            "  inflating: data/img/58109.png      \n",
            "  inflating: data/img/14072.png      \n",
            "  inflating: data/img/46532.png      \n",
            "  inflating: data/img/56987.png      \n",
            "  inflating: data/img/48039.png      \n",
            "  inflating: data/img/74913.png      \n",
            "  inflating: data/img/82094.png      \n",
            "  inflating: data/img/32956.png      \n",
            "  inflating: data/img/26914.png      \n",
            "  inflating: data/img/63502.png      \n",
            "  inflating: data/img/82673.png      \n",
            "  inflating: data/img/52918.png      \n",
            "  inflating: data/img/32576.png      \n",
            "  inflating: data/img/75031.png      \n",
            "  inflating: data/img/70132.png      \n",
            "  inflating: data/img/40693.png      \n",
            "  inflating: data/img/76239.png      \n",
            "  inflating: data/img/64278.png      \n",
            "  inflating: data/img/19452.png      \n",
            "  inflating: data/img/58417.png      \n",
            "  inflating: data/img/09723.png      \n",
            "  inflating: data/img/46852.png      \n",
            "  inflating: data/img/46925.png      \n",
            "  inflating: data/img/97025.png      \n",
            "  inflating: data/img/14920.png      \n",
            "  inflating: data/img/65807.png      \n",
            "  inflating: data/img/91283.png      \n",
            "  inflating: data/img/39725.png      \n",
            "  inflating: data/img/03468.png      \n",
            "  inflating: data/img/47825.png      \n",
            "  inflating: data/img/92185.png      \n",
            "  inflating: data/img/84395.png      \n",
            "  inflating: data/img/21609.png      \n",
            "  inflating: data/img/02634.png      \n",
            "  inflating: data/img/31048.png      \n",
            "  inflating: data/img/54837.png      \n",
            "  inflating: data/img/96784.png      \n",
            "  inflating: data/img/32670.png      \n",
            "  inflating: data/img/23845.png      \n",
            "  inflating: data/img/82457.png      \n",
            "  inflating: data/img/74218.png      \n",
            "  inflating: data/img/31568.png      \n",
            "  inflating: data/img/40927.png      \n",
            "  inflating: data/img/41268.png      \n",
            "  inflating: data/img/27891.png      \n",
            "  inflating: data/img/49675.png      \n",
            "  inflating: data/img/90318.png      \n",
            "  inflating: data/img/95072.png      \n",
            "  inflating: data/img/05879.png      \n",
            "  inflating: data/img/02763.png      \n",
            "  inflating: data/img/49621.png      \n",
            "  inflating: data/img/21637.png      \n",
            "  inflating: data/img/87620.png      \n",
            "  inflating: data/img/93046.png      \n",
            "  inflating: data/img/51763.png      \n",
            "  inflating: data/img/87519.png      \n",
            "  inflating: data/img/82960.png      \n",
            "  inflating: data/img/72936.png      \n",
            "  inflating: data/img/98751.png      \n",
            "  inflating: data/img/81673.png      \n",
            "  inflating: data/img/84162.png      \n",
            "  inflating: data/img/34190.png      \n",
            "  inflating: data/img/78965.png      \n",
            "  inflating: data/img/02543.png      \n",
            "  inflating: data/img/27514.png      \n",
            "  inflating: data/img/85042.png      \n",
            "  inflating: data/img/84273.png      \n",
            "  inflating: data/img/28563.png      \n",
            "  inflating: data/img/51870.png      \n",
            "  inflating: data/img/87903.png      \n",
            "  inflating: data/img/21643.png      \n",
            "  inflating: data/img/03896.png      \n",
            "  inflating: data/img/25178.png      \n",
            "  inflating: data/img/83547.png      \n",
            "  inflating: data/img/64038.png      \n",
            "  inflating: data/img/16053.png      \n",
            "  inflating: data/img/29614.png      \n",
            "  inflating: data/img/92617.png      \n",
            "  inflating: data/img/74132.png      \n",
            "  inflating: data/img/49786.png      \n",
            "  inflating: data/img/68079.png      \n",
            "  inflating: data/img/98701.png      \n",
            "  inflating: data/img/15630.png      \n",
            "  inflating: data/img/90856.png      \n",
            "  inflating: data/img/79140.png      \n",
            "  inflating: data/img/91024.png      \n",
            "  inflating: data/img/76953.png      \n",
            "  inflating: data/img/71530.png      \n",
            "  inflating: data/img/18547.png      \n",
            "  inflating: data/img/79018.png      \n",
            "  inflating: data/img/87504.png      \n",
            "  inflating: data/img/91845.png      \n",
            "  inflating: data/img/92014.png      \n",
            "  inflating: data/img/34107.png      \n",
            "  inflating: data/img/50683.png      \n",
            "  inflating: data/img/21698.png      \n",
            "  inflating: data/img/98726.png      \n",
            "  inflating: data/img/95403.png      \n",
            "  inflating: data/img/32564.png      \n",
            "  inflating: data/img/28964.png      \n",
            "  inflating: data/img/39425.png      \n",
            "  inflating: data/img/94083.png      \n",
            "  inflating: data/img/58340.png      \n",
            "  inflating: data/img/01325.png      \n",
            "  inflating: data/LICENSE.txt        \n",
            "  inflating: data/train.jsonl        \n",
            "  inflating: data/test.jsonl         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7okx4tevMZxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ba80141f-1598-46fc-94c4-9a4593a2300b"
      },
      "source": [
        "!git clone https://github.com/airsplay/py-bottom-up-attention.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'py-bottom-up-attention'...\n",
            "remote: Enumerating objects: 1991, done.\u001b[K\n",
            "remote: Total 1991 (delta 0), reused 0 (delta 0), pack-reused 1991\u001b[K\n",
            "Receiving objects: 100% (1991/1991), 8.94 MiB | 5.21 MiB/s, done.\n",
            "Resolving deltas: 100% (1229/1229), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_tIrO8wMm2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5038dafe-3dc7-4ee9-aba0-85cda660f9ee"
      },
      "source": [
        "%cd py-bottom-up-attention/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/py-bottom-up-attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9OS-dVQM0VY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "706f0905-9efb-4790-cec1-2c66d130c1aa"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fvcore.git (from -r requirements.txt (line 1))\n",
            "  Cloning https://github.com/facebookresearch/fvcore.git to /tmp/pip-req-build-7lk4beaj\n",
            "  Running command git clone -q https://github.com/facebookresearch/fvcore.git /tmp/pip-req-build-7lk4beaj\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 18kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.29.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore==0.1.2->-r requirements.txt (line 1)) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: fvcore, pyyaml\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.2-cp36-none-any.whl size=47718 sha256=25d3c14da2e9dfcaa4b9bdbdb122da996f7b741e952c88ca4e46497c09910d3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h6hv24dh/wheels/48/53/79/3c6485543a4455a0006f5db590ab9957622b6227011941de06\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=8b27aa1a06e9172cbef4b65894ff5ae2695fc33648961c792fc16a614599aa35\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built fvcore pyyaml\n",
            "Installing collected packages: torch, torchvision, pyyaml, yacs, portalocker, fvcore\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed fvcore-0.1.2 portalocker-2.0.0 pyyaml-5.3.1 torch-1.4.0 torchvision-0.5.0 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXTQfQhlNf42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "169b4731-cc36-4459-87e5-24fc74a5d061"
      },
      "source": [
        "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-ysi4aljv\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-ysi4aljv\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.6.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266457 sha256=86473ac2501911bdbd665bcea5a140aaa1777d22cbb809ac8b6be28b4af28e3d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dj1znsu8/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.1\n",
            "    Uninstalling pycocotools-2.0.1:\n",
            "      Successfully uninstalled pycocotools-2.0.1\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aKG--X6Nouk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d602e1d3-4ad3-46fb-a553-0d2a631a3a96"
      },
      "source": [
        "!python setup.py build develop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/detectron2\n",
            "copying detectron2/__init__.py -> build/lib.linux-x86_64-3.6/detectron2\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "copying detectron2/model_zoo/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "copying detectron2/model_zoo/model_zoo.py -> build/lib.linux-x86_64-3.6/detectron2/model_zoo\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/boxes.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/keypoints.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/masks.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/image_list.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/instances.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "copying detectron2/structures/rotated_boxes.py -> build/lib.linux-x86_64-3.6/detectron2/structures\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/catalog.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/c2_model_loading.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "copying detectron2/checkpoint/detection_checkpoint.py -> build/lib.linux-x86_64-3.6/detectron2/checkpoint\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/sampling.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/box_regression.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/matcher.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/postprocessing.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/anchor_generator.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/test_time_augmentation.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "copying detectron2/modeling/poolers.py -> build/lib.linux-x86_64-3.6/detectron2/modeling\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/memory.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/comm.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/logger.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/serialize.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/video_visualizer.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/env.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/events.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/colormap.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/registry.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/visualizer.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "copying detectron2/utils/collect_env.py -> build/lib.linux-x86_64-3.6/detectron2/utils\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/catalog.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/dataset_mapper.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/detection_utils.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/build.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "copying detectron2/data/common.py -> build/lib.linux-x86_64-3.6/detectron2/data\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/evaluator.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/cityscapes_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/pascal_voc_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/lvis_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/testing.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/panoptic_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/sem_seg_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "copying detectron2/evaluation/coco_evaluation.py -> build/lib.linux-x86_64-3.6/detectron2/evaluation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/build.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "copying detectron2/solver/lr_scheduler.py -> build/lib.linux-x86_64-3.6/detectron2/solver\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/config.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/defaults.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "copying detectron2/config/compat.py -> build/lib.linux-x86_64-3.6/detectron2/config\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/launch.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/hooks.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/defaults.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "copying detectron2/engine/train_loop.py -> build/lib.linux-x86_64-3.6/detectron2/engine\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/mask_ops.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/batch_norm.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/deform_conv.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/roi_align_rotated.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/wrappers.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/shape_spec.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/nms.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/roi_align.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "copying detectron2/layers/rotated_boxes.py -> build/lib.linux-x86_64-3.6/detectron2/layers\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/roi_heads.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/rotated_fast_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/keypoint_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/mask_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/cascade_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/fast_rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "copying detectron2/modeling/roi_heads/box_head.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/roi_heads\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/fpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/backbone.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "copying detectron2/modeling/backbone/resnet.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/backbone\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/rcnn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/panoptic_fpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/semantic_seg.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "copying detectron2/modeling/meta_arch/retinanet.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/meta_arch\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rrpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rpn.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rrpn_outputs.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/build.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/rpn_outputs.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "copying detectron2/modeling/proposal_generator/proposal_utils.py -> build/lib.linux-x86_64-3.6/detectron2/modeling/proposal_generator\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/transform_gen.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "copying detectron2/data/transforms/transform.py -> build/lib.linux-x86_64-3.6/detectron2/data/transforms\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/cityscapes.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/builtin.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/pascal_voc.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/register_coco.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/coco.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/builtin_meta.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/lvis_v0_5_categories.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "copying detectron2/data/datasets/lvis.py -> build/lib.linux-x86_64-3.6/detectron2/data/datasets\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/grouped_batch_sampler.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/distributed_sampler.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "copying detectron2/data/samplers/__init__.py -> build/lib.linux-x86_64-3.6/detectron2/data/samplers\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-C4.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RetinaNet.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "copying detectron2/model_zoo/configs/Base-RCNN-DilatedC5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/mask_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "copying detectron2/model_zoo/configs/Detectron1-Comparisons/faster_rcnn_R_50_FPN_noaug_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Detectron1-Comparisons\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Cityscapes\n",
            "copying detectron2/model_zoo/configs/Cityscapes/mask_rcnn_R_50_FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Cityscapes\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_caffemaxpool.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "copying detectron2/model_zoo/configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/VG-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_C4.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "copying detectron2/model_zoo/configs/PascalVOC-Detection/faster_rcnn_R_50_FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/PascalVOC-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_DC5_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/retinanet_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_101_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "copying detectron2/model_zoo/configs/COCO-Detection/rpn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Detection\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/Base-Panoptic-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-PanopticSegmentation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/semantic_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_normalized_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/retinanet_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_DC5_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_C4_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/fast_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/panoptic_fpn_R_50_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/rpn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/keypoint_rcnn_R_50_FPN_training_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "copying detectron2/model_zoo/configs/quick_schedules/mask_rcnn_R_50_FPN_instant_test.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/quick_schedules\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_DC5_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-InstanceSegmentation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "copying detectron2/model_zoo/configs/LVIS-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/LVIS-InstanceSegmentation\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/scratch_mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/semantic_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_gn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_gn_dconv.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_3x_syncbn.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/mask_rcnn_R_50_FPN_1x_cls_agnostic.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "copying detectron2/model_zoo/configs/Misc/panoptic_fpn_R_101_dconv_cascade_gn_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/Misc\n",
            "creating build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/Base-Keypoint-RCNN-FPN.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "copying detectron2/model_zoo/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml -> build/lib.linux-x86_64-3.6/detectron2/model_zoo/configs/COCO-Keypoints\n",
            "running build_ext\n",
            "building 'detectron2._C' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/content\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated\n",
            "creating build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_filter(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(input.type().is_cuda(), \"input tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(weight.type().is_cuda(), \"weight tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(bias.type().is_cuda(), \"bias tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::\u001b[01;35m\u001b[Kd\u001b[m\u001b[Keprecated_AT_CHECK();                 \\\n",
            "                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:355:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     ::c10::detail::deprecated_AT_CHECK(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                 \\\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:5:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_CHECK\u001b[m\u001b[K’\n",
            "     \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_CHECK(offset.type().is_cuda(), \"offset tensor is not on GPU!\");\n",
            "     \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:5:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/vision.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:13:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline void \u001b[01;36m\u001b[Kdeprecated_AT_CHECK\u001b[m\u001b[K() {}\n",
            "             \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:103:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES\u001b[m\u001b[K(dets.type(), \"nms_rotated\", [&] {\n",
            "   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:429:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"ROIAlign_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.cpp:481:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.type(), \"ROIAlign_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:446:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp:497:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:104:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:350:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:402:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:639:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:691:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:939:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = c10::Half]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:97:991:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor detectron2::nms_rotated_cuda(const at::Tensor&, const at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:107:84:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   unsigned long long* mask_host = (unsigned long long*)mask_cpu.data<int64_t>();\n",
            "                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu:114:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   int64_t* keep_out = keep.data<int64_t>();\n",
            "                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:322:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            " \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:486:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:555:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:625:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1098:100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1168:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.cu:1241:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n",
            "                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:31:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:136:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:137:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:184:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:185:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:186:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_filter(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:234:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:235:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:284:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.ty\u001b[01;35m\u001b[Kpe().is_cuda(), \"bi\u001b[m\u001b[Kas tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:285:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.type().is_cuda(), \"bia\u001b[01;35m\u001b[Ks\u001b[m\u001b[K tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:286:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.t\u001b[01;35m\u001b[Kype().is_cuda(), \"i\u001b[m\u001b[Knput tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:341:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(input.type().is_cuda(), \"in\u001b[01;35m\u001b[Kp\u001b[m\u001b[Kut tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Kweight tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:342:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(weight.type().is_cuda(), \"w\u001b[01;35m\u001b[Ke\u001b[m\u001b[Kight tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.ty\u001b[01;35m\u001b[Kpe().is_cuda(), \"bi\u001b[m\u001b[Kas tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:343:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(bias.type().is_cuda(), \"bia\u001b[01;35m\u001b[Ks\u001b[m\u001b[K tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.\u001b[01;35m\u001b[Ktype().is_cuda(), \"\u001b[m\u001b[Koffset tensor is not on GPU!\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv.h:344:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(offset.type().is_cuda(), \"o\u001b[01;35m\u001b[Kf\u001b[m\u001b[Kfset tensor is not on GPU!\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::shape_check(at::Tensor, at::Tensor, at::Tensor*, at::Tensor, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:155:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:155:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:161:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:161:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:163:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:163:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:169:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:169:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:178:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:178:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:184:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:184:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:201:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:201:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:215:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:215:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:230:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:230:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:236:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:236:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:240:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:240:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:249:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:249:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:254:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:254:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:260:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K                  \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:260:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     AT_CHECK(\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_forward_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:338:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:338:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_input_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:503:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), 3, \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:503:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K 3, \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint detectron2::deform_conv_backward_parameters_cuda(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, float, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:696:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.s\u001b[01;35m\u001b[Kize(0) == batchSize\u001b[m\u001b[K), \"invalid batch size of offset\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:696:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK((offset.size(0) == batchSize)\u001b[01;35m\u001b[K,\u001b[m\u001b[K \"invalid batch size of offset\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_cuda_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:823:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_\u001b[01;35m\u001b[Kcontiguous(), \"inpu\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:823:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_contiguous(), \"input\u001b[01;35m\u001b[K \u001b[m\u001b[Ktensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:824:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:824:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid detectron2::modulated_deform_conv_cuda_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, int, int, int, int, int, int, int, int, int, int, bool)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:953:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_\u001b[01;35m\u001b[Kcontiguous(), \"inpu\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:953:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(input.is_contiguous(), \"input\u001b[01;35m\u001b[K \u001b[m\u001b[Ktensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:954:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is\u001b[01;35m\u001b[K_contiguous(), \"wei\u001b[m\u001b[Kght tensor has to be contiguous\");\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.cu:954:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid c10::detail::deprecated_AT_CHECK()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_CHECK(weight.is_contiguous(), \"weig\u001b[01;35m\u001b[Kh\u001b[m\u001b[Kt tensor has to be contiguous\");\n",
            "                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " \u001b[01;36m\u001b[Kinline void depreca\u001b[m\u001b[Kted_AT_CHECK() {}\n",
            " \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/py-bottom-up-attention/detectron2/layers/csrc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c /content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.cu -o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/vision.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlign/ROIAlign_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda_kernel.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/deformable/deform_conv_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.o build/temp.linux-x86_64-3.6/content/py-bottom-up-attention/detectron2/layers/csrc/cuda_version.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/detectron2/_C.cpython-36m-x86_64-linux-gnu.so\n",
            "running develop\n",
            "running egg_info\n",
            "creating detectron2.egg-info\n",
            "writing detectron2.egg-info/PKG-INFO\n",
            "writing dependency_links to detectron2.egg-info/dependency_links.txt\n",
            "writing requirements to detectron2.egg-info/requires.txt\n",
            "writing top-level names to detectron2.egg-info/top_level.txt\n",
            "writing manifest file 'detectron2.egg-info/SOURCES.txt'\n",
            "writing manifest file 'detectron2.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/detectron2/_C.cpython-36m-x86_64-linux-gnu.so -> detectron2\n",
            "Creating /usr/local/lib/python3.6/dist-packages/detectron2.egg-link (link to .)\n",
            "Adding detectron2 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/py-bottom-up-attention\n",
            "Processing dependencies for detectron2==0.1\n",
            "Searching for imagesize==1.2.0\n",
            "Best match: imagesize 1.2.0\n",
            "Adding imagesize 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard==2.3.0\n",
            "Best match: tensorboard 2.3.0\n",
            "Adding tensorboard 2.3.0 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tqdm==4.41.1\n",
            "Best match: tqdm 4.41.1\n",
            "Adding tqdm 4.41.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cloudpickle==1.3.0\n",
            "Best match: cloudpickle 1.3.0\n",
            "Adding cloudpickle 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tabulate==0.8.7\n",
            "Best match: tabulate 0.8.7\n",
            "Adding tabulate 0.8.7 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Pillow==7.0.0\n",
            "Best match: Pillow 7.0.0\n",
            "Adding Pillow 7.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==49.6.0\n",
            "Best match: setuptools 49.6.0\n",
            "Adding setuptools 49.6.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.12.4\n",
            "Best match: protobuf 3.12.4\n",
            "Adding protobuf 3.12.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.35.1\n",
            "Best match: wheel 0.35.1\n",
            "Adding wheel 0.35.1 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.7.0\n",
            "Best match: tensorboard-plugin-wit 1.7.0\n",
            "Adding tensorboard-plugin-wit 1.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.8.1\n",
            "Best match: absl-py 0.8.1\n",
            "Adding absl-py 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for grpcio==1.31.0\n",
            "Best match: grpcio 1.31.0\n",
            "Adding grpcio 1.31.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.1\n",
            "Best match: google-auth-oauthlib 0.4.1\n",
            "Adding google-auth-oauthlib 0.4.1 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth==1.17.2\n",
            "Best match: google-auth 1.17.2\n",
            "Adding google-auth 1.17.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for kiwisolver==1.2.0\n",
            "Best match: kiwisolver 1.2.0\n",
            "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyparsing==2.4.7\n",
            "Best match: pyparsing 2.4.7\n",
            "Adding pyparsing 2.4.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==5.3.1\n",
            "Best match: PyYAML 5.3.1\n",
            "Adding PyYAML 5.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==1.7.0\n",
            "Best match: importlib-metadata 1.7.0\n",
            "Adding importlib-metadata 1.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.6.20\n",
            "Best match: certifi 2020.6.20\n",
            "Adding certifi 2020.6.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for rsa==4.6\n",
            "Best match: rsa 4.6\n",
            "Adding rsa 4.6 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cachetools==4.1.1\n",
            "Best match: cachetools 4.1.1\n",
            "Adding cachetools 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.1.0\n",
            "Best match: zipp 3.1.0\n",
            "Adding zipp 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for oauthlib==3.1.0\n",
            "Best match: oauthlib 3.1.0\n",
            "Adding oauthlib 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for detectron2==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anaDSGseOQ70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "import detectron2\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Show the image in ipynb\n",
        "from IPython.display import clear_output, Image, display\n",
        "import PIL.Image\n",
        "def showarray(a, fmt='jpeg'):\n",
        "    a = np.uint8(np.clip(a, 0, 255))\n",
        "    f = io.BytesIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwhj7kv-Qh5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "426e66df-6771-4a64-c2cf-1692486f1dbf"
      },
      "source": [
        "print(os.listdir(os.curdir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['original_README.md', 'setup.cfg', 'detectron2.egg-info', '.flake8', '.circleci', 'GETTING_STARTED.md', 'docker', 'tests', 'build', 'configs', 'INSTALL.md', 'docs', '.gitignore', 'datasets', 'detectron2', 'README.md', 'tools', 'dev', 'demo', '.git', 'projects', '.clang-format', '.github', 'MODEL_ZOO.md', 'original_demo', 'LICENSE', 'requirements.txt', 'setup.py']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LDSh0ZYOjdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load VG Classes\n",
        "data_path = 'data/genome/1600-400-20'\n",
        "\n",
        "vg_classes = []\n",
        "with open(\"./demo/\" + data_path + \"/objects_vocab.txt\") as f:\n",
        "    for object in f.readlines():\n",
        "        vg_classes.append(object.split(',')[0].lower().strip())\n",
        "\n",
        "MetadataCatalog.get(\"vg\").thing_classes = vg_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHEZ6DCETHEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "095f6c23-b645-448c-ff47-c7fb3de02723"
      },
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"./configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml\")\n",
        "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "# VG Weight\n",
        "cfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe.pkl\"\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config './configs/VG-Detection/faster_rcnn_R_101_C4_caffe.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
            "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
            "\n",
            "Modifications for VG in RoI heads (modeling/roi_heads/roi_heads.py):\n",
            "\t1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1.\n",
            "\t2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2).\n",
            "\tFor more details, please check 'https://github.com/peteanderson80/bottom-up-attention/blob/master/models/vg/ResNet-101/faster_rcnn_end2end_final/test.prototxt'.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "faster_rcnn_from_caffe.pkl: 255MB [00:18, 14.0MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCqJmeVNa-kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwjLXhvdWkBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "64de0b1f-09d1-4f31-adf8-6e96be106dd6"
      },
      "source": [
        "im = cv2.imread(\"./demo/data/images/input.jpg\")\n",
        "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "showarray(im_rgb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJKrqaqWXL4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "0bb3b11d-07b1-4671-9f81-64c433cd8dac"
      },
      "source": [
        "NUM_OBJECTS = 36\n",
        "\n",
        "from detectron2.modeling.postprocessing import detector_postprocess\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n",
        "\n",
        "def doit(raw_image):\n",
        "    with torch.no_grad():\n",
        "        raw_height, raw_width = raw_image.shape[:2]\n",
        "        print(\"Original image size: \", (raw_height, raw_width))\n",
        "        \n",
        "        # Preprocessing\n",
        "        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n",
        "        print(\"Transformed image size: \", image.shape[:2])\n",
        "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
        "        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n",
        "        images = predictor.model.preprocess_image(inputs)\n",
        "        \n",
        "        # Run Backbone Res1-Res4\n",
        "        features = predictor.model.backbone(images.tensor)\n",
        "        \n",
        "        # Generate proposals with RPN\n",
        "        proposals, _ = predictor.model.proposal_generator(images, features, None)\n",
        "        proposal = proposals[0]\n",
        "        print('Proposal Boxes size:', proposal.proposal_boxes.tensor.shape)\n",
        "        \n",
        "        # Run RoI head for each proposal (RoI Pooling + Res5)\n",
        "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
        "        features = [features[f] for f in predictor.model.roi_heads.in_features]\n",
        "        box_features = predictor.model.roi_heads._shared_roi_transform(\n",
        "            features, proposal_boxes\n",
        "        )\n",
        "        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n",
        "        print('Pooled features size:', feature_pooled.shape)\n",
        "        \n",
        "        # Predict classes and boxes for each proposal.\n",
        "        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n",
        "        outputs = FastRCNNOutputs(\n",
        "            predictor.model.roi_heads.box2box_transform,\n",
        "            pred_class_logits,\n",
        "            pred_proposal_deltas,\n",
        "            proposals,\n",
        "            predictor.model.roi_heads.smooth_l1_beta,\n",
        "        )\n",
        "        probs = outputs.predict_probs()[0]\n",
        "        boxes = outputs.predict_boxes()[0]\n",
        "        \n",
        "        # Note: BUTD uses raw RoI predictions,\n",
        "        #       we use the predicted boxes instead.\n",
        "        # boxes = proposal_boxes[0].tensor    \n",
        "        \n",
        "        # NMS\n",
        "        for nms_thresh in np.arange(0.5, 1.0, 0.1):\n",
        "            instances, ids = fast_rcnn_inference_single_image(\n",
        "                boxes, probs, image.shape[1:], \n",
        "                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS\n",
        "            )\n",
        "            if len(ids) == NUM_OBJECTS:\n",
        "                break\n",
        "                \n",
        "        instances = detector_postprocess(instances, raw_height, raw_width)\n",
        "        roi_features = feature_pooled[ids].detach()\n",
        "        print(instances)\n",
        "        \n",
        "        return instances, roi_features\n",
        "    \n",
        "instances, features = doit(im)\n",
        "\n",
        "print(instances.pred_boxes)\n",
        "print(instances.scores)\n",
        "print(instances.pred_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original image size:  (480, 640)\n",
            "Transformed image size:  (800, 1067)\n",
            "Proposal Boxes size: torch.Size([151, 4])\n",
            "Pooled features size: torch.Size([151, 2048])\n",
            "Instances(num_instances=36, image_height=480, image_width=640, fields=[pred_boxes, scores, pred_classes])\n",
            "Boxes(tensor([[1.7333e+02, 2.1515e+02, 4.8672e+02, 4.7373e+02],\n",
            "        [1.2166e+02, 2.0614e+02, 3.4905e+02, 4.8000e+02],\n",
            "        [5.8896e+02, 0.0000e+00, 6.3909e+02, 3.6998e+02],\n",
            "        [6.0792e+02, 9.0849e+01, 6.3765e+02, 4.2150e+02],\n",
            "        [2.8171e+02, 1.6275e+02, 3.2904e+02, 1.9557e+02],\n",
            "        [1.5337e+02, 9.6636e+01, 3.9307e+02, 4.5865e+02],\n",
            "        [3.9510e+00, 3.0139e-01, 1.7087e+02, 3.7003e+02],\n",
            "        [2.0478e+02, 0.0000e+00, 3.0078e+02, 2.7645e+02],\n",
            "        [3.8164e+02, 3.1898e+02, 6.1028e+02, 4.2289e+02],\n",
            "        [4.2380e+02, 2.7979e+02, 6.3800e+02, 3.9043e+02],\n",
            "        [5.3907e+01, 2.1506e+01, 1.2955e+02, 3.8665e+02],\n",
            "        [2.1639e+02, 3.3180e+02, 4.9085e+02, 4.7821e+02],\n",
            "        [4.5419e+01, 3.1766e+02, 5.8115e+02, 4.7680e+02],\n",
            "        [5.2262e+01, 1.5123e+02, 4.9093e+02, 4.3199e+02],\n",
            "        [3.4266e+02, 4.8674e+01, 6.3398e+02, 3.8901e+02],\n",
            "        [2.4584e+02, 1.8033e+02, 3.4975e+02, 4.0818e+02],\n",
            "        [1.7189e+02, 1.6335e+02, 6.3919e+02, 4.1045e+02],\n",
            "        [1.9629e+01, 0.0000e+00, 5.6497e+02, 1.5761e+02],\n",
            "        [3.9222e+02, 0.0000e+00, 6.3402e+02, 2.7783e+02],\n",
            "        [3.6025e+01, 0.0000e+00, 5.5431e+02, 2.8221e+02],\n",
            "        [1.5994e+02, 1.5115e+00, 3.5376e+02, 3.1772e+02],\n",
            "        [2.9326e+02, 1.4786e+02, 3.2540e+02, 1.8938e+02],\n",
            "        [0.0000e+00, 3.6491e+02, 4.3185e+02, 4.7849e+02],\n",
            "        [1.9907e+01, 4.2409e+02, 4.5854e+02, 4.7957e+02],\n",
            "        [4.9555e+00, 8.1566e+01, 2.3710e+02, 4.5235e+02],\n",
            "        [5.5625e+02, 2.7353e+02, 6.0216e+02, 3.7322e+02],\n",
            "        [9.2086e+01, 2.8328e+02, 3.2548e+02, 4.4708e+02],\n",
            "        [1.7761e+02, 3.6624e+02, 4.5720e+02, 4.6956e+02],\n",
            "        [1.7253e+02, 3.7161e+02, 6.4000e+02, 4.7876e+02],\n",
            "        [2.7954e+02, 2.0651e+02, 3.4036e+02, 3.1626e+02],\n",
            "        [1.9732e+02, 3.8317e+01, 6.4000e+02, 3.2455e+02],\n",
            "        [2.7082e+02, 1.1922e+00, 5.8803e+02, 3.0338e+02],\n",
            "        [6.5850e+00, 1.8703e+02, 3.0191e+02, 4.7954e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2748e+02, 2.3305e+02],\n",
            "        [2.4732e-01, 3.3860e+02, 3.1494e+02, 4.7737e+02],\n",
            "        [2.0554e+02, 1.9619e+00, 6.4000e+02, 2.7667e+02]], device='cuda:0'))\n",
            "tensor([0.9641, 0.9235, 0.7266, 0.6717, 0.6676, 0.5797, 0.5345, 0.4948, 0.4913,\n",
            "        0.4549, 0.4345, 0.4332, 0.4173, 0.3899, 0.3781, 0.3740, 0.3434, 0.3397,\n",
            "        0.3274, 0.3188, 0.3128, 0.3019, 0.3018, 0.2908, 0.2826, 0.2780, 0.2777,\n",
            "        0.2759, 0.2679, 0.2581, 0.2509, 0.2487, 0.2475, 0.2468, 0.2462, 0.2415],\n",
            "       device='cuda:0')\n",
            "tensor([  42,   42,  601,  601,  234,   42,  291,  291,  236,  540,  291,   42,\n",
            "         236,   42,  291,   50,  540, 1180,  291,  381,  291,  234,  465,  465,\n",
            "         291,  364,   42,  829,  465,   51,  381,  291,   42, 1180,  236, 1180],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bqn1Z0gXfdu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "425ae500-e6f4-49db-f616-33e0978cfaee"
      },
      "source": [
        "pred = instances.to('cpu')\n",
        "v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n",
        "v = v.draw_instance_predictions(pred)\n",
        "showarray(v.get_image()[:, :, ::-1])\n",
        "print('instances:\\n', instances)\n",
        "print()\n",
        "print('boxes:\\n', instances.pred_boxes)\n",
        "print()\n",
        "print('Shape of features:\\n', features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "instances:\n",
            " Instances(num_instances=36, image_height=480, image_width=640, fields=[pred_boxes, scores, pred_classes])\n",
            "\n",
            "boxes:\n",
            " Boxes(tensor([[1.7333e+02, 2.1515e+02, 4.8672e+02, 4.7373e+02],\n",
            "        [1.2166e+02, 2.0614e+02, 3.4905e+02, 4.8000e+02],\n",
            "        [5.8896e+02, 0.0000e+00, 6.3909e+02, 3.6998e+02],\n",
            "        [6.0792e+02, 9.0849e+01, 6.3765e+02, 4.2150e+02],\n",
            "        [2.8171e+02, 1.6275e+02, 3.2904e+02, 1.9557e+02],\n",
            "        [1.5337e+02, 9.6636e+01, 3.9307e+02, 4.5865e+02],\n",
            "        [3.9510e+00, 3.0139e-01, 1.7087e+02, 3.7003e+02],\n",
            "        [2.0478e+02, 0.0000e+00, 3.0078e+02, 2.7645e+02],\n",
            "        [3.8164e+02, 3.1898e+02, 6.1028e+02, 4.2289e+02],\n",
            "        [4.2380e+02, 2.7979e+02, 6.3800e+02, 3.9043e+02],\n",
            "        [5.3907e+01, 2.1506e+01, 1.2955e+02, 3.8665e+02],\n",
            "        [2.1639e+02, 3.3180e+02, 4.9085e+02, 4.7821e+02],\n",
            "        [4.5419e+01, 3.1766e+02, 5.8115e+02, 4.7680e+02],\n",
            "        [5.2262e+01, 1.5123e+02, 4.9093e+02, 4.3199e+02],\n",
            "        [3.4266e+02, 4.8674e+01, 6.3398e+02, 3.8901e+02],\n",
            "        [2.4584e+02, 1.8033e+02, 3.4975e+02, 4.0818e+02],\n",
            "        [1.7189e+02, 1.6335e+02, 6.3919e+02, 4.1045e+02],\n",
            "        [1.9629e+01, 0.0000e+00, 5.6497e+02, 1.5761e+02],\n",
            "        [3.9222e+02, 0.0000e+00, 6.3402e+02, 2.7783e+02],\n",
            "        [3.6025e+01, 0.0000e+00, 5.5431e+02, 2.8221e+02],\n",
            "        [1.5994e+02, 1.5115e+00, 3.5376e+02, 3.1772e+02],\n",
            "        [2.9326e+02, 1.4786e+02, 3.2540e+02, 1.8938e+02],\n",
            "        [0.0000e+00, 3.6491e+02, 4.3185e+02, 4.7849e+02],\n",
            "        [1.9907e+01, 4.2409e+02, 4.5854e+02, 4.7957e+02],\n",
            "        [4.9555e+00, 8.1566e+01, 2.3710e+02, 4.5235e+02],\n",
            "        [5.5625e+02, 2.7353e+02, 6.0216e+02, 3.7322e+02],\n",
            "        [9.2086e+01, 2.8328e+02, 3.2548e+02, 4.4708e+02],\n",
            "        [1.7761e+02, 3.6624e+02, 4.5720e+02, 4.6956e+02],\n",
            "        [1.7253e+02, 3.7161e+02, 6.4000e+02, 4.7876e+02],\n",
            "        [2.7954e+02, 2.0651e+02, 3.4036e+02, 3.1626e+02],\n",
            "        [1.9732e+02, 3.8317e+01, 6.4000e+02, 3.2455e+02],\n",
            "        [2.7082e+02, 1.1922e+00, 5.8803e+02, 3.0338e+02],\n",
            "        [6.5850e+00, 1.8703e+02, 3.0191e+02, 4.7954e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2748e+02, 2.3305e+02],\n",
            "        [2.4732e-01, 3.3860e+02, 3.1494e+02, 4.7737e+02],\n",
            "        [2.0554e+02, 1.9619e+00, 6.4000e+02, 2.7667e+02]], device='cuda:0'))\n",
            "\n",
            "Shape of features:\n",
            " torch.Size([36, 2048])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYuji04mb_tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "megadictionary = {}\n",
        "\n",
        "img_name = \"blah\"\n",
        "\n",
        "dictionary = instances.get_fields()\n",
        "dictionary[\"features\"] = features\n",
        "\n",
        "megadictionary[img_name] = dictionary\n",
        "\n",
        "torch.save(megadictionary, \"test.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STGc3M0mjfE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "18cf97ff-1db2-45ed-d952-dc098122a36b"
      },
      "source": [
        "readdict = {}\n",
        "\n",
        "readdict = torch.load(\"test.pt\");\n",
        "\n",
        "print(readdict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'blah': {'pred_boxes': Boxes(tensor([[2.0987e+02, 3.6450e+02, 4.5232e+02, 3.9380e+02],\n",
            "        [1.8665e+02, 2.3823e+00, 4.3830e+02, 2.0241e+02],\n",
            "        [0.0000e+00, 8.5354e-01, 6.7804e+01, 1.4026e+02],\n",
            "        [4.4880e+02, 2.6857e+02, 6.2217e+02, 2.8942e+02],\n",
            "        [8.3611e+00, 1.9524e+02, 9.7161e+01, 3.6417e+02],\n",
            "        [1.8274e+02, 1.2376e+02, 2.7702e+02, 1.6970e+02],\n",
            "        [3.1325e+02, 2.6958e+00, 5.6982e+02, 2.5743e+02],\n",
            "        [1.6225e+02, 1.0916e+02, 3.1264e+02, 1.7449e+02],\n",
            "        [0.0000e+00, 1.5102e+02, 1.3942e+02, 3.9104e+02],\n",
            "        [1.8802e+01, 1.3427e+02, 7.9615e+01, 1.6113e+02],\n",
            "        [5.8672e+02, 3.0131e+02, 6.4000e+02, 3.1959e+02],\n",
            "        [1.0343e+01, 1.3077e+02, 2.7854e+01, 1.6667e+02],\n",
            "        [8.2153e+01, 6.5260e-02, 3.5885e+02, 1.4356e+02],\n",
            "        [1.8849e+00, 9.7618e+00, 1.2041e+02, 1.4973e+02],\n",
            "        [5.1264e+02, 1.5049e+02, 5.7573e+02, 2.2194e+02],\n",
            "        [2.3276e+02, 1.6450e+02, 3.4166e+02, 2.5127e+02],\n",
            "        [0.0000e+00, 1.0218e+02, 3.9495e+02, 3.6608e+02],\n",
            "        [4.3974e+02, 1.5149e+00, 6.3805e+02, 2.2758e+02],\n",
            "        [4.7855e+02, 2.0933e+02, 6.0876e+02, 2.5409e+02],\n",
            "        [3.8695e+02, 2.7119e+02, 4.4942e+02, 2.9162e+02],\n",
            "        [1.7998e+02, 1.8418e+02, 6.3362e+02, 3.7988e+02],\n",
            "        [1.5458e+00, 4.3223e+01, 3.7518e+02, 2.6523e+02],\n",
            "        [5.0566e+02, 1.7280e+02, 5.6188e+02, 2.3039e+02],\n",
            "        [0.0000e+00, 2.0698e+02, 5.0409e+02, 3.9400e+02],\n",
            "        [1.5429e+02, 2.2672e+02, 6.4000e+02, 3.9257e+02],\n",
            "        [3.7594e+02, 2.1059e+02, 6.3344e+02, 2.6454e+02],\n",
            "        [4.4152e+02, 1.9466e+02, 6.3003e+02, 2.6199e+02],\n",
            "        [1.7814e+00, 7.7378e+01, 2.0390e+02, 3.2613e+02],\n",
            "        [1.1344e+02, 1.4635e+01, 3.7469e+02, 2.4316e+02],\n",
            "        [1.7266e+02, 3.8359e-01, 6.2736e+02, 1.6712e+02],\n",
            "        [3.4923e+02, 2.2099e+02, 6.4000e+02, 3.9312e+02],\n",
            "        [1.4995e+02, 7.6198e+01, 4.1770e+02, 3.1782e+02],\n",
            "        [2.1059e+00, 2.1763e+02, 2.6233e+02, 3.9346e+02],\n",
            "        [2.4461e+02, 1.7256e+02, 3.3085e+02, 2.4876e+02],\n",
            "        [3.7871e+02, 2.7576e-01, 6.4000e+02, 1.7034e+02],\n",
            "        [3.8010e+02, 2.1043e+02, 6.3240e+02, 2.6447e+02]], device='cuda:0')), 'scores': tensor([0.8933, 0.8112, 0.7846, 0.7817, 0.6890, 0.6546, 0.6280, 0.6272, 0.5838,\n",
            "        0.5707, 0.5531, 0.5195, 0.4879, 0.4795, 0.4494, 0.4308, 0.4015, 0.3989,\n",
            "        0.3878, 0.3856, 0.3802, 0.3757, 0.3674, 0.3563, 0.3509, 0.3253, 0.3001,\n",
            "        0.2939, 0.2641, 0.2611, 0.2584, 0.2553, 0.2512, 0.2481, 0.2275, 0.2012],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 683,  291,  291,  683,  397,  251,  291,  251,  397,   52,  937,  128,\n",
            "         291,  291,   50,   52,  299,  291,  248,  937,  299,  299,   50,  299,\n",
            "         308,  397,  248,  299,  291,  381,  299,  299,  397,  251, 1180,  906],\n",
            "       device='cuda:0'), 'COCO_train2014_000000578174.jpg': tensor([[0.0000e+00, 8.4375e-01, 0.0000e+00,  ..., 0.0000e+00, 1.1957e+00,\n",
            "         0.0000e+00],\n",
            "        [0.0000e+00, 1.4626e-03, 2.1931e+00,  ..., 1.6413e+00, 0.0000e+00,\n",
            "         1.7242e+00],\n",
            "        [5.1999e-03, 0.0000e+00, 1.9752e+00,  ..., 7.7379e-01, 0.0000e+00,\n",
            "         1.1940e-02],\n",
            "        ...,\n",
            "        [6.8626e-03, 5.2631e-03, 7.5067e-01,  ..., 2.9698e+00, 0.0000e+00,\n",
            "         2.5644e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7213e-01,  ..., 7.0565e-03, 2.1306e-01,\n",
            "         3.1533e-01],\n",
            "        [0.0000e+00, 6.2195e-01, 6.9650e-01,  ..., 0.0000e+00, 2.9579e-02,\n",
            "         1.3168e-01]], device='cuda:0')}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZCaI-G8kpH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95bccbc8-d6ec-43f1-e6c5-0b377e61b027"
      },
      "source": [
        "path = \"/content/data/img\"\n",
        "\n",
        "megadictionary = {}\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "  os.path.join(path, filename)\n",
        "  \n",
        "  im = cv2.imread(os.path.join(path, filename))\n",
        "  im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  instances, features = doit(im)\n",
        "\n",
        "\n",
        "  dictionary = instances.get_fields().copy()\n",
        "  dictionary[\"features\"] = features\n",
        "\n",
        "  megadictionary[filename] = dictionary.copy()\n",
        "\n",
        "torch.save(megadictionary, \"fhmc_embeddings.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=29, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 258)\n",
            "Transformed image size:  (1237, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=34, image_height=399, image_width=258, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 479)\n",
            "Transformed image size:  (1333, 798)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=479, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (363, 550)\n",
            "Transformed image size:  (800, 1212)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=363, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 299)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=299, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (531, 800)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=531, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 525)\n",
            "Transformed image size:  (1219, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=525, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 591)\n",
            "Transformed image size:  (1083, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=591, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (526, 800)\n",
            "Transformed image size:  (800, 1217)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=526, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 582)\n",
            "Transformed image size:  (1100, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=582, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (414, 550)\n",
            "Transformed image size:  (800, 1063)\n",
            "Proposal Boxes size: torch.Size([145, 4])\n",
            "Pooled features size: torch.Size([145, 2048])\n",
            "Instances(num_instances=36, image_height=414, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 296)\n",
            "Transformed image size:  (1081, 800)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=296, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 265)\n",
            "Transformed image size:  (1208, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=265, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 265)\n",
            "Transformed image size:  (1205, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=265, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 600)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=600, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 559)\n",
            "Transformed image size:  (1145, 800)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=559, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (588, 825)\n",
            "Transformed image size:  (800, 1122)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=588, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([168, 4])\n",
            "Pooled features size: torch.Size([168, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=29, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([84, 4])\n",
            "Pooled features size: torch.Size([84, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (387, 550)\n",
            "Transformed image size:  (800, 1137)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=387, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 707)\n",
            "Transformed image size:  (905, 800)\n",
            "Proposal Boxes size: torch.Size([152, 4])\n",
            "Pooled features size: torch.Size([152, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=707, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (391, 550)\n",
            "Transformed image size:  (800, 1125)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=391, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 295)\n",
            "Transformed image size:  (1082, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=295, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 480)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=480, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([175, 4])\n",
            "Pooled features size: torch.Size([175, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (417, 550)\n",
            "Transformed image size:  (800, 1055)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=417, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (368, 550)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=368, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (354, 550)\n",
            "Transformed image size:  (800, 1243)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=28, image_height=354, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (462, 825)\n",
            "Transformed image size:  (746, 1333)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=36, image_height=462, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 626)\n",
            "Transformed image size:  (1022, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=626, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 643)\n",
            "Transformed image size:  (995, 800)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=643, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 567)\n",
            "Transformed image size:  (1129, 800)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=567, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 449)\n",
            "Transformed image size:  (1333, 748)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=449, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 373)\n",
            "Transformed image size:  (1333, 622)\n",
            "Proposal Boxes size: torch.Size([84, 4])\n",
            "Pooled features size: torch.Size([84, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=373, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 531)\n",
            "Transformed image size:  (1205, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=531, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 296)\n",
            "Transformed image size:  (1333, 493)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=296, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 552)\n",
            "Transformed image size:  (1159, 800)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=552, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (469, 550)\n",
            "Transformed image size:  (800, 938)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=13, image_height=469, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 200)\n",
            "Transformed image size:  (1333, 667)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=200, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([137, 4])\n",
            "Pooled features size: torch.Size([137, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (532, 550)\n",
            "Transformed image size:  (800, 827)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=532, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([160, 4])\n",
            "Pooled features size: torch.Size([160, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 400)\n",
            "Transformed image size:  (1333, 667)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (423, 550)\n",
            "Transformed image size:  (800, 1040)\n",
            "Proposal Boxes size: torch.Size([160, 4])\n",
            "Pooled features size: torch.Size([160, 2048])\n",
            "Instances(num_instances=36, image_height=423, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (563, 825)\n",
            "Transformed image size:  (800, 1172)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=563, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (559, 825)\n",
            "Transformed image size:  (800, 1181)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=559, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 712)\n",
            "Transformed image size:  (899, 800)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=712, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (439, 550)\n",
            "Transformed image size:  (800, 1002)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=439, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (368, 550)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=368, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (398, 550)\n",
            "Transformed image size:  (800, 1106)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=398, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 508)\n",
            "Transformed image size:  (1260, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=508, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (488, 825)\n",
            "Transformed image size:  (788, 1333)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=488, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 360)\n",
            "Transformed image size:  (1333, 600)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=360, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (368, 550)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=368, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 532)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=532, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (567, 825)\n",
            "Transformed image size:  (800, 1164)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=567, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=33, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (342, 550)\n",
            "Transformed image size:  (800, 1287)\n",
            "Proposal Boxes size: torch.Size([151, 4])\n",
            "Pooled features size: torch.Size([151, 2048])\n",
            "Instances(num_instances=36, image_height=342, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 617)\n",
            "Transformed image size:  (1037, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=617, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 277)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=277, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 132)\n",
            "Transformed image size:  (1333, 441)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=132, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 638)\n",
            "Transformed image size:  (1003, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=638, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 721)\n",
            "Transformed image size:  (888, 800)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=721, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (329, 550)\n",
            "Transformed image size:  (797, 1333)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=329, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 347)\n",
            "Transformed image size:  (922, 800)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=27, image_height=400, image_width=347, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 282)\n",
            "Transformed image size:  (1135, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=282, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 583)\n",
            "Transformed image size:  (1098, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=583, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 245)\n",
            "Transformed image size:  (1306, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=245, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 681)\n",
            "Transformed image size:  (940, 800)\n",
            "Proposal Boxes size: torch.Size([137, 4])\n",
            "Pooled features size: torch.Size([137, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=681, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (626, 800)\n",
            "Transformed image size:  (800, 1022)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=626, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 497)\n",
            "Transformed image size:  (1288, 800)\n",
            "Proposal Boxes size: torch.Size([146, 4])\n",
            "Pooled features size: torch.Size([146, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=497, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([173, 4])\n",
            "Pooled features size: torch.Size([173, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([79, 4])\n",
            "Pooled features size: torch.Size([79, 2048])\n",
            "Instances(num_instances=20, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=24, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (583, 825)\n",
            "Transformed image size:  (800, 1132)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=583, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 245)\n",
            "Transformed image size:  (1306, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=245, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 712)\n",
            "Transformed image size:  (899, 800)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=712, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=17, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 550)\n",
            "Transformed image size:  (800, 1103)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 568)\n",
            "Transformed image size:  (1127, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=568, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 300)\n",
            "Transformed image size:  (1333, 500)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 416)\n",
            "Transformed image size:  (1333, 693)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=416, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (355, 550)\n",
            "Transformed image size:  (800, 1239)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=355, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (346, 550)\n",
            "Transformed image size:  (800, 1272)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=346, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 534)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=534, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 480)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=480, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (647, 825)\n",
            "Transformed image size:  (800, 1020)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=647, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 300)\n",
            "Transformed image size:  (1333, 500)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (362, 550)\n",
            "Transformed image size:  (800, 1215)\n",
            "Proposal Boxes size: torch.Size([157, 4])\n",
            "Pooled features size: torch.Size([157, 2048])\n",
            "Instances(num_instances=36, image_height=362, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 199)\n",
            "Transformed image size:  (1333, 663)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=199, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([84, 4])\n",
            "Pooled features size: torch.Size([84, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 703)\n",
            "Transformed image size:  (910, 800)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=703, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (551, 825)\n",
            "Transformed image size:  (800, 1198)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=551, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (518, 825)\n",
            "Transformed image size:  (800, 1274)\n",
            "Proposal Boxes size: torch.Size([153, 4])\n",
            "Pooled features size: torch.Size([153, 2048])\n",
            "Instances(num_instances=36, image_height=518, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=1, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 606)\n",
            "Transformed image size:  (1056, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=606, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (555, 825)\n",
            "Transformed image size:  (800, 1189)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=555, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([72, 4])\n",
            "Pooled features size: torch.Size([72, 2048])\n",
            "Instances(num_instances=9, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (357, 550)\n",
            "Transformed image size:  (800, 1232)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=357, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (551, 800)\n",
            "Transformed image size:  (800, 1162)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=551, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (700, 800)\n",
            "Transformed image size:  (800, 914)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=700, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([159, 4])\n",
            "Pooled features size: torch.Size([159, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 736)\n",
            "Transformed image size:  (870, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=736, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 609)\n",
            "Transformed image size:  (1051, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=609, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (539, 800)\n",
            "Transformed image size:  (800, 1187)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=539, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (556, 800)\n",
            "Transformed image size:  (800, 1151)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=556, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (518, 800)\n",
            "Transformed image size:  (800, 1236)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=518, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (480, 800)\n",
            "Transformed image size:  (800, 1333)\n",
            "Proposal Boxes size: torch.Size([164, 4])\n",
            "Pooled features size: torch.Size([164, 2048])\n",
            "Instances(num_instances=36, image_height=480, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([154, 4])\n",
            "Pooled features size: torch.Size([154, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 712)\n",
            "Transformed image size:  (899, 800)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=712, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (576, 825)\n",
            "Transformed image size:  (800, 1146)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=576, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([167, 4])\n",
            "Pooled features size: torch.Size([167, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 288)\n",
            "Transformed image size:  (1111, 800)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=288, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (344, 550)\n",
            "Transformed image size:  (800, 1279)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=344, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (631, 825)\n",
            "Transformed image size:  (800, 1046)\n",
            "Proposal Boxes size: torch.Size([155, 4])\n",
            "Pooled features size: torch.Size([155, 2048])\n",
            "Instances(num_instances=36, image_height=631, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 350)\n",
            "Transformed image size:  (1333, 583)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=350, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (368, 550)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=368, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (530, 800)\n",
            "Transformed image size:  (800, 1208)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=530, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (425, 550)\n",
            "Transformed image size:  (800, 1035)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=425, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 277)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=277, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=17, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (539, 825)\n",
            "Transformed image size:  (800, 1224)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=539, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([148, 4])\n",
            "Pooled features size: torch.Size([148, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (567, 825)\n",
            "Transformed image size:  (800, 1164)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=567, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 313)\n",
            "Transformed image size:  (1020, 800)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=313, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 275)\n",
            "Transformed image size:  (1164, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=275, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 283)\n",
            "Transformed image size:  (1131, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=283, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (420, 550)\n",
            "Transformed image size:  (800, 1048)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=420, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (363, 550)\n",
            "Transformed image size:  (800, 1212)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=363, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=16, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 625)\n",
            "Transformed image size:  (1024, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=625, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (379, 550)\n",
            "Transformed image size:  (800, 1161)\n",
            "Proposal Boxes size: torch.Size([164, 4])\n",
            "Pooled features size: torch.Size([164, 2048])\n",
            "Instances(num_instances=36, image_height=379, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 506)\n",
            "Transformed image size:  (1265, 800)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=506, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (190, 825)\n",
            "Transformed image size:  (307, 1333)\n",
            "Proposal Boxes size: torch.Size([300, 4])\n",
            "Pooled features size: torch.Size([300, 2048])\n",
            "Instances(num_instances=36, image_height=190, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 530)\n",
            "Transformed image size:  (1206, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=530, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 585)\n",
            "Transformed image size:  (1094, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=585, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 680)\n",
            "Transformed image size:  (941, 800)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=680, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 304)\n",
            "Transformed image size:  (1053, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=304, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 629)\n",
            "Transformed image size:  (1017, 800)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=629, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 693)\n",
            "Transformed image size:  (924, 800)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=693, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 299)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=299, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 133)\n",
            "Transformed image size:  (1333, 443)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=133, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (531, 800)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=531, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (309, 550)\n",
            "Transformed image size:  (749, 1333)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=28, image_height=309, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 628)\n",
            "Transformed image size:  (1019, 800)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=628, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (546, 825)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=546, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (546, 825)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=546, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([148, 4])\n",
            "Pooled features size: torch.Size([148, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 659)\n",
            "Transformed image size:  (971, 800)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=659, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 292)\n",
            "Transformed image size:  (1096, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=292, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 272)\n",
            "Transformed image size:  (1176, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=272, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 379)\n",
            "Transformed image size:  (1333, 632)\n",
            "Proposal Boxes size: torch.Size([84, 4])\n",
            "Pooled features size: torch.Size([84, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=379, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=25, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([172, 4])\n",
            "Pooled features size: torch.Size([172, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 265)\n",
            "Transformed image size:  (1208, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=265, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=32, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 259)\n",
            "Transformed image size:  (1236, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=259, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=13, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 286)\n",
            "Transformed image size:  (1119, 800)\n",
            "Proposal Boxes size: torch.Size([153, 4])\n",
            "Pooled features size: torch.Size([153, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=286, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (571, 800)\n",
            "Transformed image size:  (800, 1121)\n",
            "Proposal Boxes size: torch.Size([151, 4])\n",
            "Pooled features size: torch.Size([151, 2048])\n",
            "Instances(num_instances=36, image_height=571, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 681)\n",
            "Transformed image size:  (940, 800)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=681, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 410)\n",
            "Transformed image size:  (1333, 683)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=410, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (557, 825)\n",
            "Transformed image size:  (800, 1185)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=557, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (786, 800)\n",
            "Transformed image size:  (800, 814)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=786, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (605, 825)\n",
            "Transformed image size:  (800, 1091)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=605, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (621, 800)\n",
            "Transformed image size:  (800, 1031)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=621, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (360, 550)\n",
            "Transformed image size:  (800, 1222)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=360, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (407, 825)\n",
            "Transformed image size:  (658, 1333)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=15, image_height=407, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (546, 825)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=546, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (684, 800)\n",
            "Transformed image size:  (800, 936)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=684, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 551)\n",
            "Transformed image size:  (1162, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=551, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=18, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 669)\n",
            "Transformed image size:  (957, 800)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=669, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=34, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (384, 550)\n",
            "Transformed image size:  (800, 1146)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=384, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 289)\n",
            "Transformed image size:  (1107, 800)\n",
            "Proposal Boxes size: torch.Size([83, 4])\n",
            "Pooled features size: torch.Size([83, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=289, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 525)\n",
            "Transformed image size:  (1219, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=525, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 337)\n",
            "Transformed image size:  (1333, 562)\n",
            "Proposal Boxes size: torch.Size([80, 4])\n",
            "Pooled features size: torch.Size([80, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=337, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (418, 825)\n",
            "Transformed image size:  (675, 1333)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=418, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (600, 800)\n",
            "Transformed image size:  (800, 1067)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=600, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 560)\n",
            "Transformed image size:  (1143, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=560, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (334, 550)\n",
            "Transformed image size:  (800, 1317)\n",
            "Proposal Boxes size: torch.Size([160, 4])\n",
            "Pooled features size: torch.Size([160, 2048])\n",
            "Instances(num_instances=36, image_height=334, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (387, 550)\n",
            "Transformed image size:  (800, 1137)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=36, image_height=387, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=14, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 305)\n",
            "Transformed image size:  (1049, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=305, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([159, 4])\n",
            "Pooled features size: torch.Size([159, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 781)\n",
            "Transformed image size:  (819, 800)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=781, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (423, 825)\n",
            "Transformed image size:  (683, 1333)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=423, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (345, 550)\n",
            "Transformed image size:  (800, 1275)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=35, image_height=345, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=24, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 240)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=240, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (726, 800)\n",
            "Transformed image size:  (800, 882)\n",
            "Proposal Boxes size: torch.Size([159, 4])\n",
            "Pooled features size: torch.Size([159, 2048])\n",
            "Instances(num_instances=36, image_height=726, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 539)\n",
            "Transformed image size:  (1187, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=539, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 314)\n",
            "Transformed image size:  (1019, 800)\n",
            "Proposal Boxes size: torch.Size([174, 4])\n",
            "Pooled features size: torch.Size([174, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=314, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([159, 4])\n",
            "Pooled features size: torch.Size([159, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (363, 550)\n",
            "Transformed image size:  (800, 1212)\n",
            "Proposal Boxes size: torch.Size([160, 4])\n",
            "Pooled features size: torch.Size([160, 2048])\n",
            "Instances(num_instances=36, image_height=363, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (609, 825)\n",
            "Transformed image size:  (800, 1084)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=609, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 534)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=534, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (586, 800)\n",
            "Transformed image size:  (800, 1092)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=35, image_height=586, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([168, 4])\n",
            "Pooled features size: torch.Size([168, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 348)\n",
            "Transformed image size:  (920, 800)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=348, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (359, 550)\n",
            "Transformed image size:  (800, 1226)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=359, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 525)\n",
            "Transformed image size:  (1218, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=525, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 504)\n",
            "Transformed image size:  (1270, 800)\n",
            "Proposal Boxes size: torch.Size([145, 4])\n",
            "Pooled features size: torch.Size([145, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=504, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=15, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (312, 550)\n",
            "Transformed image size:  (756, 1333)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=312, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([80, 4])\n",
            "Pooled features size: torch.Size([80, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (586, 825)\n",
            "Transformed image size:  (800, 1126)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=586, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 531)\n",
            "Transformed image size:  (1205, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=531, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 367)\n",
            "Transformed image size:  (1333, 612)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=29, image_height=800, image_width=367, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 512)\n",
            "Transformed image size:  (1250, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=512, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=30, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 323)\n",
            "Transformed image size:  (991, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=323, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (267, 550)\n",
            "Transformed image size:  (647, 1333)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=267, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=35, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (387, 550)\n",
            "Transformed image size:  (800, 1137)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=387, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (523, 825)\n",
            "Transformed image size:  (800, 1262)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=17, image_height=523, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (544, 825)\n",
            "Transformed image size:  (800, 1213)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=544, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 413)\n",
            "Transformed image size:  (1333, 688)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=413, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (429, 550)\n",
            "Transformed image size:  (800, 1026)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=429, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=31, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (353, 550)\n",
            "Transformed image size:  (800, 1246)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=353, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (384, 550)\n",
            "Transformed image size:  (800, 1146)\n",
            "Proposal Boxes size: torch.Size([158, 4])\n",
            "Pooled features size: torch.Size([158, 2048])\n",
            "Instances(num_instances=36, image_height=384, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([181, 4])\n",
            "Pooled features size: torch.Size([181, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (534, 800)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=534, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 304)\n",
            "Transformed image size:  (1053, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=304, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (524, 800)\n",
            "Transformed image size:  (800, 1221)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=524, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (544, 550)\n",
            "Transformed image size:  (800, 809)\n",
            "Proposal Boxes size: torch.Size([170, 4])\n",
            "Pooled features size: torch.Size([170, 2048])\n",
            "Instances(num_instances=36, image_height=544, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (363, 550)\n",
            "Transformed image size:  (800, 1212)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=363, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (518, 800)\n",
            "Transformed image size:  (800, 1236)\n",
            "Proposal Boxes size: torch.Size([194, 4])\n",
            "Pooled features size: torch.Size([194, 2048])\n",
            "Instances(num_instances=36, image_height=518, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([181, 4])\n",
            "Pooled features size: torch.Size([181, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 600)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=600, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 489)\n",
            "Transformed image size:  (1309, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=489, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (420, 550)\n",
            "Transformed image size:  (800, 1048)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=420, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (479, 550)\n",
            "Transformed image size:  (800, 919)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=479, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (374, 550)\n",
            "Transformed image size:  (800, 1176)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=374, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 586)\n",
            "Transformed image size:  (1091, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=586, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (532, 800)\n",
            "Transformed image size:  (800, 1203)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=532, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=33, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 328)\n",
            "Transformed image size:  (976, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=27, image_height=400, image_width=328, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 784)\n",
            "Transformed image size:  (816, 800)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=784, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (438, 550)\n",
            "Transformed image size:  (800, 1005)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=438, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (540, 800)\n",
            "Transformed image size:  (800, 1185)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=540, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 598)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=598, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([174, 4])\n",
            "Pooled features size: torch.Size([174, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=31, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (510, 550)\n",
            "Transformed image size:  (800, 863)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=510, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (336, 550)\n",
            "Transformed image size:  (800, 1310)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=17, image_height=336, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 730)\n",
            "Transformed image size:  (877, 800)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=730, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 608)\n",
            "Transformed image size:  (1053, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=608, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 299)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=299, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (791, 800)\n",
            "Transformed image size:  (800, 809)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=791, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (544, 550)\n",
            "Transformed image size:  (800, 809)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=544, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 375)\n",
            "Transformed image size:  (1333, 625)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=28, image_height=800, image_width=375, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=34, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (460, 550)\n",
            "Transformed image size:  (800, 957)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=460, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (378, 550)\n",
            "Transformed image size:  (800, 1164)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=378, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 505)\n",
            "Transformed image size:  (1267, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=505, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 361)\n",
            "Transformed image size:  (886, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=361, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 628)\n",
            "Transformed image size:  (1019, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=29, image_height=800, image_width=628, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=25, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (499, 550)\n",
            "Transformed image size:  (800, 882)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=499, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (573, 825)\n",
            "Transformed image size:  (800, 1152)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=573, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 530)\n",
            "Transformed image size:  (1206, 800)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=530, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (586, 825)\n",
            "Transformed image size:  (800, 1126)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=36, image_height=586, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 598)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([80, 4])\n",
            "Pooled features size: torch.Size([80, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=598, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (423, 550)\n",
            "Transformed image size:  (800, 1040)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=423, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 548)\n",
            "Transformed image size:  (1168, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=548, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 671)\n",
            "Transformed image size:  (953, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=671, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (411, 550)\n",
            "Transformed image size:  (800, 1071)\n",
            "Proposal Boxes size: torch.Size([191, 4])\n",
            "Pooled features size: torch.Size([191, 2048])\n",
            "Instances(num_instances=36, image_height=411, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 327)\n",
            "Transformed image size:  (979, 800)\n",
            "Proposal Boxes size: torch.Size([95, 4])\n",
            "Pooled features size: torch.Size([95, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=327, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (552, 825)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=552, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (565, 800)\n",
            "Transformed image size:  (800, 1133)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=565, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 293)\n",
            "Transformed image size:  (1092, 800)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=293, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (592, 825)\n",
            "Transformed image size:  (800, 1115)\n",
            "Proposal Boxes size: torch.Size([163, 4])\n",
            "Pooled features size: torch.Size([163, 2048])\n",
            "Instances(num_instances=36, image_height=592, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 647)\n",
            "Transformed image size:  (989, 800)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=647, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([158, 4])\n",
            "Pooled features size: torch.Size([158, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (560, 825)\n",
            "Transformed image size:  (800, 1179)\n",
            "Proposal Boxes size: torch.Size([175, 4])\n",
            "Pooled features size: torch.Size([175, 2048])\n",
            "Instances(num_instances=36, image_height=560, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=35, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (489, 825)\n",
            "Transformed image size:  (790, 1333)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=7, image_height=489, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 532)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=9, image_height=800, image_width=532, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (615, 825)\n",
            "Transformed image size:  (800, 1073)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=615, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 628)\n",
            "Transformed image size:  (1019, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=628, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 506)\n",
            "Transformed image size:  (1265, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=506, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=18, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 277)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([83, 4])\n",
            "Pooled features size: torch.Size([83, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=277, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (529, 800)\n",
            "Transformed image size:  (800, 1210)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=529, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (397, 550)\n",
            "Transformed image size:  (800, 1108)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=397, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 531)\n",
            "Transformed image size:  (1204, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=531, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 312)\n",
            "Transformed image size:  (1333, 520)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=312, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 682)\n",
            "Transformed image size:  (938, 800)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=682, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (583, 825)\n",
            "Transformed image size:  (800, 1132)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=583, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 603)\n",
            "Transformed image size:  (1061, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=603, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (359, 550)\n",
            "Transformed image size:  (800, 1226)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=359, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (352, 550)\n",
            "Transformed image size:  (800, 1250)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=352, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=30, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=34, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (377, 550)\n",
            "Transformed image size:  (800, 1167)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=377, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 593)\n",
            "Transformed image size:  (1079, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=593, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 632)\n",
            "Transformed image size:  (1013, 800)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=632, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (531, 800)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=531, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 527)\n",
            "Transformed image size:  (1214, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=527, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (373, 550)\n",
            "Transformed image size:  (800, 1180)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=373, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 454)\n",
            "Transformed image size:  (1333, 756)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=454, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (380, 550)\n",
            "Transformed image size:  (800, 1158)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=380, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 687)\n",
            "Transformed image size:  (932, 800)\n",
            "Proposal Boxes size: torch.Size([155, 4])\n",
            "Pooled features size: torch.Size([155, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=687, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 567)\n",
            "Transformed image size:  (1129, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=567, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 470)\n",
            "Transformed image size:  (1333, 783)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=470, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (422, 550)\n",
            "Transformed image size:  (800, 1043)\n",
            "Proposal Boxes size: torch.Size([166, 4])\n",
            "Pooled features size: torch.Size([166, 2048])\n",
            "Instances(num_instances=36, image_height=422, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 719)\n",
            "Transformed image size:  (890, 800)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=719, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (620, 800)\n",
            "Transformed image size:  (800, 1032)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=620, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (360, 550)\n",
            "Transformed image size:  (800, 1222)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=360, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (384, 550)\n",
            "Transformed image size:  (800, 1146)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=384, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (388, 550)\n",
            "Transformed image size:  (800, 1134)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=388, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([83, 4])\n",
            "Pooled features size: torch.Size([83, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([157, 4])\n",
            "Pooled features size: torch.Size([157, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 506)\n",
            "Transformed image size:  (1265, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=506, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (728, 825)\n",
            "Transformed image size:  (800, 907)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=728, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 735)\n",
            "Transformed image size:  (871, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=735, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 532)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=532, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=18, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=30, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 269)\n",
            "Transformed image size:  (1190, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=269, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 268)\n",
            "Transformed image size:  (1194, 800)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=23, image_height=400, image_width=268, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 300)\n",
            "Transformed image size:  (1333, 500)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 671)\n",
            "Transformed image size:  (953, 800)\n",
            "Proposal Boxes size: torch.Size([178, 4])\n",
            "Pooled features size: torch.Size([178, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=671, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (508, 800)\n",
            "Transformed image size:  (800, 1260)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=508, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 508)\n",
            "Transformed image size:  (1260, 800)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=508, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (460, 550)\n",
            "Transformed image size:  (800, 957)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=460, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (628, 800)\n",
            "Transformed image size:  (800, 1019)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=628, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (442, 550)\n",
            "Transformed image size:  (800, 995)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=34, image_height=442, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (395, 550)\n",
            "Transformed image size:  (800, 1114)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=395, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 286)\n",
            "Transformed image size:  (1119, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=286, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (383, 550)\n",
            "Transformed image size:  (800, 1149)\n",
            "Proposal Boxes size: torch.Size([176, 4])\n",
            "Pooled features size: torch.Size([176, 2048])\n",
            "Instances(num_instances=36, image_height=383, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (631, 825)\n",
            "Transformed image size:  (800, 1046)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=631, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (378, 550)\n",
            "Transformed image size:  (800, 1164)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=378, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 600)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=600, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (439, 550)\n",
            "Transformed image size:  (800, 1002)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=439, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (464, 825)\n",
            "Transformed image size:  (750, 1333)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=464, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (368, 550)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([149, 4])\n",
            "Pooled features size: torch.Size([149, 2048])\n",
            "Instances(num_instances=36, image_height=368, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (449, 550)\n",
            "Transformed image size:  (800, 980)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=449, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 600)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=600, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 531)\n",
            "Transformed image size:  (1204, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=531, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 240)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=240, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 295)\n",
            "Transformed image size:  (1082, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=295, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 536)\n",
            "Transformed image size:  (1194, 800)\n",
            "Proposal Boxes size: torch.Size([84, 4])\n",
            "Pooled features size: torch.Size([84, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=536, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 780)\n",
            "Transformed image size:  (821, 800)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=780, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=28, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (383, 550)\n",
            "Transformed image size:  (800, 1149)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=23, image_height=383, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=23, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 323)\n",
            "Transformed image size:  (991, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=323, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([146, 4])\n",
            "Pooled features size: torch.Size([146, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 677)\n",
            "Transformed image size:  (945, 800)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=677, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 591)\n",
            "Transformed image size:  (1083, 800)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=591, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 240)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=240, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([86, 4])\n",
            "Pooled features size: torch.Size([86, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (371, 550)\n",
            "Transformed image size:  (800, 1186)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=371, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([149, 4])\n",
            "Pooled features size: torch.Size([149, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (607, 800)\n",
            "Transformed image size:  (800, 1054)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=607, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 455)\n",
            "Transformed image size:  (1333, 758)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=455, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (607, 825)\n",
            "Transformed image size:  (800, 1087)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=607, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (355, 550)\n",
            "Transformed image size:  (800, 1239)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=355, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (281, 550)\n",
            "Transformed image size:  (681, 1333)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=281, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 234)\n",
            "Transformed image size:  (1333, 780)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=234, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 717)\n",
            "Transformed image size:  (893, 800)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=717, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (420, 550)\n",
            "Transformed image size:  (800, 1048)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=420, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 277)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=277, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 604)\n",
            "Transformed image size:  (1060, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=7, image_height=800, image_width=604, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 428)\n",
            "Transformed image size:  (1333, 713)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=428, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 240)\n",
            "Transformed image size:  (1333, 800)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=240, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (535, 800)\n",
            "Transformed image size:  (800, 1196)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=535, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 424)\n",
            "Transformed image size:  (1333, 706)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=424, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (359, 550)\n",
            "Transformed image size:  (800, 1226)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=359, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (384, 550)\n",
            "Transformed image size:  (800, 1146)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=384, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 540)\n",
            "Transformed image size:  (1185, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=540, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 266)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=266, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (344, 550)\n",
            "Transformed image size:  (800, 1279)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=344, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (435, 550)\n",
            "Transformed image size:  (800, 1011)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=435, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (515, 825)\n",
            "Transformed image size:  (800, 1282)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=515, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 403)\n",
            "Transformed image size:  (1333, 671)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=403, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 643)\n",
            "Transformed image size:  (995, 800)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=643, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 739)\n",
            "Transformed image size:  (866, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=22, image_height=800, image_width=739, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 825)\n",
            "Transformed image size:  (800, 1238)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (537, 800)\n",
            "Transformed image size:  (800, 1192)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=537, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (429, 550)\n",
            "Transformed image size:  (800, 1026)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=429, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 574)\n",
            "Transformed image size:  (1115, 800)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=574, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (492, 800)\n",
            "Transformed image size:  (800, 1301)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=492, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([157, 4])\n",
            "Pooled features size: torch.Size([157, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 550)\n",
            "Transformed image size:  (800, 1103)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 392)\n",
            "Transformed image size:  (816, 800)\n",
            "Proposal Boxes size: torch.Size([89, 4])\n",
            "Pooled features size: torch.Size([89, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=392, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (539, 825)\n",
            "Transformed image size:  (800, 1224)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=539, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (640, 800)\n",
            "Transformed image size:  (800, 1000)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=640, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 532)\n",
            "Transformed image size:  (1203, 800)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=532, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([139, 4])\n",
            "Pooled features size: torch.Size([139, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([151, 4])\n",
            "Pooled features size: torch.Size([151, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([135, 4])\n",
            "Pooled features size: torch.Size([135, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([148, 4])\n",
            "Pooled features size: torch.Size([148, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 171)\n",
            "Transformed image size:  (1333, 570)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=171, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=3, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=23, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (336, 550)\n",
            "Transformed image size:  (800, 1310)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=336, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 550)\n",
            "Transformed image size:  (1164, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 276)\n",
            "Transformed image size:  (1159, 800)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=276, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (578, 800)\n",
            "Transformed image size:  (800, 1107)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=578, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 351)\n",
            "Transformed image size:  (1333, 585)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=351, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([83, 4])\n",
            "Pooled features size: torch.Size([83, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 310)\n",
            "Transformed image size:  (1032, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=21, image_height=400, image_width=310, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (387, 550)\n",
            "Transformed image size:  (800, 1137)\n",
            "Proposal Boxes size: torch.Size([152, 4])\n",
            "Pooled features size: torch.Size([152, 2048])\n",
            "Instances(num_instances=36, image_height=387, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 521)\n",
            "Transformed image size:  (1228, 800)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=521, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (392, 550)\n",
            "Transformed image size:  (800, 1122)\n",
            "Proposal Boxes size: torch.Size([142, 4])\n",
            "Pooled features size: torch.Size([142, 2048])\n",
            "Instances(num_instances=36, image_height=392, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 706)\n",
            "Transformed image size:  (907, 800)\n",
            "Proposal Boxes size: torch.Size([83, 4])\n",
            "Pooled features size: torch.Size([83, 2048])\n",
            "Instances(num_instances=26, image_height=800, image_width=706, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 285)\n",
            "Transformed image size:  (1123, 800)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=32, image_height=400, image_width=285, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (792, 800)\n",
            "Transformed image size:  (800, 808)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=792, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 287)\n",
            "Transformed image size:  (1115, 800)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=287, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (532, 800)\n",
            "Transformed image size:  (800, 1203)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=532, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 328)\n",
            "Transformed image size:  (976, 800)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=328, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 451)\n",
            "Transformed image size:  (1333, 751)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=451, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 825)\n",
            "Transformed image size:  (800, 1238)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (377, 550)\n",
            "Transformed image size:  (800, 1167)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=377, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (565, 825)\n",
            "Transformed image size:  (800, 1168)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=565, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 477)\n",
            "Transformed image size:  (1333, 795)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=477, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 547)\n",
            "Transformed image size:  (1170, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=547, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 549)\n",
            "Transformed image size:  (1166, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=549, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=33, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 299)\n",
            "Transformed image size:  (1070, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=299, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (565, 800)\n",
            "Transformed image size:  (800, 1133)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=565, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (362, 550)\n",
            "Transformed image size:  (800, 1215)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=362, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 601)\n",
            "Transformed image size:  (1065, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=601, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 784)\n",
            "Transformed image size:  (816, 800)\n",
            "Proposal Boxes size: torch.Size([149, 4])\n",
            "Pooled features size: torch.Size([149, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=784, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (458, 550)\n",
            "Transformed image size:  (800, 961)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=458, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (399, 550)\n",
            "Transformed image size:  (800, 1103)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=399, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (601, 825)\n",
            "Transformed image size:  (800, 1098)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=601, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (528, 825)\n",
            "Transformed image size:  (800, 1250)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=528, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (365, 550)\n",
            "Transformed image size:  (800, 1205)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=365, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 531)\n",
            "Transformed image size:  (1205, 800)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=531, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=35, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 320)\n",
            "Transformed image size:  (1000, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=320, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (427, 550)\n",
            "Transformed image size:  (800, 1030)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=427, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (382, 550)\n",
            "Transformed image size:  (800, 1152)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=382, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 351)\n",
            "Transformed image size:  (912, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=351, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([148, 4])\n",
            "Pooled features size: torch.Size([148, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([158, 4])\n",
            "Pooled features size: torch.Size([158, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([137, 4])\n",
            "Pooled features size: torch.Size([137, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 537)\n",
            "Transformed image size:  (1192, 800)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=537, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 601)\n",
            "Transformed image size:  (1065, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=601, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (469, 550)\n",
            "Transformed image size:  (800, 938)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=13, image_height=469, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 800)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([134, 4])\n",
            "Pooled features size: torch.Size([134, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 459)\n",
            "Transformed image size:  (1333, 765)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=459, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (394, 550)\n",
            "Transformed image size:  (800, 1117)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=394, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (526, 825)\n",
            "Transformed image size:  (800, 1255)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=526, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (497, 825)\n",
            "Transformed image size:  (800, 1328)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=497, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([157, 4])\n",
            "Pooled features size: torch.Size([157, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (313, 550)\n",
            "Transformed image size:  (759, 1333)\n",
            "Proposal Boxes size: torch.Size([130, 4])\n",
            "Pooled features size: torch.Size([130, 2048])\n",
            "Instances(num_instances=36, image_height=313, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 595)\n",
            "Transformed image size:  (1076, 800)\n",
            "Proposal Boxes size: torch.Size([88, 4])\n",
            "Pooled features size: torch.Size([88, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=595, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 525)\n",
            "Transformed image size:  (1219, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=525, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 300)\n",
            "Transformed image size:  (1067, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=300, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([156, 4])\n",
            "Pooled features size: torch.Size([156, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 298)\n",
            "Transformed image size:  (1333, 497)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=298, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (542, 800)\n",
            "Transformed image size:  (800, 1181)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=542, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 265)\n",
            "Transformed image size:  (1208, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=265, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([113, 4])\n",
            "Pooled features size: torch.Size([113, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 561)\n",
            "Transformed image size:  (1141, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=561, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (380, 550)\n",
            "Transformed image size:  (800, 1158)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=380, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (372, 550)\n",
            "Transformed image size:  (800, 1183)\n",
            "Proposal Boxes size: torch.Size([140, 4])\n",
            "Pooled features size: torch.Size([140, 2048])\n",
            "Instances(num_instances=36, image_height=372, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (392, 550)\n",
            "Transformed image size:  (800, 1122)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=392, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (667, 825)\n",
            "Transformed image size:  (800, 990)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=667, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([115, 4])\n",
            "Pooled features size: torch.Size([115, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 250)\n",
            "Transformed image size:  (1280, 800)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=250, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([133, 4])\n",
            "Pooled features size: torch.Size([133, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (532, 800)\n",
            "Transformed image size:  (800, 1203)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=532, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 512)\n",
            "Transformed image size:  (1250, 800)\n",
            "Proposal Boxes size: torch.Size([81, 4])\n",
            "Pooled features size: torch.Size([81, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=512, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=24, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (547, 825)\n",
            "Transformed image size:  (800, 1207)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=547, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 523)\n",
            "Transformed image size:  (1224, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=523, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 472)\n",
            "Transformed image size:  (1333, 786)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=472, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 540)\n",
            "Transformed image size:  (1185, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=540, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (539, 825)\n",
            "Transformed image size:  (800, 1224)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=539, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (349, 550)\n",
            "Transformed image size:  (800, 1261)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=36, image_height=349, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (543, 800)\n",
            "Transformed image size:  (800, 1179)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=543, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (364, 550)\n",
            "Transformed image size:  (800, 1209)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=364, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (557, 800)\n",
            "Transformed image size:  (800, 1149)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=557, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=24, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([98, 4])\n",
            "Pooled features size: torch.Size([98, 2048])\n",
            "Instances(num_instances=30, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 306)\n",
            "Transformed image size:  (1046, 800)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=306, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 593)\n",
            "Transformed image size:  (1079, 800)\n",
            "Proposal Boxes size: torch.Size([92, 4])\n",
            "Pooled features size: torch.Size([92, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=593, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([174, 4])\n",
            "Pooled features size: torch.Size([174, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([152, 4])\n",
            "Pooled features size: torch.Size([152, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (563, 825)\n",
            "Transformed image size:  (800, 1172)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=563, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 774)\n",
            "Transformed image size:  (827, 800)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=774, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([114, 4])\n",
            "Pooled features size: torch.Size([114, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (634, 825)\n",
            "Transformed image size:  (800, 1041)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=634, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 348)\n",
            "Transformed image size:  (920, 800)\n",
            "Proposal Boxes size: torch.Size([100, 4])\n",
            "Pooled features size: torch.Size([100, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=348, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 534)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=534, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (592, 825)\n",
            "Transformed image size:  (800, 1115)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=592, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 289)\n",
            "Transformed image size:  (1107, 800)\n",
            "Proposal Boxes size: torch.Size([87, 4])\n",
            "Pooled features size: torch.Size([87, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=289, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (546, 800)\n",
            "Transformed image size:  (800, 1172)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=546, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([119, 4])\n",
            "Pooled features size: torch.Size([119, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (602, 825)\n",
            "Transformed image size:  (800, 1096)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=602, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=25, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (543, 825)\n",
            "Transformed image size:  (800, 1215)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=543, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (524, 800)\n",
            "Transformed image size:  (800, 1221)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=524, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([159, 4])\n",
            "Pooled features size: torch.Size([159, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 541)\n",
            "Transformed image size:  (1183, 800)\n",
            "Proposal Boxes size: torch.Size([85, 4])\n",
            "Pooled features size: torch.Size([85, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=541, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 712)\n",
            "Transformed image size:  (899, 800)\n",
            "Proposal Boxes size: torch.Size([124, 4])\n",
            "Pooled features size: torch.Size([124, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=712, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 503)\n",
            "Transformed image size:  (1272, 800)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=503, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (705, 825)\n",
            "Transformed image size:  (800, 936)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=705, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (362, 550)\n",
            "Transformed image size:  (800, 1215)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=362, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 320)\n",
            "Transformed image size:  (1000, 800)\n",
            "Proposal Boxes size: torch.Size([101, 4])\n",
            "Pooled features size: torch.Size([101, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=320, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 584)\n",
            "Transformed image size:  (1096, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=584, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([104, 4])\n",
            "Pooled features size: torch.Size([104, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (350, 550)\n",
            "Transformed image size:  (800, 1257)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=350, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (363, 550)\n",
            "Transformed image size:  (800, 1212)\n",
            "Proposal Boxes size: torch.Size([129, 4])\n",
            "Pooled features size: torch.Size([129, 2048])\n",
            "Instances(num_instances=31, image_height=363, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([143, 4])\n",
            "Pooled features size: torch.Size([143, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (423, 550)\n",
            "Transformed image size:  (800, 1040)\n",
            "Proposal Boxes size: torch.Size([96, 4])\n",
            "Pooled features size: torch.Size([96, 2048])\n",
            "Instances(num_instances=36, image_height=423, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (618, 825)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=618, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (591, 800)\n",
            "Transformed image size:  (800, 1083)\n",
            "Proposal Boxes size: torch.Size([109, 4])\n",
            "Pooled features size: torch.Size([109, 2048])\n",
            "Instances(num_instances=36, image_height=591, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([152, 4])\n",
            "Pooled features size: torch.Size([152, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([108, 4])\n",
            "Pooled features size: torch.Size([108, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (416, 550)\n",
            "Transformed image size:  (800, 1058)\n",
            "Proposal Boxes size: torch.Size([106, 4])\n",
            "Pooled features size: torch.Size([106, 2048])\n",
            "Instances(num_instances=36, image_height=416, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (427, 550)\n",
            "Transformed image size:  (800, 1030)\n",
            "Proposal Boxes size: torch.Size([126, 4])\n",
            "Pooled features size: torch.Size([126, 2048])\n",
            "Instances(num_instances=36, image_height=427, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([127, 4])\n",
            "Pooled features size: torch.Size([127, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([118, 4])\n",
            "Pooled features size: torch.Size([118, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([144, 4])\n",
            "Pooled features size: torch.Size([144, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 525)\n",
            "Transformed image size:  (1219, 800)\n",
            "Proposal Boxes size: torch.Size([93, 4])\n",
            "Pooled features size: torch.Size([93, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=525, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([125, 4])\n",
            "Pooled features size: torch.Size([125, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (520, 800)\n",
            "Transformed image size:  (800, 1231)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=520, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([132, 4])\n",
            "Pooled features size: torch.Size([132, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([111, 4])\n",
            "Pooled features size: torch.Size([111, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (412, 550)\n",
            "Transformed image size:  (800, 1068)\n",
            "Proposal Boxes size: torch.Size([131, 4])\n",
            "Pooled features size: torch.Size([131, 2048])\n",
            "Instances(num_instances=36, image_height=412, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([123, 4])\n",
            "Pooled features size: torch.Size([123, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (644, 800)\n",
            "Transformed image size:  (800, 994)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=644, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([136, 4])\n",
            "Pooled features size: torch.Size([136, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 400)\n",
            "Transformed image size:  (800, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=400, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (369, 550)\n",
            "Transformed image size:  (800, 1192)\n",
            "Proposal Boxes size: torch.Size([121, 4])\n",
            "Pooled features size: torch.Size([121, 2048])\n",
            "Instances(num_instances=36, image_height=369, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([102, 4])\n",
            "Pooled features size: torch.Size([102, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 566)\n",
            "Transformed image size:  (1131, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=566, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (534, 800)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([110, 4])\n",
            "Pooled features size: torch.Size([110, 2048])\n",
            "Instances(num_instances=36, image_height=534, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([122, 4])\n",
            "Pooled features size: torch.Size([122, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([112, 4])\n",
            "Pooled features size: torch.Size([112, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 641)\n",
            "Transformed image size:  (998, 800)\n",
            "Proposal Boxes size: torch.Size([99, 4])\n",
            "Pooled features size: torch.Size([99, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=641, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (524, 800)\n",
            "Transformed image size:  (800, 1221)\n",
            "Proposal Boxes size: torch.Size([147, 4])\n",
            "Pooled features size: torch.Size([147, 2048])\n",
            "Instances(num_instances=36, image_height=524, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=25, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([116, 4])\n",
            "Pooled features size: torch.Size([116, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 693)\n",
            "Transformed image size:  (924, 800)\n",
            "Proposal Boxes size: torch.Size([103, 4])\n",
            "Pooled features size: torch.Size([103, 2048])\n",
            "Instances(num_instances=35, image_height=800, image_width=693, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([97, 4])\n",
            "Pooled features size: torch.Size([97, 2048])\n",
            "Instances(num_instances=36, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 533)\n",
            "Transformed image size:  (1201, 800)\n",
            "Proposal Boxes size: torch.Size([94, 4])\n",
            "Pooled features size: torch.Size([94, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=533, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([107, 4])\n",
            "Pooled features size: torch.Size([107, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (549, 825)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([149, 4])\n",
            "Pooled features size: torch.Size([149, 2048])\n",
            "Instances(num_instances=36, image_height=549, image_width=825, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (367, 550)\n",
            "Transformed image size:  (800, 1199)\n",
            "Proposal Boxes size: torch.Size([150, 4])\n",
            "Pooled features size: torch.Size([150, 2048])\n",
            "Instances(num_instances=36, image_height=367, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (800, 592)\n",
            "Transformed image size:  (1081, 800)\n",
            "Proposal Boxes size: torch.Size([105, 4])\n",
            "Pooled features size: torch.Size([105, 2048])\n",
            "Instances(num_instances=36, image_height=800, image_width=592, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (533, 800)\n",
            "Transformed image size:  (800, 1201)\n",
            "Proposal Boxes size: torch.Size([141, 4])\n",
            "Pooled features size: torch.Size([141, 2048])\n",
            "Instances(num_instances=36, image_height=533, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([128, 4])\n",
            "Pooled features size: torch.Size([128, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (400, 267)\n",
            "Transformed image size:  (1199, 800)\n",
            "Proposal Boxes size: torch.Size([90, 4])\n",
            "Pooled features size: torch.Size([90, 2048])\n",
            "Instances(num_instances=15, image_height=400, image_width=267, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (366, 550)\n",
            "Transformed image size:  (800, 1202)\n",
            "Proposal Boxes size: torch.Size([138, 4])\n",
            "Pooled features size: torch.Size([138, 2048])\n",
            "Instances(num_instances=36, image_height=366, image_width=550, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (529, 800)\n",
            "Transformed image size:  (800, 1210)\n",
            "Proposal Boxes size: torch.Size([148, 4])\n",
            "Pooled features size: torch.Size([148, 2048])\n",
            "Instances(num_instances=36, image_height=529, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (799, 555)\n",
            "Transformed image size:  (1152, 800)\n",
            "Proposal Boxes size: torch.Size([91, 4])\n",
            "Pooled features size: torch.Size([91, 2048])\n",
            "Instances(num_instances=36, image_height=799, image_width=555, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (539, 800)\n",
            "Transformed image size:  (800, 1187)\n",
            "Proposal Boxes size: torch.Size([117, 4])\n",
            "Pooled features size: torch.Size([117, 2048])\n",
            "Instances(num_instances=36, image_height=539, image_width=800, fields=[pred_boxes, scores, pred_classes])\n",
            "Original image size:  (581, 825)\n",
            "Transformed image size:  (800, 1136)\n",
            "Proposal Boxes size: torch.Size([120, 4])\n",
            "Pooled features size: torch.Size([120, 2048])\n",
            "Instances(num_instances=36, image_height=581, image_width=825, fields=[pred_boxes, scores, pred_classes])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurhvYGVm9IZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc6dc86c-068c-4a69-fe33-95e6217b043e"
      },
      "source": [
        "readdict = {}\n",
        "\n",
        "readdict = torch.load(\"test1.pt\");\n",
        "\n",
        "print(readdict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'COCO_train2014_000000131486.jpg': {'pred_boxes': Boxes(tensor([[3.7709e+01, 0.0000e+00, 4.0700e+02, 1.1858e+02],\n",
            "        [0.0000e+00, 8.1039e-01, 2.8286e+02, 1.3554e+02],\n",
            "        [1.2342e+02, 6.4478e-01, 4.0700e+02, 1.3375e+02],\n",
            "        [1.2958e+02, 1.8197e+02, 2.6400e+02, 3.4330e+02],\n",
            "        [0.0000e+00, 2.0749e+01, 4.0671e+02, 1.6286e+02],\n",
            "        [1.4625e+02, 3.2517e+02, 2.3760e+02, 4.8162e+02],\n",
            "        [1.3683e+02, 1.9407e+02, 2.4867e+02, 3.7632e+02],\n",
            "        [6.9868e-01, 1.9129e+02, 4.0700e+02, 4.9770e+02],\n",
            "        [7.2326e-01, 3.4473e+02, 1.7396e+02, 6.4000e+02],\n",
            "        [1.2437e+02, 4.0023e+02, 2.4735e+02, 5.2113e+02],\n",
            "        [3.6786e+01, 1.6200e+02, 4.0700e+02, 4.4529e+02],\n",
            "        [1.8843e+00, 2.5082e+02, 4.0563e+02, 5.5542e+02],\n",
            "        [9.8890e-01, 4.2138e+02, 3.3298e+02, 6.4000e+02],\n",
            "        [1.7027e+00, 4.4332e+02, 2.5546e+02, 6.4000e+02],\n",
            "        [1.9254e-01, 2.0836e+02, 2.7285e+02, 5.3406e+02],\n",
            "        [1.5955e+00, 4.7656e+02, 4.0368e+02, 6.4000e+02],\n",
            "        [9.4407e+00, 3.8482e+02, 4.0499e+02, 6.3964e+02],\n",
            "        [0.0000e+00, 3.2353e+02, 2.4802e+02, 6.4000e+02],\n",
            "        [0.0000e+00, 2.3977e+01, 2.7688e+02, 1.7266e+02],\n",
            "        [1.5406e+02, 2.0376e+02, 4.0700e+02, 5.1264e+02],\n",
            "        [2.5293e+02, 2.4280e+02, 4.0700e+02, 6.1570e+02],\n",
            "        [1.1225e+02, 2.0779e+02, 2.8174e+02, 5.1344e+02],\n",
            "        [4.6159e+01, 2.9857e+02, 4.0659e+02, 6.0797e+02],\n",
            "        [1.3841e+02, 2.7434e+02, 4.0700e+02, 5.8555e+02],\n",
            "        [1.8684e+00, 2.5919e+02, 1.7308e+02, 6.0133e+02],\n",
            "        [7.1268e-01, 2.6469e+02, 2.4963e+02, 5.8497e+02],\n",
            "        [1.0381e+02, 1.0891e+02, 2.6968e+02, 5.3447e+02],\n",
            "        [1.0117e+00, 2.9587e+02, 3.3030e+02, 5.9548e+02],\n",
            "        [9.3360e+01, 1.6564e+02, 3.7796e+02, 4.8384e+02],\n",
            "        [1.1514e+02, 3.3920e+02, 3.7062e+02, 5.9748e+02],\n",
            "        [1.2775e+02, 8.6742e+01, 2.8673e+02, 5.0617e+02],\n",
            "        [0.0000e+00, 1.5609e+02, 2.5775e+02, 4.7279e+02],\n",
            "        [1.0692e+00, 4.9558e+02, 3.1167e+02, 6.4000e+02],\n",
            "        [2.1210e+02, 2.1102e+02, 4.0651e+02, 5.5408e+02],\n",
            "        [9.1902e+01, 1.0953e+02, 2.9591e+02, 3.8512e+02],\n",
            "        [0.0000e+00, 8.9320e+01, 4.0539e+02, 2.0602e+02]], device='cuda:0')), 'scores': tensor([0.9065, 0.8803, 0.7937, 0.7806, 0.7774, 0.7578, 0.6317, 0.5855, 0.5498,\n",
            "        0.5486, 0.5437, 0.5435, 0.4816, 0.4781, 0.4763, 0.4755, 0.4596, 0.4496,\n",
            "        0.4464, 0.4453, 0.4385, 0.4336, 0.4330, 0.4116, 0.4062, 0.3795, 0.3783,\n",
            "        0.3654, 0.3631, 0.3474, 0.3470, 0.3418, 0.3411, 0.3386, 0.3267, 0.3212],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 72,  72,  72, 181,  72,  98, 181, 176, 176, 608, 176, 176, 176, 176,\n",
            "        176, 176, 176, 176,  72, 176, 176,  50, 176, 176, 176, 176,  50, 176,\n",
            "        176, 461,  50, 176, 176, 176,  90, 456], device='cuda:0'), 'features': tensor([[0.6833, 0.0072, 0.0177,  ..., 0.0000, 0.1154, 0.0172],\n",
            "        [0.5030, 0.0023, 0.1878,  ..., 0.2008, 0.2486, 0.0044],\n",
            "        [0.3340, 0.1356, 0.1608,  ..., 0.1942, 0.1375, 0.0807],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0306,  ..., 0.0211, 1.1735, 0.2742],\n",
            "        [0.3033, 0.0000, 0.0191,  ..., 0.3275, 1.3679, 0.3703],\n",
            "        [0.0014, 0.0000, 0.1192,  ..., 0.0244, 0.1602, 0.0943]],\n",
            "       device='cuda:0')}, 'COCO_train2014_000000494628.jpg': {'pred_boxes': Boxes(tensor([[384.6557, 328.8521, 431.0185, 376.4070],\n",
            "        [  0.0000,   2.3318, 321.3125, 206.5483],\n",
            "        [  6.9154, 143.7872, 230.5046, 237.0952],\n",
            "        [  1.7479,   0.7514, 585.9396, 133.5891],\n",
            "        [239.5969,  56.9438, 636.1035, 293.7935],\n",
            "        [ 70.8362,   3.6981, 496.7808, 238.2474],\n",
            "        [226.6648,  99.2405, 382.6074, 253.1111],\n",
            "        [371.8372, 324.8034, 474.7091, 384.7169],\n",
            "        [121.7176, 290.4816, 560.5367, 362.7152],\n",
            "        [183.7734,   2.4318, 640.0000, 202.2344],\n",
            "        [ 15.5544, 165.1414, 220.8029, 283.0190],\n",
            "        [413.4635, 330.0062, 495.8044, 393.1937],\n",
            "        [216.8797,  97.9447, 441.7122, 251.4934],\n",
            "        [ 77.1969, 256.0784, 628.1779, 475.9114],\n",
            "        [593.0902,  38.5617, 639.4937, 478.3458],\n",
            "        [218.2263, 171.6023, 484.0539, 299.9323],\n",
            "        [  1.6735,  12.5725, 247.2961, 265.8048],\n",
            "        [107.9550, 369.4153, 640.0000, 510.6147],\n",
            "        [372.8939, 324.1372, 472.4388, 385.0879],\n",
            "        [  0.0000, 227.6123, 326.2656, 506.1048],\n",
            "        [421.6025, 345.5222, 488.5877, 384.3241],\n",
            "        [ 20.9784, 317.8804, 255.1662, 468.3415],\n",
            "        [148.7918, 307.0497, 618.0209, 398.9680],\n",
            "        [ 24.2055, 356.6335, 298.7062, 483.3888],\n",
            "        [ 44.4626, 413.8062, 453.8847, 498.0674],\n",
            "        [  0.0000, 360.7909, 441.3835, 510.6920],\n",
            "        [243.5913, 241.3791, 576.0692, 320.5194],\n",
            "        [501.2342,  11.1430, 636.2507, 360.9852],\n",
            "        [178.8121,   6.2831, 572.5195, 236.0604],\n",
            "        [  0.0000, 268.3647, 375.9190, 357.0657],\n",
            "        [ 22.9788, 189.1794, 236.3300, 290.1331],\n",
            "        [  3.9440,  86.9963, 326.2064, 325.1285],\n",
            "        [422.5067, 345.1446, 487.8348, 384.6104],\n",
            "        [ 79.5196, 253.3952, 627.7660, 477.7122],\n",
            "        [395.4265, 335.8509, 498.7440, 389.6847],\n",
            "        [201.5647, 268.8987, 558.7220, 333.6468]], device='cuda:0')), 'scores': tensor([0.8465, 0.7375, 0.7104, 0.6899, 0.5747, 0.5642, 0.5349, 0.4882, 0.4839,\n",
            "        0.4699, 0.4015, 0.3951, 0.3581, 0.3471, 0.3417, 0.3335, 0.3238, 0.3188,\n",
            "        0.3176, 0.3116, 0.3111, 0.3095, 0.3064, 0.3045, 0.2723, 0.2717, 0.2601,\n",
            "        0.2578, 0.2491, 0.2466, 0.2429, 0.2423, 0.2400, 0.2396, 0.2338, 0.2265],\n",
            "       device='cuda:0'), 'pred_classes': tensor([  42,   72,  212,   72,  381,   72,  291,  186,  183,   72,  212,  489,\n",
            "         381,  465,  601,  517,  212,  465,   42,  465,  499, 1227,   62,  291,\n",
            "         465,  465,  176,  291,  381,  299,  381,  212,  489,  176,  499,  183],\n",
            "       device='cuda:0'), 'features': tensor([[0.0000e+00, 0.0000e+00, 3.6436e-03,  ..., 8.6217e+00, 0.0000e+00,\n",
            "         1.5277e+00],\n",
            "        [3.7506e-01, 0.0000e+00, 8.2496e-01,  ..., 1.9005e-02, 1.1182e+00,\n",
            "         0.0000e+00],\n",
            "        [4.2864e-01, 0.0000e+00, 3.2063e+00,  ..., 0.0000e+00, 5.5845e-01,\n",
            "         6.0153e-02],\n",
            "        ...,\n",
            "        [0.0000e+00, 1.8121e+00, 2.5006e-02,  ..., 1.3686e+00, 2.2950e-01,\n",
            "         8.1109e-03],\n",
            "        [0.0000e+00, 2.1715e-02, 1.6211e-02,  ..., 1.5215e+01, 0.0000e+00,\n",
            "         4.5902e-01],\n",
            "        [0.0000e+00, 2.4141e-01, 2.0074e-01,  ..., 7.5262e-01, 1.4138e+00,\n",
            "         1.8340e+00]], device='cuda:0')}, '000542.jpg': {'pred_boxes': Boxes(tensor([[2.4959e+00, 2.1052e+02, 2.0968e+02, 3.7167e+02],\n",
            "        [5.9040e+00, 2.4220e+02, 3.5163e+02, 3.7375e+02],\n",
            "        [2.8981e+02, 5.7225e+01, 3.8733e+02, 1.4573e+02],\n",
            "        [1.0356e+02, 6.1185e+01, 1.6644e+02, 1.2256e+02],\n",
            "        [3.0282e+02, 6.1629e+01, 3.7007e+02, 1.2944e+02],\n",
            "        [1.1724e+02, 6.9283e+01, 3.9926e+02, 3.1134e+02],\n",
            "        [2.1535e+01, 0.0000e+00, 4.1511e+02, 2.2343e+02],\n",
            "        [1.8029e+02, 4.1329e+01, 4.0932e+02, 3.6325e+02],\n",
            "        [8.1524e+01, 6.6196e+00, 3.1697e+02, 3.1903e+02],\n",
            "        [3.9207e+01, 1.7525e+02, 4.1313e+02, 3.7272e+02],\n",
            "        [1.5638e+02, 3.9495e+00, 4.3311e+02, 2.6020e+02],\n",
            "        [1.9666e+02, 4.3728e+01, 4.9923e+02, 2.7767e+02],\n",
            "        [5.4823e-01, 1.0209e+02, 1.5453e+02, 3.3708e+02],\n",
            "        [2.9412e+02, 1.2977e+02, 5.0000e+02, 3.4668e+02],\n",
            "        [1.0324e+02, 1.3102e+02, 3.6950e+02, 3.6574e+02],\n",
            "        [3.2917e+02, 2.2502e+00, 4.9591e+02, 1.7106e+02],\n",
            "        [1.3470e+02, 1.2453e+02, 4.9502e+02, 3.5193e+02],\n",
            "        [5.4264e+01, 3.0341e+00, 2.6671e+02, 2.4956e+02],\n",
            "        [3.3155e+00, 1.0043e+02, 3.2594e+02, 3.3285e+02],\n",
            "        [0.0000e+00, 3.2310e+01, 1.7464e+02, 2.9940e+02],\n",
            "        [2.5277e+02, 2.1343e+02, 4.9722e+02, 3.7253e+02],\n",
            "        [5.1831e+01, 4.7578e+01, 2.8060e+02, 3.6415e+02],\n",
            "        [0.0000e+00, 1.9552e+00, 2.1885e+02, 1.9125e+02],\n",
            "        [1.6884e+02, 1.7205e+02, 4.9961e+02, 3.7294e+02],\n",
            "        [2.8483e+02, 1.9499e+00, 5.0000e+02, 2.0849e+02],\n",
            "        [0.0000e+00, 2.7259e+01, 2.9585e+02, 2.7277e+02],\n",
            "        [1.9976e+02, 2.1475e+02, 2.7102e+02, 2.6408e+02],\n",
            "        [8.8367e+01, 6.1283e-02, 3.9231e+02, 1.8212e+02],\n",
            "        [5.5775e+01, 5.9138e+01, 4.5428e+02, 3.1106e+02],\n",
            "        [9.1785e+01, 6.9384e+00, 3.9105e+02, 1.7100e+02],\n",
            "        [1.1570e+02, 2.3890e+02, 4.9847e+02, 3.7408e+02],\n",
            "        [2.0853e+02, 2.2172e+02, 2.6532e+02, 2.6882e+02],\n",
            "        [2.9768e+02, 3.8454e+01, 5.0000e+02, 3.4659e+02],\n",
            "        [1.2083e+02, 0.0000e+00, 5.0000e+02, 1.8561e+02],\n",
            "        [3.6933e+02, 1.0179e+01, 4.9854e+02, 3.3433e+02],\n",
            "        [4.4146e+01, 1.2156e+02, 3.0555e+02, 3.6246e+02]], device='cuda:0')), 'scores': tensor([0.9816, 0.9573, 0.9284, 0.8816, 0.8724, 0.8642, 0.7692, 0.7451, 0.7239,\n",
            "        0.7222, 0.6993, 0.6128, 0.6012, 0.5997, 0.5841, 0.5650, 0.5498, 0.5426,\n",
            "        0.5408, 0.4811, 0.4725, 0.4723, 0.4355, 0.4220, 0.4166, 0.4148, 0.4076,\n",
            "        0.3977, 0.3882, 0.3787, 0.3708, 0.3355, 0.3252, 0.2965, 0.2537, 0.2512],\n",
            "       device='cuda:0'), 'pred_classes': tensor([633, 633, 467, 467, 467, 327, 117, 327, 327, 633, 327, 327, 633, 633,\n",
            "        327, 274, 327, 327, 327, 327, 633, 327, 327, 633, 327, 117, 391, 327,\n",
            "        117, 546, 633, 198, 327, 117, 274, 633], device='cuda:0'), 'features': tensor([[1.7752e-02, 3.6765e-03, 0.0000e+00,  ..., 0.0000e+00, 9.0478e-02,\n",
            "         2.6414e-01],\n",
            "        [8.3949e-02, 1.4390e-01, 2.9448e-03,  ..., 0.0000e+00, 4.7250e-03,\n",
            "         3.6027e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.6377e-04,  ..., 0.0000e+00, 9.2118e-02,\n",
            "         2.0239e-01],\n",
            "        ...,\n",
            "        [1.3482e-01, 0.0000e+00, 1.6482e-02,  ..., 0.0000e+00, 6.8252e+00,\n",
            "         1.5489e-02],\n",
            "        [2.0054e-01, 0.0000e+00, 0.0000e+00,  ..., 2.3298e-02, 2.2283e+00,\n",
            "         2.9659e-03],\n",
            "        [2.7238e-01, 0.0000e+00, 1.1923e-03,  ..., 0.0000e+00, 2.8161e-02,\n",
            "         3.3188e-01]], device='cuda:0')}, 'COCO_train2014_000000573195.jpg': {'pred_boxes': Boxes(tensor([[  4.1870, 118.7577, 472.5717, 411.8146],\n",
            "        [461.3747,  31.1936, 639.4766, 300.7056],\n",
            "        [121.1785, 143.3020, 307.1068, 264.1370],\n",
            "        [111.8111, 128.2732, 599.7175, 391.0179],\n",
            "        [  1.0703,  11.5041, 123.9131, 286.3557],\n",
            "        [  0.0000, 106.7250, 370.7661, 387.4633],\n",
            "        [399.9296,  13.5733, 635.1549, 273.1708],\n",
            "        [417.8338,  21.7496, 631.1603, 392.8138],\n",
            "        [ 81.9250, 109.7298, 556.2501, 346.2739],\n",
            "        [ 87.8610, 112.9596, 409.0877, 400.2148],\n",
            "        [ 48.8203, 214.7409, 481.4753, 428.0000],\n",
            "        [104.4820,  54.4137, 389.1996, 299.6229],\n",
            "        [503.5068,  98.5747, 639.6537, 421.6425],\n",
            "        [133.4328, 127.1086, 286.1357, 267.0355],\n",
            "        [  0.0000,  77.3855, 430.9083, 348.4064],\n",
            "        [183.6939,  59.5448, 457.9046, 276.4438],\n",
            "        [202.4399, 108.5796, 639.9758, 412.3289],\n",
            "        [345.8728,  63.2954, 576.8434, 427.9081],\n",
            "        [108.1188,   6.5445, 432.1859, 206.4749],\n",
            "        [  1.4504,  18.5354, 130.3183, 397.8170],\n",
            "        [  0.0000,  79.2951, 297.9695, 368.8981],\n",
            "        [  0.0000,  78.3351, 302.7451, 362.7258],\n",
            "        [ 89.4274,   2.5105, 561.3289,  95.5212],\n",
            "        [ 18.0774,  33.8418, 476.8849, 323.7702],\n",
            "        [ 20.7070,  35.3498, 470.5147, 330.3662],\n",
            "        [ 84.4050,  63.9985, 510.0472, 300.9164],\n",
            "        [ 83.0118,  68.1575, 511.7305, 292.6950],\n",
            "        [396.5241,  70.0714, 634.6166, 349.8973],\n",
            "        [103.2193,  26.9080, 528.7559, 252.5791],\n",
            "        [146.9655,  60.8398, 447.8810, 342.4465],\n",
            "        [  0.0000,  18.8609, 439.8838, 272.3742],\n",
            "        [117.5511, 211.6889, 573.0897, 428.0000],\n",
            "        [434.3194, 232.3602, 639.9999, 428.0000],\n",
            "        [140.7760, 140.3719, 476.9821, 415.3537],\n",
            "        [ 29.3419,  39.6572, 460.6276, 326.1104],\n",
            "        [ 69.4567,  89.6659, 351.0903, 339.6570]], device='cuda:0')), 'scores': tensor([0.7066, 0.6956, 0.6621, 0.5508, 0.5330, 0.5144, 0.4801, 0.4672, 0.4558,\n",
            "        0.4282, 0.4264, 0.4036, 0.3975, 0.3864, 0.3717, 0.3230, 0.3190, 0.3133,\n",
            "        0.3127, 0.2945, 0.2919, 0.2828, 0.2812, 0.2805, 0.2802, 0.2787, 0.2674,\n",
            "        0.2637, 0.2589, 0.2567, 0.2549, 0.2387, 0.2343, 0.2318, 0.2316, 0.2269],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 52, 453,  53,  52, 453,  52, 453, 314,  52,  52, 314, 453, 291, 117,\n",
            "        155, 453,  52, 314, 453, 314,  52, 155, 500, 155,  52, 314, 453, 453,\n",
            "        453, 314,  52, 314, 248,  52, 314,  52], device='cuda:0'), 'features': tensor([[0.0227, 0.2413, 0.0000,  ..., 1.6127, 0.0000, 4.3360],\n",
            "        [0.0360, 0.0490, 0.0000,  ..., 0.9689, 1.9811, 0.3766],\n",
            "        [1.0279, 0.1389, 0.3103,  ..., 0.4017, 1.1162, 0.5752],\n",
            "        ...,\n",
            "        [0.0308, 0.1824, 0.0000,  ..., 1.0921, 0.2087, 7.1224],\n",
            "        [0.1989, 0.0910, 0.0000,  ..., 2.3126, 1.3025, 0.5346],\n",
            "        [0.2876, 0.2926, 0.0000,  ..., 1.0213, 0.7781, 0.2002]],\n",
            "       device='cuda:0')}, 'input.jpg': {'pred_boxes': Boxes(tensor([[1.7333e+02, 2.1515e+02, 4.8672e+02, 4.7373e+02],\n",
            "        [1.2166e+02, 2.0614e+02, 3.4905e+02, 4.8000e+02],\n",
            "        [5.8896e+02, 0.0000e+00, 6.3909e+02, 3.6998e+02],\n",
            "        [6.0792e+02, 9.0849e+01, 6.3765e+02, 4.2150e+02],\n",
            "        [2.8171e+02, 1.6275e+02, 3.2904e+02, 1.9557e+02],\n",
            "        [1.5337e+02, 9.6636e+01, 3.9307e+02, 4.5865e+02],\n",
            "        [3.9510e+00, 3.0139e-01, 1.7087e+02, 3.7003e+02],\n",
            "        [2.0478e+02, 0.0000e+00, 3.0078e+02, 2.7645e+02],\n",
            "        [3.8164e+02, 3.1898e+02, 6.1028e+02, 4.2289e+02],\n",
            "        [4.2380e+02, 2.7979e+02, 6.3800e+02, 3.9043e+02],\n",
            "        [5.3907e+01, 2.1506e+01, 1.2955e+02, 3.8665e+02],\n",
            "        [2.1639e+02, 3.3180e+02, 4.9085e+02, 4.7821e+02],\n",
            "        [4.5419e+01, 3.1766e+02, 5.8115e+02, 4.7680e+02],\n",
            "        [5.2262e+01, 1.5123e+02, 4.9093e+02, 4.3199e+02],\n",
            "        [3.4266e+02, 4.8674e+01, 6.3398e+02, 3.8901e+02],\n",
            "        [2.4584e+02, 1.8033e+02, 3.4975e+02, 4.0818e+02],\n",
            "        [1.7189e+02, 1.6335e+02, 6.3919e+02, 4.1045e+02],\n",
            "        [1.9629e+01, 0.0000e+00, 5.6497e+02, 1.5761e+02],\n",
            "        [3.9222e+02, 0.0000e+00, 6.3402e+02, 2.7783e+02],\n",
            "        [3.6025e+01, 0.0000e+00, 5.5431e+02, 2.8221e+02],\n",
            "        [1.5994e+02, 1.5115e+00, 3.5376e+02, 3.1772e+02],\n",
            "        [2.9326e+02, 1.4786e+02, 3.2540e+02, 1.8938e+02],\n",
            "        [0.0000e+00, 3.6491e+02, 4.3185e+02, 4.7849e+02],\n",
            "        [1.9907e+01, 4.2409e+02, 4.5854e+02, 4.7957e+02],\n",
            "        [4.9555e+00, 8.1566e+01, 2.3710e+02, 4.5235e+02],\n",
            "        [5.5625e+02, 2.7353e+02, 6.0216e+02, 3.7322e+02],\n",
            "        [9.2086e+01, 2.8328e+02, 3.2548e+02, 4.4708e+02],\n",
            "        [1.7761e+02, 3.6624e+02, 4.5720e+02, 4.6956e+02],\n",
            "        [1.7253e+02, 3.7161e+02, 6.4000e+02, 4.7876e+02],\n",
            "        [2.7954e+02, 2.0651e+02, 3.4036e+02, 3.1626e+02],\n",
            "        [1.9732e+02, 3.8317e+01, 6.4000e+02, 3.2455e+02],\n",
            "        [2.7082e+02, 1.1922e+00, 5.8803e+02, 3.0338e+02],\n",
            "        [6.5850e+00, 1.8703e+02, 3.0191e+02, 4.7954e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.2748e+02, 2.3305e+02],\n",
            "        [2.4732e-01, 3.3860e+02, 3.1494e+02, 4.7737e+02],\n",
            "        [2.0554e+02, 1.9619e+00, 6.4000e+02, 2.7667e+02]], device='cuda:0')), 'scores': tensor([0.9641, 0.9235, 0.7266, 0.6717, 0.6676, 0.5797, 0.5345, 0.4948, 0.4913,\n",
            "        0.4549, 0.4345, 0.4332, 0.4173, 0.3899, 0.3781, 0.3740, 0.3434, 0.3397,\n",
            "        0.3274, 0.3188, 0.3128, 0.3019, 0.3018, 0.2908, 0.2826, 0.2780, 0.2777,\n",
            "        0.2759, 0.2679, 0.2581, 0.2509, 0.2487, 0.2475, 0.2468, 0.2462, 0.2415],\n",
            "       device='cuda:0'), 'pred_classes': tensor([  42,   42,  601,  601,  234,   42,  291,  291,  236,  540,  291,   42,\n",
            "         236,   42,  291,   50,  540, 1180,  291,  381,  291,  234,  465,  465,\n",
            "         291,  364,   42,  829,  465,   51,  381,  291,   42, 1180,  236, 1180],\n",
            "       device='cuda:0'), 'features': tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.5319e+00, 0.0000e+00,\n",
            "         7.2346e+00],\n",
            "        [0.0000e+00, 5.1048e-02, 5.3453e-02,  ..., 5.4974e+00, 0.0000e+00,\n",
            "         5.6174e+00],\n",
            "        [2.1346e-03, 3.7233e-03, 2.3407e-03,  ..., 1.7078e-01, 4.5449e-01,\n",
            "         3.5948e+00],\n",
            "        ...,\n",
            "        [7.2195e-02, 0.0000e+00, 1.1024e+00,  ..., 0.0000e+00, 1.7581e+00,\n",
            "         1.1493e-02],\n",
            "        [0.0000e+00, 5.7147e-01, 1.5179e+00,  ..., 3.4173e-01, 0.0000e+00,\n",
            "         1.8128e-02],\n",
            "        [3.3889e-02, 5.4348e-02, 0.0000e+00,  ..., 7.1072e-01, 1.1888e+00,\n",
            "         1.4662e-02]], device='cuda:0')}, 'COCO_train2014_000000201446.jpg': {'pred_boxes': Boxes(tensor([[3.4400e+02, 1.4137e+02, 5.1696e+02, 2.4731e+02],\n",
            "        [3.5452e+02, 1.4753e+02, 5.4942e+02, 2.5666e+02],\n",
            "        [4.9061e+02, 2.3873e+02, 6.2122e+02, 3.7531e+02],\n",
            "        [4.8071e+02, 2.5689e+02, 6.1135e+02, 3.8762e+02],\n",
            "        [1.7033e+02, 2.1489e+02, 6.0415e+02, 4.3800e+02],\n",
            "        [3.5111e+02, 3.4759e+01, 3.9450e+02, 1.7184e+02],\n",
            "        [2.6002e+02, 1.3180e+02, 5.9359e+02, 4.3468e+02],\n",
            "        [3.4272e+02, 6.6144e-01, 6.4000e+02, 1.4935e+02],\n",
            "        [7.9470e-01, 3.4443e+01, 2.9970e+02, 3.1382e+02],\n",
            "        [3.2445e+02, 1.1217e+01, 6.3906e+02, 2.5731e+02],\n",
            "        [3.3921e+02, 1.6631e+02, 6.4000e+02, 4.3800e+02],\n",
            "        [2.3373e+02, 1.8950e+02, 5.4829e+02, 4.3800e+02],\n",
            "        [1.3671e+02, 1.8741e+02, 4.3704e+02, 4.3800e+02],\n",
            "        [2.1176e+02, 2.6673e+02, 6.3750e+02, 4.3800e+02],\n",
            "        [1.4425e+02, 2.6683e+02, 4.5536e+02, 4.3800e+02],\n",
            "        [2.0570e+02, 1.3146e+02, 5.3466e+02, 4.2974e+02],\n",
            "        [4.9928e+01, 2.2749e+02, 5.2456e+02, 4.3800e+02],\n",
            "        [0.0000e+00, 1.8080e+01, 2.2190e+02, 3.5053e+02],\n",
            "        [3.8528e+02, 2.4309e+00, 6.4000e+02, 2.1720e+02],\n",
            "        [1.2973e+02, 2.8782e+02, 5.9512e+02, 4.3800e+02],\n",
            "        [2.3144e+02, 1.8804e+02, 5.4993e+02, 4.3800e+02],\n",
            "        [3.0274e+00, 1.4180e+02, 3.3900e+02, 4.3800e+02],\n",
            "        [0.0000e+00, 1.4917e+00, 5.1956e+02, 2.2769e+02],\n",
            "        [2.5050e+02, 1.0394e+00, 6.4000e+02, 1.6913e+02],\n",
            "        [3.6657e+02, 2.3985e+01, 6.4000e+02, 3.3297e+02],\n",
            "        [3.4443e+01, 6.7987e+00, 3.5264e+02, 2.7373e+02],\n",
            "        [1.7412e+02, 1.6588e+02, 4.8348e+02, 4.3691e+02],\n",
            "        [7.8025e+01, 2.5164e+02, 3.6995e+02, 4.3800e+02],\n",
            "        [1.7997e+02, 1.9461e-01, 6.3976e+02, 1.3418e+02],\n",
            "        [1.1995e+02, 1.2325e+02, 5.7929e+02, 4.1065e+02],\n",
            "        [4.9338e+01, 7.7465e+01, 3.4991e+02, 3.5769e+02],\n",
            "        [0.0000e+00, 1.2912e+00, 5.2476e+02, 1.5731e+02],\n",
            "        [1.4709e+02, 2.6826e+02, 4.5308e+02, 4.3800e+02],\n",
            "        [8.9817e+01, 4.4296e-01, 6.1258e+02, 1.7643e+02],\n",
            "        [1.2605e+02, 2.8554e+02, 5.9811e+02, 4.3800e+02],\n",
            "        [1.5031e+02, 8.6646e-01, 5.9229e+02, 2.6192e+02]], device='cuda:0')), 'scores': tensor([0.9513, 0.8993, 0.7455, 0.7443, 0.5320, 0.4720, 0.4597, 0.4512, 0.4165,\n",
            "        0.4012, 0.3913, 0.3819, 0.3819, 0.3569, 0.3551, 0.3533, 0.3418, 0.3246,\n",
            "        0.3237, 0.3168, 0.3137, 0.3059, 0.3038, 0.3028, 0.2958, 0.2934, 0.2930,\n",
            "        0.2892, 0.2869, 0.2863, 0.2844, 0.2834, 0.2686, 0.2662, 0.2651, 0.2569],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 529,  529, 1360, 1360,  734,  756,  734,  106,  369,  756,  284,  734,\n",
            "        1227,  734, 1227,  734, 1227,  369,  106,  734, 1227,  291,  106,  106,\n",
            "         106,  369,  734, 1227,  106,  734,  369,  106,  734,  106, 1227,  106],\n",
            "       device='cuda:0'), 'features': tensor([[0.0000e+00, 5.5904e-01, 0.0000e+00,  ..., 0.0000e+00, 9.7469e-03,\n",
            "         8.3732e+00],\n",
            "        [2.3254e-01, 3.0616e-01, 0.0000e+00,  ..., 0.0000e+00, 7.3613e-02,\n",
            "         8.2281e+00],\n",
            "        [0.0000e+00, 1.5476e+00, 5.4108e-02,  ..., 0.0000e+00, 1.1620e+00,\n",
            "         3.5310e+00],\n",
            "        ...,\n",
            "        [4.9435e-01, 3.0959e+00, 9.3492e-03,  ..., 1.3386e-01, 7.3399e-01,\n",
            "         2.7860e-03],\n",
            "        [1.3010e-01, 2.3633e-01, 0.0000e+00,  ..., 0.0000e+00, 1.9002e-01,\n",
            "         1.4657e+00],\n",
            "        [5.2825e-01, 2.9854e+00, 0.0000e+00,  ..., 0.0000e+00, 4.5905e-01,\n",
            "         8.3844e-01]], device='cuda:0')}, 'COCO_train2014_000000084002.jpg': {'pred_boxes': Boxes(tensor([[1.6070e+02, 1.2578e+02, 5.0093e+02, 4.3490e+02],\n",
            "        [2.2665e+02, 6.3243e+01, 5.6139e+02, 3.5836e+02],\n",
            "        [6.9964e+01, 8.5555e+01, 5.2293e+02, 3.5230e+02],\n",
            "        [2.0988e+02, 0.0000e+00, 2.8636e+02, 1.7594e+02],\n",
            "        [2.8205e+02, 1.2141e+02, 6.1044e+02, 4.2426e+02],\n",
            "        [4.5228e+02, 0.0000e+00, 6.3780e+02, 3.7104e+02],\n",
            "        [2.7045e+00, 0.0000e+00, 1.9533e+02, 4.1348e+02],\n",
            "        [1.3212e+02, 1.6476e+01, 2.0893e+02, 9.1588e+01],\n",
            "        [2.3818e+00, 2.7999e+02, 2.9151e+02, 4.7857e+02],\n",
            "        [4.0826e+02, 3.3182e+02, 4.7826e+02, 3.8972e+02],\n",
            "        [5.7621e+01, 3.5700e-01, 2.4543e+02, 3.0998e+02],\n",
            "        [1.4043e+02, 3.0749e+02, 6.4000e+02, 4.8000e+02],\n",
            "        [2.1112e+02, 1.7916e+02, 4.7152e+02, 4.7637e+02],\n",
            "        [3.1206e+00, 0.0000e+00, 1.4002e+02, 3.3279e+02],\n",
            "        [1.4358e+02, 4.0320e+02, 1.9591e+02, 4.3415e+02],\n",
            "        [2.4213e+01, 1.5476e+02, 4.9291e+02, 3.9794e+02],\n",
            "        [2.2688e+02, 3.0121e+02, 4.2454e+02, 4.0628e+02],\n",
            "        [1.3887e+01, 2.1351e+02, 3.5907e+02, 4.7620e+02],\n",
            "        [1.0064e+02, 1.4434e+01, 5.9234e+02, 3.0286e+02],\n",
            "        [6.2647e+02, 9.3641e+00, 6.3983e+02, 3.9837e+01],\n",
            "        [4.8271e+02, 6.0136e+01, 5.6129e+02, 2.6942e+02],\n",
            "        [2.0255e+02, 7.5896e-01, 5.0613e+02, 2.1550e+02],\n",
            "        [2.4282e+01, 2.4185e+02, 5.2937e+02, 4.7732e+02],\n",
            "        [3.2751e+02, 3.6127e+01, 6.3611e+02, 3.7144e+02],\n",
            "        [4.0240e-01, 3.0335e+02, 1.5392e+02, 4.7402e+02],\n",
            "        [2.4976e+02, 2.1522e+02, 3.3222e+02, 2.9114e+02],\n",
            "        [8.7307e+01, 8.7208e+01, 3.7013e+02, 4.7488e+02],\n",
            "        [3.5970e+02, 2.6583e+02, 6.3987e+02, 4.7795e+02],\n",
            "        [3.8179e+00, 0.0000e+00, 3.7712e+02, 3.0419e+02],\n",
            "        [1.7891e+00, 6.8024e+01, 1.4336e+02, 4.7096e+02],\n",
            "        [1.4612e+02, 2.0138e+02, 6.0141e+02, 4.5212e+02],\n",
            "        [2.0729e+02, 1.3647e+02, 4.7018e+02, 2.3175e+02],\n",
            "        [0.0000e+00, 3.3708e+02, 4.5300e+02, 4.8000e+02],\n",
            "        [1.1510e+02, 6.6669e-02, 4.5804e+02, 2.2308e+02],\n",
            "        [2.6222e+02, 0.0000e+00, 6.3570e+02, 2.7119e+02],\n",
            "        [0.0000e+00, 2.0129e+02, 1.6961e+02, 4.7219e+02]], device='cuda:0')), 'scores': tensor([0.8802, 0.8115, 0.8022, 0.7692, 0.7032, 0.6996, 0.6920, 0.6862, 0.6850,\n",
            "        0.6631, 0.6126, 0.5892, 0.5681, 0.5611, 0.5567, 0.5425, 0.4922, 0.4254,\n",
            "        0.4194, 0.4105, 0.3930, 0.3845, 0.3799, 0.3673, 0.3448, 0.3385, 0.3352,\n",
            "        0.3215, 0.3166, 0.3059, 0.2982, 0.2895, 0.2831, 0.2545, 0.2296, 0.2264],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 33,  33,  33, 273,  33, 248, 248, 395, 200, 680, 248, 200,  33, 248,\n",
            "        242,  33, 166, 200, 248, 800, 224, 453, 200, 248, 200, 320,  33, 200,\n",
            "        248, 248,  33, 331, 200, 248, 248, 200], device='cuda:0'), 'features': tensor([[2.3688e+00, 7.0104e-02, 0.0000e+00,  ..., 4.8844e+00, 5.2768e-01,\n",
            "         1.9205e+00],\n",
            "        [2.7649e+00, 4.8691e-01, 5.4496e-03,  ..., 4.5353e+00, 2.8115e+00,\n",
            "         1.2117e+00],\n",
            "        [2.6585e+00, 2.3304e-01, 5.5512e-02,  ..., 5.9903e+00, 2.6668e+00,\n",
            "         6.8443e-01],\n",
            "        ...,\n",
            "        [3.5909e+00, 6.1176e-02, 6.0869e-02,  ..., 2.1540e+00, 2.3752e+00,\n",
            "         0.0000e+00],\n",
            "        [3.5272e+00, 5.8667e-01, 2.2504e-03,  ..., 1.9987e+00, 4.6840e+00,\n",
            "         0.0000e+00],\n",
            "        [1.2270e-02, 0.0000e+00, 3.5597e-04,  ..., 5.9896e-03, 8.2102e-03,\n",
            "         1.3012e-03]], device='cuda:0')}, '000456.jpg': {'pred_boxes': Boxes(tensor([[1.6387e+02, 5.6178e+01, 4.3384e+02, 2.9528e+02],\n",
            "        [9.7870e+01, 6.5075e+01, 3.7183e+02, 3.1557e+02],\n",
            "        [8.8790e+01, 2.8907e+01, 4.2742e+02, 2.4740e+02],\n",
            "        [2.5198e+02, 2.5296e+00, 4.3743e+02, 2.9395e+02],\n",
            "        [3.8366e-01, 7.4473e+00, 2.9257e+02, 6.0494e+01],\n",
            "        [6.6482e+01, 9.3067e+01, 2.9819e+02, 3.2034e+02],\n",
            "        [8.5849e+01, 7.2620e+01, 2.5487e+02, 2.0415e+02],\n",
            "        [1.3373e+02, 1.2887e+02, 4.0310e+02, 3.7202e+02],\n",
            "        [1.5320e+02, 5.8702e-01, 4.3726e+02, 2.1038e+02],\n",
            "        [6.8097e+01, 4.2994e+00, 2.6680e+02, 2.9905e+02],\n",
            "        [0.0000e+00, 1.0853e-01, 2.9286e+02, 9.6502e+01],\n",
            "        [1.3280e+01, 5.3424e+01, 3.5370e+02, 2.4638e+02],\n",
            "        [0.0000e+00, 9.7298e+01, 1.6437e+02, 3.6537e+02],\n",
            "        [2.1987e+01, 1.6864e+02, 4.2417e+02, 3.6960e+02],\n",
            "        [1.7972e+01, 0.0000e+00, 4.0869e+02, 1.9713e+02],\n",
            "        [0.0000e+00, 1.2422e+02, 2.9798e+02, 3.5582e+02],\n",
            "        [0.0000e+00, 3.1172e+01, 1.1075e+02, 1.2794e+02],\n",
            "        [3.3169e+02, 5.4961e+01, 4.9982e+02, 3.3006e+02],\n",
            "        [1.0366e+02, 1.3073e+02, 2.5711e+02, 2.2498e+02],\n",
            "        [4.5762e+01, 5.6816e+01, 2.5371e+02, 3.6857e+02],\n",
            "        [2.8855e+02, 4.0895e+01, 4.7440e+02, 3.6090e+02],\n",
            "        [2.1769e+02, 2.8334e+01, 4.9456e+02, 2.2596e+02],\n",
            "        [9.4307e+01, 2.1246e+02, 2.6603e+02, 2.9298e+02],\n",
            "        [1.4127e+02, 2.0286e+02, 5.0000e+02, 3.7500e+02],\n",
            "        [1.4229e+02, 4.2472e+01, 4.0994e+02, 9.4721e+01],\n",
            "        [1.1027e+02, 1.7167e+02, 1.5038e+02, 2.0026e+02],\n",
            "        [8.7859e+01, 2.2919e+02, 3.9674e+02, 3.7459e+02],\n",
            "        [1.8674e+02, 1.6506e+02, 2.4081e+02, 2.2983e+02],\n",
            "        [1.4221e+00, 3.6811e+00, 1.6994e+02, 1.3855e+02],\n",
            "        [1.7900e+01, 9.6780e-01, 4.4766e+02, 1.2221e+02],\n",
            "        [0.0000e+00, 1.2545e+02, 2.9954e+02, 3.5667e+02],\n",
            "        [1.9950e+01, 1.7017e+02, 4.2734e+02, 3.7031e+02],\n",
            "        [2.9029e+02, 1.2954e+02, 4.9950e+02, 3.4837e+02],\n",
            "        [2.1184e+02, 1.0203e+02, 4.8799e+02, 3.4881e+02],\n",
            "        [9.3212e-01, 1.5437e+02, 1.1981e+02, 3.7475e+02],\n",
            "        [2.8802e-01, 5.6090e+01, 1.3157e+02, 2.8200e+02]], device='cuda:0')), 'scores': tensor([0.9908, 0.9879, 0.9771, 0.9124, 0.8731, 0.8465, 0.8056, 0.7722, 0.7709,\n",
            "        0.6923, 0.6871, 0.6724, 0.5160, 0.5094, 0.4911, 0.4649, 0.4529, 0.4405,\n",
            "        0.4356, 0.4022, 0.3964, 0.3873, 0.3862, 0.3809, 0.3641, 0.3455, 0.3395,\n",
            "        0.3316, 0.3311, 0.3227, 0.3192, 0.3175, 0.3152, 0.2868, 0.2790, 0.2728],\n",
            "       device='cuda:0'), 'pred_classes': tensor([  55,   55,   55,   55,  222,   55,  287,   55,   55,   55,  222,   55,\n",
            "         299,  299,   55,  299,  594,  299,  287,   55,  299,   55,  493,  299,\n",
            "         500,  911,  299, 1470,  594,  222,  308,  308,  937,  299,  299,  299],\n",
            "       device='cuda:0'), 'features': tensor([[6.2032e-02, 4.6890e-03, 1.6145e+00,  ..., 9.7965e+00, 2.0251e-02,\n",
            "         5.0674e+00],\n",
            "        [5.2133e-04, 0.0000e+00, 7.5041e-01,  ..., 6.3023e+00, 5.8351e-03,\n",
            "         5.8178e+00],\n",
            "        [1.0237e-02, 0.0000e+00, 6.0753e-01,  ..., 1.1332e+01, 4.5740e-01,\n",
            "         5.5115e+00],\n",
            "        ...,\n",
            "        [9.3654e-03, 2.2415e-02, 3.5341e+00,  ..., 5.5440e+00, 1.3838e-03,\n",
            "         2.2087e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 2.8001e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0268e+00,  ..., 2.9663e-01, 3.4905e-01,\n",
            "         0.0000e+00]], device='cuda:0')}, 'COCO_train2014_000000306661.jpg': {'pred_boxes': Boxes(tensor([[2.6472e+00, 0.0000e+00, 4.4844e+02, 1.7365e+02],\n",
            "        [9.2876e-01, 2.9768e+02, 2.4914e+02, 4.2700e+02],\n",
            "        [4.5728e+02, 9.1046e+00, 6.2543e+02, 1.7602e+02],\n",
            "        [2.1785e+02, 1.1152e+02, 3.7063e+02, 3.0222e+02],\n",
            "        [4.4301e+00, 1.2547e+02, 3.5830e+02, 3.4837e+02],\n",
            "        [3.9909e+02, 0.0000e+00, 6.3714e+02, 1.5002e+02],\n",
            "        [3.2293e+02, 1.6757e+01, 5.8364e+02, 4.1439e+02],\n",
            "        [2.0242e+02, 7.6918e+01, 3.6093e+02, 3.6172e+02],\n",
            "        [1.3062e+00, 1.6151e+02, 3.9032e+02, 4.1357e+02],\n",
            "        [1.4235e+00, 4.6312e+01, 2.6189e+02, 3.2701e+02],\n",
            "        [5.4999e+02, 2.4153e-01, 6.2951e+02, 4.8731e+01],\n",
            "        [7.3667e+01, 1.2392e+00, 3.9117e+02, 2.3395e+02],\n",
            "        [0.0000e+00, 1.2827e+02, 2.3361e+02, 4.0853e+02],\n",
            "        [3.2802e+02, 1.7982e+02, 6.2871e+02, 4.2403e+02],\n",
            "        [1.6936e+00, 3.4900e+02, 2.5344e+02, 4.2700e+02],\n",
            "        [1.6932e+02, 0.0000e+00, 3.6540e+02, 6.3152e+01],\n",
            "        [4.2182e+02, 4.5934e+01, 6.4000e+02, 3.7408e+02],\n",
            "        [2.5230e+02, 1.0706e+02, 3.5294e+02, 2.5972e+02],\n",
            "        [2.4383e+02, 7.7429e+01, 6.3585e+02, 3.7230e+02],\n",
            "        [3.2637e+02, 2.9635e+01, 6.3584e+02, 2.9132e+02],\n",
            "        [5.7942e+01, 1.2383e+02, 4.9605e+02, 3.6528e+02],\n",
            "        [1.6684e+02, 1.1496e+02, 4.3215e+02, 3.8708e+02],\n",
            "        [2.2764e+02, 6.9294e+01, 4.5557e+02, 4.1946e+02],\n",
            "        [2.1818e+02, 1.6425e+02, 3.5794e+02, 3.0090e+02],\n",
            "        [4.4647e+02, 4.8104e+00, 6.4000e+02, 2.7108e+02],\n",
            "        [0.0000e+00, 1.0396e+00, 2.7558e+02, 2.3942e+02],\n",
            "        [1.4481e+02, 1.9807e+00, 4.6642e+02, 2.6667e+02],\n",
            "        [2.2082e+02, 0.0000e+00, 6.4000e+02, 2.3450e+02],\n",
            "        [1.9572e+02, 2.4459e+02, 6.2103e+02, 4.2700e+02],\n",
            "        [9.3874e+01, 2.7291e+02, 3.8201e+02, 4.2660e+02],\n",
            "        [4.4539e+02, 2.1996e+00, 6.3825e+02, 2.7580e+02],\n",
            "        [1.2648e+02, 1.6367e+02, 5.6411e+02, 4.2700e+02],\n",
            "        [1.0457e+02, 9.2603e+01, 3.9698e+02, 3.5724e+02],\n",
            "        [0.0000e+00, 1.2757e+02, 2.3405e+02, 4.1196e+02],\n",
            "        [7.5283e+01, 2.1267e+02, 3.7480e+02, 4.2700e+02],\n",
            "        [3.7398e+02, 0.0000e+00, 5.6936e+02, 2.5060e+02]], device='cuda:0')), 'scores': tensor([0.8276, 0.8225, 0.8152, 0.7503, 0.7470, 0.7467, 0.7392, 0.6838, 0.6599,\n",
            "        0.6386, 0.6375, 0.5800, 0.5777, 0.5705, 0.5573, 0.5273, 0.5199, 0.4999,\n",
            "        0.4687, 0.4639, 0.4637, 0.4533, 0.4356, 0.3811, 0.3805, 0.3596, 0.3528,\n",
            "        0.3521, 0.3430, 0.3361, 0.3327, 0.3297, 0.3054, 0.2897, 0.2810, 0.2732],\n",
            "       device='cuda:0'), 'pred_classes': tensor([248, 106, 119,  90, 136, 119,  50,  90, 136, 136, 395, 248, 136,  51,\n",
            "        106, 345,  50,  90,  50,  50, 136,  90,  90,  51, 119, 248, 248, 248,\n",
            "         51, 941, 191,  50, 136, 423,  50, 191], device='cuda:0'), 'features': tensor([[5.7066e-01, 0.0000e+00, 0.0000e+00,  ..., 3.4853e-01, 3.8694e+00,\n",
            "         0.0000e+00],\n",
            "        [6.6084e-02, 7.4124e-01, 0.0000e+00,  ..., 0.0000e+00, 7.2015e-02,\n",
            "         1.8142e+00],\n",
            "        [3.3805e-02, 0.0000e+00, 7.6879e-02,  ..., 8.0442e-02, 3.1882e+00,\n",
            "         1.1606e-01],\n",
            "        ...,\n",
            "        [4.8797e-01, 6.2054e-01, 0.0000e+00,  ..., 4.7507e-02, 2.7156e+00,\n",
            "         1.0044e+00],\n",
            "        [5.9770e-02, 3.5418e-01, 0.0000e+00,  ..., 5.4808e-02, 2.3741e+00,\n",
            "         6.3616e-01],\n",
            "        [1.3159e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.7871e-01,\n",
            "         2.6843e-03]], device='cuda:0')}, 'COCO_train2014_000000132417.jpg': {'pred_boxes': Boxes(tensor([[0.0000e+00, 6.8092e-01, 2.1884e+02, 2.2805e+02],\n",
            "        [0.0000e+00, 2.0006e-01, 3.7301e+02, 1.5913e+02],\n",
            "        [2.7860e+01, 1.4088e+01, 4.1145e+02, 2.1738e+02],\n",
            "        [4.4326e+02, 1.7144e+02, 4.9834e+02, 2.4517e+02],\n",
            "        [9.7492e+01, 0.0000e+00, 4.8681e+02, 1.9966e+02],\n",
            "        [3.0703e+01, 5.8206e-01, 4.3655e+02, 1.1378e+02],\n",
            "        [6.6228e+01, 2.4703e+02, 4.2708e+02, 3.0259e+02],\n",
            "        [0.0000e+00, 2.4455e+02, 3.0527e+02, 2.9791e+02],\n",
            "        [0.0000e+00, 1.1202e+00, 2.3831e+02, 1.3289e+02],\n",
            "        [5.7753e+01, 2.3256e+02, 4.7000e+02, 3.3040e+02],\n",
            "        [2.7017e+02, 7.9241e-02, 5.0000e+02, 2.1699e+02],\n",
            "        [1.5774e+02, 5.0730e-01, 5.0000e+02, 1.4258e+02],\n",
            "        [0.0000e+00, 3.0368e+02, 2.8082e+02, 3.3262e+02],\n",
            "        [0.0000e+00, 2.3425e+02, 2.3011e+02, 3.3009e+02],\n",
            "        [9.1159e-02, 6.5274e-01, 3.8048e+02, 8.3768e+01],\n",
            "        [1.0353e+00, 2.0883e+00, 1.4969e+02, 1.5030e+02],\n",
            "        [0.0000e+00, 5.7631e+01, 2.9950e+02, 2.4478e+02],\n",
            "        [2.0226e+02, 8.2404e+00, 4.6416e+02, 2.3642e+02],\n",
            "        [3.0596e+02, 1.8970e+02, 3.1822e+02, 2.4432e+02],\n",
            "        [2.8790e+02, 1.3967e+00, 4.9753e+02, 1.2741e+02],\n",
            "        [0.0000e+00, 2.0985e+02, 2.7126e+02, 3.0846e+02],\n",
            "        [2.0985e+01, 2.5705e+02, 3.5530e+02, 3.1374e+02],\n",
            "        [1.3770e+02, 2.4281e+02, 5.0000e+02, 2.8759e+02],\n",
            "        [3.1747e+01, 2.7810e+02, 4.1479e+02, 3.1997e+02],\n",
            "        [6.7577e+01, 1.8637e+02, 4.7611e+02, 3.0761e+02],\n",
            "        [1.2440e+02, 2.5176e+02, 4.7770e+02, 3.0694e+02],\n",
            "        [2.9240e+02, 2.4111e+02, 5.0000e+02, 3.3090e+02],\n",
            "        [1.7627e+01, 2.5741e+02, 3.6093e+02, 3.1366e+02],\n",
            "        [1.1564e+02, 2.5811e+02, 5.0000e+02, 3.3120e+02],\n",
            "        [1.9177e+02, 2.3268e+02, 4.8211e+02, 2.7186e+02],\n",
            "        [1.6951e+02, 3.0034e+02, 4.9508e+02, 3.3244e+02],\n",
            "        [2.7957e+00, 2.1651e+02, 3.1908e+02, 2.5850e+02],\n",
            "        [8.4839e+01, 2.7373e+02, 4.7584e+02, 3.3245e+02],\n",
            "        [1.1481e+02, 2.6183e+02, 4.7770e+02, 3.1768e+02],\n",
            "        [1.3396e+02, 2.0879e+02, 5.0000e+02, 2.9292e+02],\n",
            "        [1.5531e+02, 8.6183e+01, 3.8198e+02, 2.5442e+02]], device='cuda:0')), 'scores': tensor([0.7935, 0.7243, 0.7192, 0.6888, 0.6513, 0.6456, 0.6444, 0.6075, 0.6019,\n",
            "        0.5994, 0.5732, 0.5669, 0.5575, 0.5090, 0.4801, 0.4491, 0.4405, 0.4333,\n",
            "        0.4251, 0.4176, 0.4011, 0.3895, 0.3816, 0.3313, 0.3277, 0.2679, 0.2643,\n",
            "        0.2514, 0.2456, 0.2345, 0.2226, 0.2222, 0.2143, 0.2119, 0.2092, 0.2069],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 72,  72,  72, 291,  72,  72, 100, 100,  72, 100,  72,  72, 183, 100,\n",
            "         72,  72,  72,  72, 601,  72, 100, 100, 100, 222, 100, 222, 100, 222,\n",
            "        222, 100, 183, 381, 183, 100, 100, 291], device='cuda:0'), 'features': tensor([[1.3147e-01, 3.1145e-02, 1.2152e-01,  ..., 0.0000e+00, 5.7638e-02,\n",
            "         0.0000e+00],\n",
            "        [3.0590e-01, 0.0000e+00, 5.2095e-02,  ..., 1.3611e-01, 2.5172e+00,\n",
            "         0.0000e+00],\n",
            "        [1.1653e-01, 3.4964e-01, 2.9178e-01,  ..., 1.2426e-01, 3.0427e-01,\n",
            "         4.2890e-02],\n",
            "        ...,\n",
            "        [0.0000e+00, 1.9225e+00, 7.8461e-03,  ..., 0.0000e+00, 1.0002e-01,\n",
            "         6.0037e-02],\n",
            "        [3.4569e-03, 4.4127e+00, 0.0000e+00,  ..., 0.0000e+00, 7.5128e-01,\n",
            "         0.0000e+00],\n",
            "        [0.0000e+00, 9.4211e-01, 1.4420e-01,  ..., 1.5189e-02, 0.0000e+00,\n",
            "         2.3504e+00]], device='cuda:0')}, '004545.jpg': {'pred_boxes': Boxes(tensor([[2.5399e+02, 6.2883e+01, 4.4369e+02, 3.4404e+02],\n",
            "        [1.2280e+02, 1.9666e+02, 1.9908e+02, 3.4950e+02],\n",
            "        [2.0328e+02, 9.7284e+01, 3.7980e+02, 3.7500e+02],\n",
            "        [3.7382e+02, 1.9945e+02, 4.1253e+02, 3.0831e+02],\n",
            "        [1.7944e+02, 5.0097e+01, 4.8756e+02, 2.8113e+02],\n",
            "        [2.3917e+02, 1.7068e+01, 3.5602e+02, 1.9768e+02],\n",
            "        [2.3289e+02, 9.4383e+01, 3.2610e+02, 2.9884e+02],\n",
            "        [6.5185e+00, 1.0078e+02, 1.2492e+02, 1.9988e+02],\n",
            "        [4.2929e+02, 1.2839e+02, 4.4847e+02, 1.8189e+02],\n",
            "        [2.9099e+02, 1.0759e+02, 3.6486e+02, 2.4646e+02],\n",
            "        [4.6776e+02, 1.4854e+02, 4.9972e+02, 1.7670e+02],\n",
            "        [5.3242e+01, 1.5618e+02, 4.9031e+02, 3.7329e+02],\n",
            "        [1.4251e+00, 2.8220e-01, 1.4096e+02, 1.6252e+02],\n",
            "        [1.5088e+02, 1.6829e+02, 2.1895e+02, 1.8693e+02],\n",
            "        [0.0000e+00, 1.5553e+02, 2.4030e+02, 3.7252e+02],\n",
            "        [4.2792e+02, 1.3280e+02, 4.5309e+02, 1.5802e+02],\n",
            "        [2.8133e+02, 3.1616e+01, 3.7140e+02, 2.2951e+02],\n",
            "        [2.6819e+02, 2.0011e+02, 5.0000e+02, 3.7461e+02],\n",
            "        [2.1287e+02, 1.0803e+02, 3.1554e+02, 2.2994e+02],\n",
            "        [1.8673e+02, 3.4060e+01, 3.4211e+02, 2.9355e+02],\n",
            "        [0.0000e+00, 2.0441e+02, 2.7719e+02, 3.7417e+02],\n",
            "        [3.3356e+00, 1.1748e+00, 2.4073e+02, 1.8801e+02],\n",
            "        [2.9368e+02, 8.2925e+01, 3.7470e+02, 2.3070e+02],\n",
            "        [4.2131e+01, 5.9464e+01, 4.0046e+02, 2.9836e+02],\n",
            "        [4.3012e+02, 1.2878e+02, 4.5327e+02, 1.5378e+02],\n",
            "        [0.0000e+00, 1.8861e+00, 2.5217e+02, 1.8779e+02],\n",
            "        [2.0262e+02, 7.5878e+01, 3.0677e+02, 2.2067e+02],\n",
            "        [5.5106e+01, 1.5862e+00, 4.5628e+02, 1.7589e+02],\n",
            "        [0.0000e+00, 2.3703e+02, 4.0700e+02, 3.7500e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.6657e+02, 1.1566e+02],\n",
            "        [1.4035e+02, 1.2025e+02, 4.8887e+02, 3.6043e+02],\n",
            "        [1.2606e+02, 9.7820e+01, 3.7282e+02, 3.6029e+02],\n",
            "        [2.6000e+02, 5.0346e+01, 3.5183e+02, 2.3284e+02],\n",
            "        [2.6806e+02, 1.9956e+02, 5.0000e+02, 3.7308e+02],\n",
            "        [2.7746e+02, 4.3145e+01, 3.4649e+02, 1.2185e+02],\n",
            "        [4.7063e+01, 5.5629e+01, 3.9822e+02, 3.0026e+02]], device='cuda:0')), 'scores': tensor([0.9752, 0.9599, 0.9581, 0.9226, 0.8108, 0.5987, 0.5423, 0.5155, 0.4833,\n",
            "        0.4633, 0.4566, 0.4380, 0.4327, 0.4275, 0.3968, 0.3895, 0.3846, 0.3810,\n",
            "        0.3807, 0.3784, 0.3567, 0.3356, 0.3102, 0.3085, 0.2801, 0.2798, 0.2773,\n",
            "        0.2638, 0.2622, 0.2618, 0.2592, 0.2527, 0.2520, 0.2423, 0.2352, 0.2306],\n",
            "       device='cuda:0'), 'pred_classes': tensor([  42,  117,   42,  283,   42,   50,   42,  155,   90,  840,  266,  465,\n",
            "         291,  623,  465,   51,   50,  611,  573,   42,  611,  291,   47,  186,\n",
            "         181,  381,  191,  381,  465, 1180,   42,  186,   90,  465,   51,   42],\n",
            "       device='cuda:0'), 'features': tensor([[0.0000e+00, 2.0533e-02, 0.0000e+00,  ..., 7.1536e+00, 0.0000e+00,\n",
            "         7.1541e+00],\n",
            "        [0.0000e+00, 2.2855e-02, 0.0000e+00,  ..., 2.0603e-01, 5.1647e-04,\n",
            "         9.8811e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 9.1270e-03,  ..., 5.3362e+00, 0.0000e+00,\n",
            "         6.6489e+00],\n",
            "        ...,\n",
            "        [0.0000e+00, 6.6821e-01, 0.0000e+00,  ..., 5.5512e-01, 1.1020e-01,\n",
            "         5.0843e-01],\n",
            "        [3.6467e-01, 0.0000e+00, 0.0000e+00,  ..., 7.8842e-03, 1.5318e+00,\n",
            "         2.2341e-01],\n",
            "        [0.0000e+00, 1.5607e+00, 4.2893e-02,  ..., 1.0265e+01, 8.0747e-01,\n",
            "         5.2701e-01]], device='cuda:0')}, 'COCO_train2014_000000207647.jpg': {'pred_boxes': Boxes(tensor([[107.9097, 184.2087, 302.1315, 302.9165],\n",
            "        [134.8606,  25.9716, 192.9023,  77.7961],\n",
            "        [160.2345, 336.7727, 303.3979, 360.7060],\n",
            "        [145.7880, 145.0686, 236.6114, 299.8655],\n",
            "        [  0.0000, 269.5218, 392.3010, 389.0455],\n",
            "        [ 70.0735,   4.0500, 252.8372, 311.9594],\n",
            "        [198.6222, 104.5402, 248.7671, 125.5417],\n",
            "        [ 36.8743, 296.7173, 367.9554, 366.7027],\n",
            "        [  0.0000, 315.8291, 351.6746, 380.2603],\n",
            "        [119.3692,  67.2369, 209.3522, 206.8540],\n",
            "        [111.3973,  39.2222, 376.9167, 287.7454],\n",
            "        [306.9089, 265.6061, 340.8416, 308.2322],\n",
            "        [ 94.2438,  67.4416, 117.1679, 301.5137],\n",
            "        [ 56.3089, 326.9030, 398.3097, 390.3978],\n",
            "        [ 16.8137,  67.3644, 406.4090, 305.1952],\n",
            "        [298.8834,   3.7707, 406.1282, 275.3391],\n",
            "        [129.3664, 308.4424, 161.9873, 343.0090],\n",
            "        [216.6467, 163.8700, 245.1767, 188.2436],\n",
            "        [356.1708,  20.2115, 408.0000, 258.2127],\n",
            "        [133.4428,  41.2657, 189.7590, 100.4617],\n",
            "        [125.8882,  58.8676, 219.3742, 190.6607],\n",
            "        [105.6310,  43.4458, 333.4123, 325.7253],\n",
            "        [  3.4680,   0.0000, 406.1775, 211.4727],\n",
            "        [133.0412,  41.6514, 190.4613,  99.2148],\n",
            "        [  0.7936,  33.5628, 165.6133, 314.6554],\n",
            "        [215.8435, 154.0968, 241.6210, 350.8240],\n",
            "        [135.3913, 111.1804, 265.8709, 267.5166],\n",
            "        [ 57.8622, 113.7572, 297.9303, 371.7571],\n",
            "        [203.2340, 139.2804, 256.3169, 353.7168],\n",
            "        [ 56.2949, 113.9367, 298.1937, 371.9317],\n",
            "        [ 73.7415,  32.9276, 214.1472, 189.1965],\n",
            "        [ 52.3702, 285.3783, 408.0000, 390.3308],\n",
            "        [306.8614, 265.5008, 340.8593, 308.3159],\n",
            "        [216.9114, 163.7918, 245.3110, 188.5307],\n",
            "        [ 14.9745, 336.0365, 292.5783, 391.0000],\n",
            "        [120.7063,  77.5423, 260.8643, 254.3006]], device='cuda:0')), 'scores': tensor([0.8580, 0.7886, 0.7391, 0.6988, 0.6808, 0.5359, 0.5272, 0.5162, 0.5048,\n",
            "        0.4845, 0.4844, 0.4360, 0.4345, 0.4066, 0.4020, 0.4009, 0.3969, 0.3916,\n",
            "        0.3602, 0.3470, 0.3391, 0.3352, 0.3326, 0.3258, 0.3216, 0.3163, 0.2990,\n",
            "        0.2888, 0.2858, 0.2553, 0.2519, 0.2185, 0.2139, 0.2134, 0.2099, 0.2095],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 98,  36, 894,  98, 465,  50, 300, 465, 611,  51,  50, 242, 601, 465,\n",
            "        236,  50, 242, 673,  50, 191, 132,  91, 236,  36, 236, 601,  98,  50,\n",
            "        300,  91,  50, 611, 976, 567, 611, 201], device='cuda:0'), 'features': tensor([[0.0000, 0.0988, 0.4288,  ..., 0.0000, 0.1110, 0.2799],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0214, 0.6377],\n",
            "        [0.3731, 0.1155, 0.0000,  ..., 0.0000, 0.0000, 1.0475],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.9324,  ..., 0.0000, 0.6496, 3.2631],\n",
            "        [0.0253, 0.1157, 0.0178,  ..., 0.0000, 0.0000, 0.0084],\n",
            "        [0.3934, 0.1228, 0.0333,  ..., 0.0000, 0.2329, 3.7387]],\n",
            "       device='cuda:0')}, '001150.jpg': {'pred_boxes': Boxes(tensor([[167.7819, 146.2298, 411.9776, 375.0000],\n",
            "        [ 88.4849, 112.0547, 436.8233, 341.3371],\n",
            "        [150.4854,  64.3183, 349.6281, 371.1002],\n",
            "        [226.2918,  66.1785, 418.3409, 371.1551],\n",
            "        [  1.0925, 142.1433, 177.9196, 372.0457],\n",
            "        [ 89.4098, 140.7710, 348.3733, 374.7471],\n",
            "        [  6.6469, 130.9880, 284.3221, 328.1899],\n",
            "        [171.4989,  76.9047, 394.9900, 309.4576],\n",
            "        [344.8781,  43.3549, 499.9612, 314.0547],\n",
            "        [278.8124,   0.0000, 364.6920, 103.5378],\n",
            "        [249.4515, 216.6507, 298.8907, 244.9501],\n",
            "        [101.4745,  12.5753, 471.9063, 228.1653],\n",
            "        [250.4607,  60.1904, 500.0000, 312.3422],\n",
            "        [ 38.0694,   1.3884, 220.1033, 222.5585],\n",
            "        [  2.9732,  71.9794, 193.2273, 329.7179],\n",
            "        [  4.9165, 197.8765, 298.5303, 374.2036],\n",
            "        [  0.0000,  36.2101, 301.1753, 260.4474],\n",
            "        [119.4585,  65.4390, 445.8461, 266.8146],\n",
            "        [  1.0781,   1.6768, 351.2329, 209.3873],\n",
            "        [  5.6840,   0.0000, 236.8295, 136.6525],\n",
            "        [  2.5317, 231.1436, 211.0703, 375.0000],\n",
            "        [ 30.3933,   0.0000, 378.0893, 168.7866],\n",
            "        [ 81.8853,  44.6109, 340.8278, 297.3060],\n",
            "        [250.7689,   0.0000, 385.7028, 166.0677],\n",
            "        [ 55.9207, 217.3516, 424.3783, 375.0000],\n",
            "        [  0.6498,   0.5053, 333.0135, 113.9173],\n",
            "        [  3.5669,   1.2119, 329.9505, 117.2072],\n",
            "        [156.6445,   0.0000, 404.9704, 229.0946],\n",
            "        [251.1855,   0.0000, 386.4419, 166.2475],\n",
            "        [172.9721,   0.0000, 499.7790, 162.8203],\n",
            "        [223.0434,   0.0000, 426.8517, 228.1929],\n",
            "        [201.6276,   5.8516, 393.4230, 253.9915],\n",
            "        [ 65.7171,  10.3369, 210.9293, 140.7346],\n",
            "        [106.3647, 203.8163, 222.5629, 327.7615],\n",
            "        [285.2936,   1.6953, 500.0000, 197.1089],\n",
            "        [ 65.5754,   7.3816, 211.5711, 143.2066]], device='cuda:0')), 'scores': tensor([0.9712, 0.9673, 0.9567, 0.9503, 0.9354, 0.8972, 0.8304, 0.8122, 0.7723,\n",
            "        0.7639, 0.7100, 0.6878, 0.6871, 0.6826, 0.6697, 0.6525, 0.5844, 0.5401,\n",
            "        0.5305, 0.4988, 0.4812, 0.4643, 0.4104, 0.3531, 0.3085, 0.3070, 0.3061,\n",
            "        0.3049, 0.2980, 0.2969, 0.2888, 0.2729, 0.2710, 0.2709, 0.2682, 0.2537],\n",
            "       device='cuda:0'), 'pred_classes': tensor([117, 117, 117, 117,  47, 117,  47, 117, 136, 119, 198, 136, 136,  90,\n",
            "         47,  47, 136, 117, 136,  90,  47,  90, 117,  90, 117, 248, 136,  90,\n",
            "        139, 136, 139, 117,  51, 941, 136,  90], device='cuda:0'), 'features': tensor([[2.8229e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0392e+00,\n",
            "         7.1676e-01],\n",
            "        [6.9500e-01, 0.0000e+00, 0.0000e+00,  ..., 1.1147e-01, 2.5556e+00,\n",
            "         4.7973e-04],\n",
            "        [2.3529e-01, 0.0000e+00, 5.8187e-03,  ..., 5.6340e-02, 3.4078e-01,\n",
            "         1.2257e-01],\n",
            "        ...,\n",
            "        [1.1579e-02, 0.0000e+00, 0.0000e+00,  ..., 1.8347e-03, 2.3226e+00,\n",
            "         3.3749e-01],\n",
            "        [8.0758e-01, 0.0000e+00, 0.0000e+00,  ..., 1.3421e-01, 7.4029e+00,\n",
            "         0.0000e+00],\n",
            "        [3.6526e-02, 3.0472e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7793e-01,\n",
            "         2.3319e-01]], device='cuda:0')}, '001763.jpg': {'pred_boxes': Boxes(tensor([[2.7936e+01, 1.3670e+01, 2.9475e+02, 2.7900e+02],\n",
            "        [2.3978e+01, 8.7222e+01, 2.9443e+02, 3.4922e+02],\n",
            "        [0.0000e+00, 5.7595e+01, 2.0050e+02, 3.1307e+02],\n",
            "        [4.0032e+01, 3.1393e+02, 9.9977e+01, 3.6578e+02],\n",
            "        [9.3986e+01, 1.8640e+01, 3.3184e+02, 3.5709e+02],\n",
            "        [2.8768e+02, 9.2425e+01, 5.0000e+02, 3.3436e+02],\n",
            "        [3.0268e+01, 6.6101e+01, 3.9963e+02, 3.1039e+02],\n",
            "        [1.7050e+00, 3.9459e+00, 2.2357e+02, 2.4481e+02],\n",
            "        [2.6211e+02, 1.6123e+02, 5.0000e+02, 3.7500e+02],\n",
            "        [3.7846e+02, 2.0079e+02, 4.0523e+02, 2.2605e+02],\n",
            "        [3.0405e+01, 1.7735e+02, 1.1185e+02, 2.2184e+02],\n",
            "        [3.5108e+02, 1.7554e+02, 4.3452e+02, 2.0703e+02],\n",
            "        [4.5098e+02, 2.8393e+02, 4.9953e+02, 3.6489e+02],\n",
            "        [2.6770e+02, 3.0459e+01, 4.5950e+02, 3.2767e+02],\n",
            "        [3.5090e+02, 1.8251e+02, 3.7864e+02, 1.9817e+02],\n",
            "        [0.0000e+00, 2.4609e+02, 5.0368e+01, 2.9676e+02],\n",
            "        [0.0000e+00, 1.4509e+02, 2.5041e+02, 3.7500e+02],\n",
            "        [1.8483e+02, 3.0810e+01, 5.0000e+02, 2.7563e+02],\n",
            "        [1.8961e+02, 1.3055e+02, 4.5924e+02, 3.7437e+02],\n",
            "        [4.3021e-01, 8.9313e-01, 1.3025e+02, 2.5151e+02],\n",
            "        [2.2937e+01, 7.0583e+01, 1.6774e+02, 2.1271e+02],\n",
            "        [3.7198e+01, 1.2853e+02, 4.1107e+02, 3.6535e+02],\n",
            "        [3.2788e+02, 1.0631e+02, 4.5379e+02, 2.2932e+02],\n",
            "        [2.2996e+01, 1.1135e+01, 4.3242e+02, 2.1858e+02],\n",
            "        [3.3021e+02, 1.4990e+02, 4.6623e+02, 3.0425e+02],\n",
            "        [1.1468e+02, 3.2997e+00, 4.9908e+02, 1.8554e+02],\n",
            "        [1.4797e+02, 8.9341e+01, 4.3752e+02, 3.1946e+02],\n",
            "        [1.3161e+02, 3.1129e-01, 4.6264e+02, 4.9063e+01],\n",
            "        [3.3285e+02, 1.4297e+02, 4.5570e+02, 2.6932e+02],\n",
            "        [0.0000e+00, 2.6175e+02, 3.1667e+02, 3.7500e+02],\n",
            "        [0.0000e+00, 1.5289e+02, 3.7109e+02, 3.7500e+02],\n",
            "        [2.4777e+01, 7.4574e+01, 1.6417e+02, 2.1061e+02],\n",
            "        [3.2857e+02, 1.0830e+02, 4.5385e+02, 2.2829e+02],\n",
            "        [1.2948e+01, 0.0000e+00, 2.4776e+02, 6.6923e+01],\n",
            "        [2.4230e+02, 2.1303e+02, 5.0000e+02, 3.7500e+02],\n",
            "        [2.3778e+02, 1.0013e+01, 5.0000e+02, 2.4726e+02]], device='cuda:0')), 'scores': tensor([0.9724, 0.9680, 0.9472, 0.9203, 0.9026, 0.8800, 0.8651, 0.8484, 0.8449,\n",
            "        0.8316, 0.7825, 0.7821, 0.7638, 0.7270, 0.7182, 0.6576, 0.6482, 0.6333,\n",
            "        0.6033, 0.6025, 0.5193, 0.5085, 0.4758, 0.4553, 0.4464, 0.4036, 0.4011,\n",
            "        0.3864, 0.3660, 0.2592, 0.2373, 0.2365, 0.2337, 0.2283, 0.2168, 0.2038],\n",
            "       device='cuda:0'), 'pred_classes': tensor([117, 117, 117, 786, 117,  53, 117, 117,  53, 391, 198, 546, 786,  53,\n",
            "        467, 786, 117, 136,  53, 117, 117, 117, 191, 136,  53, 136,  53, 248,\n",
            "        327, 775, 136, 327, 327, 453, 117, 248], device='cuda:0'), 'features': tensor([[3.7890e-01, 2.3373e-01, 0.0000e+00,  ..., 1.4091e+00, 2.6362e+00,\n",
            "         1.5132e-02],\n",
            "        [3.9542e-01, 0.0000e+00, 0.0000e+00,  ..., 2.1728e-01, 1.5782e+00,\n",
            "         1.8923e-01],\n",
            "        [2.0565e-01, 0.0000e+00, 2.0803e-03,  ..., 2.5553e-01, 3.9951e+00,\n",
            "         8.5679e-03],\n",
            "        ...,\n",
            "        [7.9336e-04, 4.8984e+00, 0.0000e+00,  ..., 2.7860e+00, 4.5546e+00,\n",
            "         9.6216e-02],\n",
            "        [8.0755e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.6080e+00,\n",
            "         1.1887e+00],\n",
            "        [1.2584e+00, 1.6126e-01, 2.2939e-01,  ..., 1.5437e-01, 2.4645e+00,\n",
            "         4.1327e-02]], device='cuda:0')}, 'COCO_train2014_000000387267.jpg': {'pred_boxes': Boxes(tensor([[2.2664e+02, 3.3990e+01, 6.4000e+02, 2.6522e+02],\n",
            "        [0.0000e+00, 4.7161e+01, 3.7980e+02, 2.7007e+02],\n",
            "        [9.2550e+01, 2.3406e+01, 5.5830e+02, 2.5275e+02],\n",
            "        [0.0000e+00, 3.0465e+01, 4.5310e+02, 2.1174e+02],\n",
            "        [6.9946e+01, 1.3112e+02, 1.4632e+02, 1.8782e+02],\n",
            "        [1.7557e+02, 1.3083e+02, 2.3783e+02, 1.9018e+02],\n",
            "        [1.5059e+02, 3.2427e+01, 6.2061e+02, 1.9185e+02],\n",
            "        [2.8329e+02, 1.3992e+02, 5.7596e+02, 3.5145e+02],\n",
            "        [1.2617e+02, 2.4999e+02, 1.9922e+02, 3.2637e+02],\n",
            "        [0.0000e+00, 6.4915e-01, 4.4992e+02, 7.0684e+01],\n",
            "        [3.1283e+02, 1.1894e+01, 6.1778e+02, 2.2486e+02],\n",
            "        [1.7572e+02, 7.6465e+01, 6.0763e+02, 3.0382e+02],\n",
            "        [6.1085e+02, 1.0554e+02, 6.4000e+02, 1.4468e+02],\n",
            "        [4.8989e+02, 2.5284e+02, 5.6092e+02, 3.2501e+02],\n",
            "        [0.0000e+00, 2.6457e+01, 2.1678e+02, 2.5787e+02],\n",
            "        [4.0216e+02, 2.4337e+02, 4.7773e+02, 3.2549e+02],\n",
            "        [1.9602e+02, 3.7977e-01, 6.2556e+02, 5.4752e+01],\n",
            "        [1.3192e+02, 1.4818e+02, 5.6165e+02, 3.3792e+02],\n",
            "        [1.5267e+01, 1.5517e+02, 4.6142e+02, 3.5515e+02],\n",
            "        [1.1062e+02, 2.4048e+02, 2.1144e+02, 3.3294e+02],\n",
            "        [4.3414e+02, 2.8319e+00, 6.4000e+02, 2.7261e+02],\n",
            "        [3.5867e+02, 1.4424e+02, 6.4000e+02, 3.5196e+02],\n",
            "        [0.0000e+00, 1.3459e+00, 2.6524e+02, 1.0380e+02],\n",
            "        [9.6231e+01, 1.4663e+00, 5.6533e+02, 8.3802e+01],\n",
            "        [1.1000e+02, 2.4062e+02, 2.1223e+02, 3.3340e+02],\n",
            "        [1.0805e+02, 2.9672e+01, 4.1192e+02, 2.8741e+02],\n",
            "        [4.0162e+02, 2.4327e+02, 4.7846e+02, 3.2616e+02],\n",
            "        [2.3331e+02, 2.0439e+01, 5.3790e+02, 2.2912e+02],\n",
            "        [4.7803e+02, 2.5068e+02, 5.7409e+02, 3.3022e+02],\n",
            "        [0.0000e+00, 7.9095e+01, 2.7093e+02, 3.1862e+02],\n",
            "        [5.6065e+01, 3.1456e+00, 5.4280e+02, 1.5015e+02],\n",
            "        [3.4092e+02, 1.0403e+00, 6.4000e+02, 8.2076e+01],\n",
            "        [0.0000e+00, 1.1179e+02, 3.5068e+02, 3.3542e+02],\n",
            "        [1.8400e+02, 2.6872e+02, 6.4000e+02, 3.5870e+02],\n",
            "        [1.0534e+02, 4.5411e+01, 3.1258e+02, 3.5038e+02],\n",
            "        [9.7738e+01, 2.8201e+02, 5.3893e+02, 3.5892e+02]], device='cuda:0')), 'scores': tensor([0.8795, 0.8727, 0.8354, 0.7529, 0.7245, 0.7005, 0.6963, 0.6272, 0.6242,\n",
            "        0.6217, 0.6030, 0.5893, 0.5873, 0.5536, 0.5229, 0.4937, 0.4885, 0.4865,\n",
            "        0.4851, 0.4782, 0.4729, 0.4565, 0.4536, 0.4520, 0.4462, 0.4424, 0.4423,\n",
            "        0.4127, 0.3955, 0.3498, 0.3495, 0.3463, 0.3384, 0.3279, 0.3139, 0.3098],\n",
            "       device='cuda:0'), 'pred_classes': tensor([177, 177, 177, 177, 453, 453, 177, 155, 292,  72, 177, 177, 453, 292,\n",
            "        177, 292,  72, 155, 155, 292, 177, 155,  72,  72, 173, 177, 173, 177,\n",
            "        173, 177, 177,  72, 155, 299, 177, 299], device='cuda:0'), 'features': tensor([[8.3259e-01, 1.5581e-01, 6.0853e-01,  ..., 1.5425e+00, 1.3400e-02,\n",
            "         2.2299e-01],\n",
            "        [1.0862e+00, 2.3553e-01, 7.0838e-01,  ..., 1.2068e+00, 7.0623e-04,\n",
            "         2.1380e-02],\n",
            "        [1.4091e+00, 8.5757e-02, 6.1720e-01,  ..., 4.0298e-01, 5.7928e-03,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [0.0000e+00, 1.6628e+00, 2.3589e+00,  ..., 1.6002e+00, 0.0000e+00,\n",
            "         4.1500e-02],\n",
            "        [6.8262e-01, 1.0785e-01, 1.5632e+00,  ..., 2.2214e+00, 0.0000e+00,\n",
            "         7.4412e-01],\n",
            "        [0.0000e+00, 3.1624e+00, 2.3809e+00,  ..., 1.2088e+00, 0.0000e+00,\n",
            "         4.5539e-02]], device='cuda:0')}, 'COCO_train2014_000000578174.jpg': {'pred_boxes': Boxes(tensor([[2.0987e+02, 3.6450e+02, 4.5232e+02, 3.9380e+02],\n",
            "        [1.8665e+02, 2.3823e+00, 4.3830e+02, 2.0241e+02],\n",
            "        [0.0000e+00, 8.5354e-01, 6.7804e+01, 1.4026e+02],\n",
            "        [4.4880e+02, 2.6857e+02, 6.2217e+02, 2.8942e+02],\n",
            "        [8.3611e+00, 1.9524e+02, 9.7161e+01, 3.6417e+02],\n",
            "        [1.8274e+02, 1.2376e+02, 2.7702e+02, 1.6970e+02],\n",
            "        [3.1325e+02, 2.6958e+00, 5.6982e+02, 2.5743e+02],\n",
            "        [1.6225e+02, 1.0916e+02, 3.1264e+02, 1.7449e+02],\n",
            "        [0.0000e+00, 1.5102e+02, 1.3942e+02, 3.9104e+02],\n",
            "        [1.8802e+01, 1.3427e+02, 7.9615e+01, 1.6113e+02],\n",
            "        [5.8672e+02, 3.0131e+02, 6.4000e+02, 3.1959e+02],\n",
            "        [1.0343e+01, 1.3077e+02, 2.7854e+01, 1.6667e+02],\n",
            "        [8.2153e+01, 6.5260e-02, 3.5885e+02, 1.4356e+02],\n",
            "        [1.8849e+00, 9.7618e+00, 1.2041e+02, 1.4973e+02],\n",
            "        [5.1264e+02, 1.5049e+02, 5.7573e+02, 2.2194e+02],\n",
            "        [2.3276e+02, 1.6450e+02, 3.4166e+02, 2.5127e+02],\n",
            "        [0.0000e+00, 1.0218e+02, 3.9495e+02, 3.6608e+02],\n",
            "        [4.3974e+02, 1.5149e+00, 6.3805e+02, 2.2758e+02],\n",
            "        [4.7855e+02, 2.0933e+02, 6.0876e+02, 2.5409e+02],\n",
            "        [3.8695e+02, 2.7119e+02, 4.4942e+02, 2.9162e+02],\n",
            "        [1.7998e+02, 1.8418e+02, 6.3362e+02, 3.7988e+02],\n",
            "        [1.5458e+00, 4.3223e+01, 3.7518e+02, 2.6523e+02],\n",
            "        [5.0566e+02, 1.7280e+02, 5.6188e+02, 2.3039e+02],\n",
            "        [0.0000e+00, 2.0698e+02, 5.0409e+02, 3.9400e+02],\n",
            "        [1.5429e+02, 2.2672e+02, 6.4000e+02, 3.9257e+02],\n",
            "        [3.7594e+02, 2.1059e+02, 6.3344e+02, 2.6454e+02],\n",
            "        [4.4152e+02, 1.9466e+02, 6.3003e+02, 2.6199e+02],\n",
            "        [1.7814e+00, 7.7378e+01, 2.0390e+02, 3.2613e+02],\n",
            "        [1.1344e+02, 1.4635e+01, 3.7469e+02, 2.4316e+02],\n",
            "        [1.7266e+02, 3.8359e-01, 6.2736e+02, 1.6712e+02],\n",
            "        [3.4923e+02, 2.2099e+02, 6.4000e+02, 3.9312e+02],\n",
            "        [1.4995e+02, 7.6198e+01, 4.1770e+02, 3.1782e+02],\n",
            "        [2.1059e+00, 2.1763e+02, 2.6233e+02, 3.9346e+02],\n",
            "        [2.4461e+02, 1.7256e+02, 3.3085e+02, 2.4876e+02],\n",
            "        [3.7871e+02, 2.7576e-01, 6.4000e+02, 1.7034e+02],\n",
            "        [3.8010e+02, 2.1043e+02, 6.3240e+02, 2.6447e+02]], device='cuda:0')), 'scores': tensor([0.8933, 0.8112, 0.7846, 0.7817, 0.6890, 0.6546, 0.6280, 0.6272, 0.5838,\n",
            "        0.5707, 0.5531, 0.5195, 0.4879, 0.4795, 0.4494, 0.4308, 0.4015, 0.3989,\n",
            "        0.3878, 0.3856, 0.3802, 0.3757, 0.3674, 0.3563, 0.3509, 0.3253, 0.3001,\n",
            "        0.2939, 0.2641, 0.2611, 0.2584, 0.2553, 0.2512, 0.2481, 0.2275, 0.2012],\n",
            "       device='cuda:0'), 'pred_classes': tensor([ 683,  291,  291,  683,  397,  251,  291,  251,  397,   52,  937,  128,\n",
            "         291,  291,   50,   52,  299,  291,  248,  937,  299,  299,   50,  299,\n",
            "         308,  397,  248,  299,  291,  381,  299,  299,  397,  251, 1180,  906],\n",
            "       device='cuda:0'), 'features': tensor([[0.0000e+00, 8.4375e-01, 0.0000e+00,  ..., 0.0000e+00, 1.1957e+00,\n",
            "         0.0000e+00],\n",
            "        [0.0000e+00, 1.4626e-03, 2.1931e+00,  ..., 1.6413e+00, 0.0000e+00,\n",
            "         1.7242e+00],\n",
            "        [5.1999e-03, 0.0000e+00, 1.9752e+00,  ..., 7.7379e-01, 0.0000e+00,\n",
            "         1.1940e-02],\n",
            "        ...,\n",
            "        [6.8626e-03, 5.2631e-03, 7.5067e-01,  ..., 2.9698e+00, 0.0000e+00,\n",
            "         2.5644e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7213e-01,  ..., 7.0565e-03, 2.1306e-01,\n",
            "         3.1533e-01],\n",
            "        [0.0000e+00, 6.2195e-01, 6.9650e-01,  ..., 0.0000e+00, 2.9579e-02,\n",
            "         1.3168e-01]], device='cuda:0')}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWckHOky8Dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0cfde2ab-780d-4345-e34e-525531087392"
      },
      "source": [
        "!scp -P 33005 /content/py-bottom-up-attention/fhmc_embeddings.pt fhmc@113.30.156.94:~/hdd_home/Images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Host key verification failed.\r\n",
            "lost connection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eiF5sqr1aGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "files.download(\"/content/py-bottom-up-attention/fhmc_embeddings.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYQxejoj23tV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ec56d010-529b-4bbd-ec2b-272d25a89b57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLDhIJ8x36pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -i /content/py-bottom-up-attention/fhmc_embeddings.pt /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}